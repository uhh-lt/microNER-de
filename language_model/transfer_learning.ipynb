{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '/home/jindal/notebooks/jindal/NER')\n",
    "import fastText\n",
    "import numpy as np \n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import sklearn, re, math\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "import linecache\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "epochs = 50\n",
    "trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_sequence_length=50\n",
    "nb_embedding_dims=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To load the initial model for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def createMatrices(sentences, word2Idx, case2Idx, char2Idx):\n",
    "#     #{'numeric': 0, 'allLower': 1, 'contains_digit': 6, 'PADDING_TOKEN': 7, 'other': 4, 'allUpper': 2, 'mainly_numeric': 5, 'initialUpper': 3}\n",
    "#     unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "#     paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "        \n",
    "#     dataset = []\n",
    "    \n",
    "#     wordCount = 0\n",
    "#     unknownWordCount = 0\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         wordIndices = []    \n",
    "#         caseIndices = []\n",
    "#         charIndices = []\n",
    "# #         labelIndices = []\n",
    "        \n",
    "#         for word,char in sentence:  \n",
    "#             wordCount += 1\n",
    "#             if word in word2Idx:\n",
    "#                 wordIdx = word2Idx[word]\n",
    "#             elif word.lower() in word2Idx:\n",
    "#                 wordIdx = word2Idx[word.lower()]                 \n",
    "#             else:\n",
    "#                 wordIdx = unknownIdx\n",
    "#                 unknownWordCount += 1\n",
    "#             charIdx = []\n",
    "#             for x in char:\n",
    "#                 charIdx.append(char2Idx[x])\n",
    "#             #Get the label and map to int            \n",
    "#             wordIndices.append(wordIdx)\n",
    "#             caseIndices.append(getCasing(word, case2Idx))\n",
    "#             charIndices.append(charIdx)\n",
    "# #             labelIndices.append(label2Idx[label])\n",
    "           \n",
    "#         dataset.append([wordIndices,caseIndices, charIndices]) \n",
    "        \n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# def getCasing(word, caseLookup):   \n",
    "#     casing = 'other'\n",
    "    \n",
    "#     numDigits = 0\n",
    "#     for char in word:\n",
    "#         if char.isdigit():\n",
    "#             numDigits += 1\n",
    "            \n",
    "#     digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "#     if word.isdigit(): #Is a digit\n",
    "#         casing = 'numeric'\n",
    "#     elif digitFraction > 0.5:\n",
    "#         casing = 'mainly_numeric'\n",
    "#     elif word.islower(): #All lower case\n",
    "#         casing = 'allLower'\n",
    "#     elif word.isupper(): #All upper case\n",
    "#         casing = 'allUpper'\n",
    "#     elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "#         casing = 'initialUpper'\n",
    "#     elif numDigits > 0:\n",
    "#         casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "#     return caseLookup[casing]\n",
    "\n",
    "# def padding(Sentences):\n",
    "#     maxlen = 52\n",
    "#     for sentence in Sentences:\n",
    "#         char = sentence[2]\n",
    "#         for x in char:\n",
    "#             maxlen = max(maxlen,len(x))\n",
    "#     for i,sentence in enumerate(Sentences):\n",
    "#         Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "#     return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n"
     ]
    }
   ],
   "source": [
    "# case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "# caseEmbeddings = np.identity(len(case2Idx), dtype='float32')\n",
    "# print(caseEmbeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# characters={}\n",
    "\n",
    "# with open ('/home/jindal/notebooks/jindal/NER/language_model/twitter_dataset_sequences_one_million') as f:\n",
    "#     for line in f:\n",
    "#         for word in line:\n",
    "#             word =str(word)\n",
    "#             for char in word:\n",
    "# #                 print(char)\n",
    "#                 characters[char]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# char2Idx={}\n",
    "# for char in characters:\n",
    "#     char2Idx[char] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# toy_vocab ={}\n",
    "\n",
    "# with open('twitter_dataset_sequences_one_million') as f:\n",
    "#     for line in f:\n",
    "# #         print(line)\n",
    "#         line = line.split()\n",
    "# #         print(len(line))\n",
    "#         for word in line:\n",
    "# #             word = word\n",
    "# #             print(word)\n",
    "#             toy_vocab[word]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word2Idx={}\n",
    "# for word in toy_vocab.keys():\n",
    "#     word2Idx[word] = len(word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917425\n"
     ]
    }
   ],
   "source": [
    "# vocab_size = len(word2Idx.keys())+1\n",
    "# print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # for wiki\n",
    "# words_input = Input(shape=(None,300), dtype='float32',name='words_input')\n",
    "# # words = Embedding(input_dim =50, output_dim=300, trainable=False)(words_input)\n",
    "# casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "# casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\n",
    "# character_input=Input(shape=(None,52,),name='char_input')\n",
    "# embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "# dropout= Dropout(0.5, name='dropout1')(embed_char_out)\n",
    "# conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, name='conv'))(dropout)\n",
    "# maxpool_out=TimeDistributed(MaxPooling1D(52), name='maxpool')(conv1d_out)\n",
    "# char = TimeDistributed(Flatten())(maxpool_out)\n",
    "# char = Dropout(0.5)(char)\n",
    "# output = concatenate([words_input, char])\n",
    "# output = Bidirectional(LSTM(200, return_sequences=False, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "# # output = TimeDistributed(Dense(vocab_size))(output)\n",
    "# # crf = CRF(len(label2Idx))\n",
    "# # output = crf(output)\n",
    "# # output = Flatten()(output)\n",
    "# output = Dense(vocab_size, activation='softmax')(output)\n",
    "# # output = Flatten()(output)\n",
    "# # output = Dense(vocab_size, activation='softmax')(output)\n",
    "# # model.add(Dense(vocab_size, activation='softmax'))\n",
    "# model = Model(inputs=[words_input,casing_input, character_input], outputs=[output])\n",
    "# # model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 50, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 50, 200)      320800      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 50, 200)      0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 48, 200)      120200      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 200)      160200      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 46, 200)      200200      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 48, 200)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 47, 200)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 46, 200)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 200)          0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 200)          0           leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 200)          0           leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 200)          0           global_max_pooling1d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 200)          0           global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 200)          0           global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 600)          0           dropout_10[0][0]                 \n",
      "                                                                 dropout_11[0][0]                 \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 100)          60100       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 100)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 300)          30300       leaky_re_lu_20[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 891,800\n",
      "Trainable params: 891,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# final model\n",
    "model_input_embedding = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "lstm_block = Bidirectional(LSTM(100, dropout = 0.5, return_sequences=True))(model_input_embedding)\n",
    "lstm_block = LeakyReLU()(lstm_block)\n",
    "\n",
    "filter_sizes = (3, 4, 5)\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Conv1D(\n",
    "        filters = 200,\n",
    "        kernel_size = sz,\n",
    "        padding = 'valid',\n",
    "        strides = 1\n",
    "    )(lstm_block)\n",
    "    conv = LeakyReLU()(conv)\n",
    "    conv = GlobalMaxPooling1D()(conv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "    conv_blocks.append(conv)\n",
    "model_concatenated = concatenate([conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "# model_concatenated = Dropout(0.8)(model_concatenated)\n",
    "model_concatenated = Dense(100)(model_concatenated)\n",
    "model_concatenated = LeakyReLU()(model_concatenated)\n",
    "model_output = Dense(300, activation = \"softmax\")(model_concatenated)\n",
    "model = Model(model_input_embedding, model_output)\n",
    "model.compile(loss='cosine_proximity', optimizer='nadam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('german_lm_twitter_50m_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_5\n",
      "bidirectional_5\n",
      "leaky_re_lu_16\n",
      "conv1d_10\n",
      "conv1d_11\n",
      "conv1d_12\n",
      "leaky_re_lu_17\n",
      "leaky_re_lu_18\n",
      "leaky_re_lu_19\n",
      "global_max_pooling1d_10\n",
      "global_max_pooling1d_11\n",
      "global_max_pooling1d_12\n",
      "dropout_10\n",
      "dropout_11\n",
      "dropout_12\n",
      "concatenate_4\n",
      "dense_10\n",
      "leaky_re_lu_20\n",
      "dense_11\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7fe8252f2c88>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_5\n",
      "bidirectional_5\n",
      "leaky_re_lu_16\n",
      "conv1d_10\n",
      "conv1d_11\n",
      "conv1d_12\n",
      "leaky_re_lu_17\n",
      "leaky_re_lu_18\n",
      "leaky_re_lu_19\n",
      "global_max_pooling1d_10\n",
      "global_max_pooling1d_11\n",
      "global_max_pooling1d_12\n",
      "dropout_10\n",
      "dropout_11\n",
      "dropout_12\n",
      "concatenate_4\n",
      "dense_10\n",
      "leaky_re_lu_20\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# characters={}\n",
    "labels = {}\n",
    "# with open('/home/jindal/notebooks/jindal/NER/language_model/germeval2018.training.txt') as f:\n",
    "with open('/home/gwiedemann/notebooks/OffLang/sample_train.txt') as f:\n",
    "    for line in f:\n",
    "        text, label_simple, label_complex = line.split('\\t')\n",
    "        labels[label_simple]=True\n",
    "#         for word in text:\n",
    "#             for char in word:\n",
    "#                 characters[char]=True\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label2Idx = {}\n",
    "for label in labels:\n",
    "    label2Idx[label] = len(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OFFENSE': 0, 'OTHER': 1}\n"
     ]
    }
   ],
   "source": [
    "print(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = Dense(len(label2Idx), activation = 'softmax')(model.layers[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model = Model(inputs=model.input, outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_5\n",
      "bidirectional_5\n",
      "leaky_re_lu_16\n",
      "conv1d_10\n",
      "conv1d_11\n",
      "conv1d_12\n",
      "leaky_re_lu_17\n",
      "leaky_re_lu_18\n",
      "leaky_re_lu_19\n",
      "global_max_pooling1d_10\n",
      "global_max_pooling1d_11\n",
      "global_max_pooling1d_12\n",
      "dropout_10\n",
      "dropout_11\n",
      "dropout_12\n",
      "concatenate_4\n",
      "dense_10\n",
      "leaky_re_lu_20\n",
      "dense_12\n"
     ]
    }
   ],
   "source": [
    "for layer in final_model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 50, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 50, 200)      320800      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 50, 200)      0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 48, 200)      120200      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 200)      160200      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 46, 200)      200200      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 48, 200)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 47, 200)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 46, 200)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 200)          0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 200)          0           leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 200)          0           leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 200)          0           global_max_pooling1d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 200)          0           global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 200)          0           global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 600)          0           dropout_10[0][0]                 \n",
      "                                                                 dropout_11[0][0]                 \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 100)          60100       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 100)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 2)            202         leaky_re_lu_20[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 861,702\n",
      "Trainable params: 861,702\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# out_file1 = open('/home/jindal/notebooks/jindal/NER/language_model/germeval2018.training_simple_labels.txt','wb')\n",
    "# out_file2 = open('/home/jindal/notebooks/jindal/NER/language_model/germeval2018.training_complex_labels.txt','wb')\n",
    "# with open('/home/jindal/notebooks/jindal/NER/language_model/germeval2018.training.txt', ) as f:\n",
    "#     for line in f:\n",
    "#         line = line.split('\\t')\n",
    "# #         print(len(line))\n",
    "#         text, label_simple, label_complex = line\n",
    "#         simple = [text, label_simple]\n",
    "#         compl = [text, label_complex]\n",
    "#         string_simple = '\\t'.join(simple)+' \\n'\n",
    "#         string_complex = '\\t'.join(compl)+' \\n'\n",
    "#         out_file1.write(string_simple.encode())\n",
    "#         out_file2.write(string_complex.encode())\n",
    "# out_file1.close()\n",
    "# out_file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# line_number=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home/jindal/notebooks/fastText/wiki.de.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# char2Idx={}\n",
    "# for char in characters:\n",
    "#     char2Idx[char] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def file_len(fname):\n",
    "#     with open(fname) as f:\n",
    "#         for i, l in enumerate(f):\n",
    "#             pass\n",
    "#     return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sequential_generator(filename, batch_size):\n",
    "    \n",
    "    f = open(filename)\n",
    "    \n",
    "#     file_length = sum(1 for line in open(filename, encoding = 'UTF-8'))\n",
    "#     shuffled_indexes = range(1, file_length + 1)\n",
    "    # shuffled_indexes = sample(shuffled_indexes, len(shuffled_indexes))\n",
    "#     index_position = 0\n",
    "    \n",
    "    batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "#     batch_features_lg = np.zeros((batch_size, nb_sequence_length, nb_embedding2_dims))\n",
    "    # batch_features_idx = np.zeros((batch_size, nb_sequence_length))\n",
    "    batch_labels = np.zeros((batch_size, 2))\n",
    "\n",
    "    while True:\n",
    "        # print(len(features))\n",
    "        for i in range(batch_size):\n",
    "            line = f.readline()\n",
    "            if (\"\" == line):\n",
    "                f.seek(0)\n",
    "                line = f.readline()\n",
    "#             line = linecache.getline(filename, shuffled_indexes[index_position])\n",
    "            data = line.strip().split('\\t')\n",
    "            batch_features_ft[i] = process_features(data[0], nb_sequence_length, nb_embedding_dims)\n",
    "            # print(batch_features_ft[i])\n",
    "            # print(batch_features_ft[i].shape)\n",
    "            batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "#             index_position += 1\n",
    "#             if index_position == file_length:\n",
    "                # shuffle indexes again\n",
    "#                 shuffled_indexes = range(1, file_length + 1)\n",
    "                # shuffled_indexes = sample(shuffled_indexes, len(shuffled_indexes))\n",
    "#                 index_position = 0\n",
    "#                 break\n",
    "        # yield [batch_features_ft, batch_features_lg], batch_labels\n",
    "        yield ([batch_features_ft], batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(textline):\n",
    "    textline = re.sub('@[\\w_]+', 'USER_MENTION', textline)\n",
    "    textline = re.sub('\\|LBR\\|', '', textline)\n",
    "    textline = re.sub('\\.\\.\\.+', '...', textline)\n",
    "    textline = re.sub('!!+', '!!', textline)\n",
    "    textline = re.sub('\\?\\?+', '??', textline)\n",
    "    words = re.compile('[\\U00010000-\\U0010ffff]|[\\w-]+|[^ \\w\\U00010000-\\U0010ffff]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    # print(words)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_ft = {}\n",
    "def process_features(textline, nb_sequence_length, nb_embedding_dims):\n",
    "    words = twitter_tokenizer(textline)\n",
    "    features_ft = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    features_idx = np.zeros(nb_sequence_length)\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            word_vectors_ft[w] = wv\n",
    "        features_ft[idx] = wv\n",
    "        \n",
    "        idx = idx + 1\n",
    "    return features_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# line_number=0\n",
    "# for inp, output in my_generator('/home/gwiedemann/notebooks/OffLang/sample_train.txt',32):\n",
    "#     continue\n",
    "# #     print(inp[0].shape)\n",
    "# #     print(inp[1].shape)\n",
    "# #     print(inp[2].shape)\n",
    "# #     print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def validation_data_generator(file, batch_size):\n",
    "    \n",
    "#     global line_number_validation, lock\n",
    "\n",
    "#     while True:\n",
    "#         word_embeddings = []\n",
    "#         case_embeddings = []\n",
    "#         char_embeddings = []\n",
    "#         output_labels = []\n",
    "#         total_lines = file_len(file)\n",
    "#         for i in range(batch_size):\n",
    "            \n",
    "# #             lock.acquire()\n",
    "            \n",
    "#             index = line_number_validation%total_lines +1\n",
    "#             line_number_validation+=1\n",
    "# #             lock.release()\n",
    "# #             index = random.choice(len(features), 1)[0]\n",
    "#             line = linecache.getline(file, index)\n",
    "#             if len(line.split('\\t'))!=3:\n",
    "#                 continue\n",
    "#             text, simple_label, complex_label = line.split('\\t')\n",
    "# #             assert index<=100\n",
    "# #             print(text)\n",
    "#             temp_casing = []\n",
    "#             temp_char=[]\n",
    "#             temp_word=[]\n",
    "# #             print(line)\n",
    "# #             print(len)\n",
    "#             for word in text.split():\n",
    "# #                 print(word)\n",
    "#                 casing =getCasing(word, case2Idx)\n",
    "# #                 print(casing)\n",
    "#                 temp_casing.append(casing)\n",
    "#                 temp_char2=[]\n",
    "#                 for char in word:\n",
    "#                     temp_char2.append(char2Idx[char])\n",
    "#                 temp_char2 = np.array(temp_char2)\n",
    "# #                 print(temp_char2)\n",
    "# #                 temp_char2 = pad_sequences(temp_char2, 52, padding='post')\n",
    "# #                 temp_char.append(pad_sequences(temp_char2, 52, padding='post'))\n",
    "#                 temp_char.append(temp_char2)\n",
    "#                 word_vector = ft.get_word_vector(word.lower())\n",
    "# #                 print(len(word_vector))\n",
    "#                 temp_word.append(word_vector)\n",
    "#             temp_char = pad_sequences(temp_char, 52)\n",
    "# #             print(temp_word)\n",
    "# #             print(len(temp_word))\n",
    "# #             print(len(temp_casing))\n",
    "# #             print(temp_char)\n",
    "# #             print(len(temp_char))\n",
    "# #             print(label2Idx[label])\n",
    "# #             print(index)\n",
    "# #             print(\" **************** \")\n",
    "#             word_embeddings.append(temp_word)\n",
    "# #             print((len(word_embeddings), len(word_embeddings[0]), len(word_embeddings[0][0])))\n",
    "\n",
    "#             case_embeddings.append(temp_casing)\n",
    "#             char_embeddings.append(temp_char)\n",
    "#             output_labels.append(label2Idx[simple_label])\n",
    "# #             print(np.array(word_embeddings))\n",
    "# #             batch_features[i] = process_features(line, window_size-1, nb_embedding_dims)\n",
    "#             # print(batch_features[i])\n",
    "#             # print(batch_features[i].shape)\n",
    "# #             batch_labels[i] = labels[index]\n",
    "#         yield ([np.array(word_embeddings), np.array(case_embeddings), np.array(char_embeddings)], to_categorical(output_labels, len(label2Idx)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_lines = [line.strip().split(\"\\t\") for line in open('/home/gwiedemann/notebooks/OffLang/sample_train.txt', encoding = \"UTF-8\")]\n",
    "dev_lines = [line.strip().split(\"\\t\") for line in open('/home/gwiedemann/notebooks/OffLang/sample_dev.txt', encoding = \"UTF-8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sentences = [x[0] for x in train_lines]\n",
    "train_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in train_lines])\n",
    "# train_labels = [0 if x[1] == \"OTHER\" else 1 for x in train_lines]\n",
    "\n",
    "dev_sentences = [x[0] for x in dev_lines]\n",
    "dev_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in dev_lines])\n",
    "# dev_labels = [0 if x[1] == \"OTHER\" else 1 for x in dev_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples_per_epoch = len(train_sentences)\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "steps_per_epoch = math.ceil(samples_per_epoch / batch_size)\n",
    "checkpoint = ModelCheckpoint('best_classification_model.h5', \n",
    "                             monitor='val_acc', \n",
    "                             verbose = 1, \n",
    "                             save_best_only = True, \n",
    "                             save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "132/132 [==============================] - 23s 175ms/step - loss: 0.7501 - acc: 0.6001 - val_loss: 0.6500 - val_acc: 0.6547\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65470, saving model to best_classification_model.h5\n",
      "Epoch 2/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.6693 - acc: 0.6269 - val_loss: 0.6513 - val_acc: 0.6609\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.65470 to 0.66089, saving model to best_classification_model.h5\n",
      "Epoch 3/200\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.6635 - acc: 0.6392 - val_loss: 0.6501 - val_acc: 0.6621\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.66089 to 0.66213, saving model to best_classification_model.h5\n",
      "Epoch 4/200\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.6555 - acc: 0.6456 - val_loss: 0.6622 - val_acc: 0.6572\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.66213\n",
      "Epoch 5/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.6510 - acc: 0.6458 - val_loss: 0.6443 - val_acc: 0.6671\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.66213 to 0.66708, saving model to best_classification_model.h5\n",
      "Epoch 6/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.6499 - acc: 0.6503 - val_loss: 0.6338 - val_acc: 0.6646\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.66708\n",
      "Epoch 7/200\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.6400 - acc: 0.6579 - val_loss: 0.6371 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.66708\n",
      "Epoch 8/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.6378 - acc: 0.6584 - val_loss: 0.6339 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.66708\n",
      "Epoch 9/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.6361 - acc: 0.6579 - val_loss: 0.6284 - val_acc: 0.6646\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.66708\n",
      "Epoch 10/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.6347 - acc: 0.6676 - val_loss: 0.6252 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.66708\n",
      "Epoch 11/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.6293 - acc: 0.6641 - val_loss: 0.6157 - val_acc: 0.6696\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.66708 to 0.66955, saving model to best_classification_model.h5\n",
      "Epoch 12/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.6219 - acc: 0.6664 - val_loss: 0.6149 - val_acc: 0.6795\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.66955 to 0.67946, saving model to best_classification_model.h5\n",
      "Epoch 13/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.6191 - acc: 0.6735 - val_loss: 0.6056 - val_acc: 0.6745\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.67946\n",
      "Epoch 14/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.6114 - acc: 0.6766 - val_loss: 0.5961 - val_acc: 0.6918\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.67946 to 0.69183, saving model to best_classification_model.h5\n",
      "Epoch 15/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.6124 - acc: 0.6688 - val_loss: 0.5894 - val_acc: 0.7017\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.69183 to 0.70173, saving model to best_classification_model.h5\n",
      "Epoch 16/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.5989 - acc: 0.6830 - val_loss: 0.5843 - val_acc: 0.6968\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.70173\n",
      "Epoch 17/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.5962 - acc: 0.6849 - val_loss: 0.5758 - val_acc: 0.6980\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.70173\n",
      "Epoch 18/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.5874 - acc: 0.6908 - val_loss: 0.5696 - val_acc: 0.6931\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.70173\n",
      "Epoch 19/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.5688 - acc: 0.7057 - val_loss: 0.5674 - val_acc: 0.7005\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.70173\n",
      "Epoch 20/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.5726 - acc: 0.7083 - val_loss: 0.5564 - val_acc: 0.7054\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.70173 to 0.70545, saving model to best_classification_model.h5\n",
      "Epoch 21/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.5598 - acc: 0.7107 - val_loss: 0.5559 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.70545 to 0.71411, saving model to best_classification_model.h5\n",
      "Epoch 22/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.5625 - acc: 0.7161 - val_loss: 0.5539 - val_acc: 0.7129\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.71411\n",
      "Epoch 23/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.5538 - acc: 0.7133 - val_loss: 0.5601 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.71411\n",
      "Epoch 24/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.5515 - acc: 0.7192 - val_loss: 0.5528 - val_acc: 0.7030\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.71411\n",
      "Epoch 25/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.5457 - acc: 0.7270 - val_loss: 0.5612 - val_acc: 0.7178\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.71411 to 0.71782, saving model to best_classification_model.h5\n",
      "Epoch 26/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.5413 - acc: 0.7240 - val_loss: 0.5611 - val_acc: 0.7178\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.71782\n",
      "Epoch 27/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.5354 - acc: 0.7341 - val_loss: 0.5394 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.71782 to 0.73267, saving model to best_classification_model.h5\n",
      "Epoch 28/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.5234 - acc: 0.7353 - val_loss: 0.5467 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.73267\n",
      "Epoch 29/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.5291 - acc: 0.7341 - val_loss: 0.5458 - val_acc: 0.7351\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.73267 to 0.73515, saving model to best_classification_model.h5\n",
      "Epoch 30/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.5269 - acc: 0.7398 - val_loss: 0.5428 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.73515\n",
      "Epoch 31/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.5246 - acc: 0.7500 - val_loss: 0.5219 - val_acc: 0.7389\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.73515 to 0.73886, saving model to best_classification_model.h5\n",
      "Epoch 32/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.5198 - acc: 0.7441 - val_loss: 0.5148 - val_acc: 0.7351\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.73886\n",
      "Epoch 33/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.5163 - acc: 0.7481 - val_loss: 0.5114 - val_acc: 0.7475\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.73886 to 0.74752, saving model to best_classification_model.h5\n",
      "Epoch 34/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.5208 - acc: 0.7422 - val_loss: 0.5025 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.74752\n",
      "Epoch 35/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.5057 - acc: 0.7460 - val_loss: 0.5080 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.74752 to 0.75743, saving model to best_classification_model.h5\n",
      "Epoch 36/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.5168 - acc: 0.7493 - val_loss: 0.5025 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.75743\n",
      "Epoch 37/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.5128 - acc: 0.7431 - val_loss: 0.5023 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.75743\n",
      "Epoch 38/200\n",
      "132/132 [==============================] - 21s 155ms/step - loss: 0.5035 - acc: 0.7566 - val_loss: 0.5031 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.75743\n",
      "Epoch 39/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.5010 - acc: 0.7502 - val_loss: 0.5060 - val_acc: 0.7537\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.75743\n",
      "Epoch 40/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4968 - acc: 0.7580 - val_loss: 0.4986 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.75743\n",
      "Epoch 41/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.5008 - acc: 0.7543 - val_loss: 0.4946 - val_acc: 0.7587\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.75743 to 0.75866, saving model to best_classification_model.h5\n",
      "Epoch 42/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.5018 - acc: 0.7576 - val_loss: 0.4934 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.75866 to 0.76238, saving model to best_classification_model.h5\n",
      "Epoch 43/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.4922 - acc: 0.7545 - val_loss: 0.4929 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.76238\n",
      "Epoch 44/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.4812 - acc: 0.7585 - val_loss: 0.5084 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.76238\n",
      "Epoch 45/200\n",
      "132/132 [==============================] - 21s 155ms/step - loss: 0.4815 - acc: 0.7675 - val_loss: 0.4924 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.76238\n",
      "Epoch 46/200\n",
      "132/132 [==============================] - 21s 155ms/step - loss: 0.4877 - acc: 0.7685 - val_loss: 0.5101 - val_acc: 0.7537\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.76238\n",
      "Epoch 47/200\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.4839 - acc: 0.7768 - val_loss: 0.4994 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.76238\n",
      "Epoch 48/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.4815 - acc: 0.7706 - val_loss: 0.4943 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.76238\n",
      "Epoch 49/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.4714 - acc: 0.7699 - val_loss: 0.4945 - val_acc: 0.7488\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.76238\n",
      "Epoch 50/200\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.4691 - acc: 0.7782 - val_loss: 0.5019 - val_acc: 0.7426\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.76238\n",
      "Epoch 51/200\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.4688 - acc: 0.7706 - val_loss: 0.5055 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.76238\n",
      "Epoch 52/200\n",
      "132/132 [==============================] - 20s 151ms/step - loss: 0.4691 - acc: 0.7730 - val_loss: 0.5040 - val_acc: 0.7537\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.76238\n",
      "Epoch 53/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.4581 - acc: 0.7839 - val_loss: 0.4819 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.76238\n",
      "Epoch 54/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.4596 - acc: 0.7798 - val_loss: 0.4975 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.76238\n",
      "Epoch 55/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4598 - acc: 0.7808 - val_loss: 0.4858 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.76238\n",
      "Epoch 56/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.4642 - acc: 0.7782 - val_loss: 0.4823 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.76238\n",
      "Epoch 57/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.4566 - acc: 0.7824 - val_loss: 0.4982 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.76238\n",
      "Epoch 58/200\n",
      "132/132 [==============================] - 21s 157ms/step - loss: 0.4415 - acc: 0.7912 - val_loss: 0.4914 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.76238\n",
      "Epoch 59/200\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.4545 - acc: 0.7879 - val_loss: 0.4794 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.76238\n",
      "Epoch 60/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.4519 - acc: 0.7912 - val_loss: 0.4821 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.76238 to 0.76485, saving model to best_classification_model.h5\n",
      "Epoch 61/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.4515 - acc: 0.7841 - val_loss: 0.4820 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.76485 to 0.76980, saving model to best_classification_model.h5\n",
      "Epoch 62/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4456 - acc: 0.7834 - val_loss: 0.4817 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.76980\n",
      "Epoch 63/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4509 - acc: 0.7827 - val_loss: 0.4760 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.76980\n",
      "Epoch 64/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.4386 - acc: 0.8007 - val_loss: 0.4782 - val_acc: 0.7587\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.76980\n",
      "Epoch 65/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.4353 - acc: 0.7992 - val_loss: 0.4790 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.76980\n",
      "Epoch 66/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4453 - acc: 0.7917 - val_loss: 0.4924 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.76980\n",
      "Epoch 67/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4377 - acc: 0.7964 - val_loss: 0.4749 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.76980\n",
      "Epoch 68/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.4411 - acc: 0.7831 - val_loss: 0.4786 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.76980\n",
      "Epoch 69/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.4347 - acc: 0.7985 - val_loss: 0.4794 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.76980\n",
      "Epoch 70/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4312 - acc: 0.8030 - val_loss: 0.4826 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.76980\n",
      "Epoch 71/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.4232 - acc: 0.7995 - val_loss: 0.4758 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00071: val_acc improved from 0.76980 to 0.77104, saving model to best_classification_model.h5\n",
      "Epoch 72/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.4174 - acc: 0.7988 - val_loss: 0.4897 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.77104\n",
      "Epoch 73/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.4238 - acc: 0.8004 - val_loss: 0.4811 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.77104\n",
      "Epoch 74/200\n",
      "132/132 [==============================] - 21s 157ms/step - loss: 0.4227 - acc: 0.7971 - val_loss: 0.4844 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.77104\n",
      "Epoch 75/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4297 - acc: 0.7983 - val_loss: 0.4826 - val_acc: 0.7636\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.77104\n",
      "Epoch 76/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.4195 - acc: 0.8014 - val_loss: 0.5038 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.77104\n",
      "Epoch 77/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4157 - acc: 0.8002 - val_loss: 0.4865 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.77104\n",
      "Epoch 78/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4126 - acc: 0.8082 - val_loss: 0.4887 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.77104\n",
      "Epoch 79/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.4169 - acc: 0.8068 - val_loss: 0.4849 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.77104\n",
      "Epoch 80/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.4042 - acc: 0.8151 - val_loss: 0.4801 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.77104\n",
      "Epoch 81/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.4192 - acc: 0.8092 - val_loss: 0.4795 - val_acc: 0.7611\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.77104\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4125 - acc: 0.8002 - val_loss: 0.4824 - val_acc: 0.7636\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.77104\n",
      "Epoch 83/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4059 - acc: 0.8085 - val_loss: 0.4765 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.77104\n",
      "Epoch 84/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.4106 - acc: 0.8056 - val_loss: 0.4751 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.77104 to 0.77228, saving model to best_classification_model.h5\n",
      "Epoch 85/200\n",
      "132/132 [==============================] - 21s 158ms/step - loss: 0.4029 - acc: 0.8101 - val_loss: 0.4891 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.77228\n",
      "Epoch 86/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.3993 - acc: 0.8170 - val_loss: 0.4839 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00086: val_acc improved from 0.77228 to 0.77599, saving model to best_classification_model.h5\n",
      "Epoch 87/200\n",
      "132/132 [==============================] - 21s 159ms/step - loss: 0.4005 - acc: 0.8108 - val_loss: 0.4830 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00087: val_acc improved from 0.77599 to 0.77970, saving model to best_classification_model.h5\n",
      "Epoch 88/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.4030 - acc: 0.8146 - val_loss: 0.4774 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.77970\n",
      "Epoch 89/200\n",
      "132/132 [==============================] - 21s 155ms/step - loss: 0.3983 - acc: 0.8156 - val_loss: 0.4962 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.77970\n",
      "Epoch 90/200\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.4084 - acc: 0.8113 - val_loss: 0.4763 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.77970\n",
      "Epoch 91/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.3936 - acc: 0.8172 - val_loss: 0.4918 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.77970\n",
      "Epoch 92/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.3856 - acc: 0.8217 - val_loss: 0.5022 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.77970\n",
      "Epoch 93/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.3881 - acc: 0.8156 - val_loss: 0.5027 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.77970\n",
      "Epoch 94/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.3911 - acc: 0.8239 - val_loss: 0.4805 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.77970\n",
      "Epoch 95/200\n",
      "132/132 [==============================] - 21s 156ms/step - loss: 0.3799 - acc: 0.8307 - val_loss: 0.5130 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.77970\n",
      "Epoch 96/200\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.3793 - acc: 0.8215 - val_loss: 0.4895 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.77970\n",
      "Epoch 97/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.3902 - acc: 0.8156 - val_loss: 0.4774 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.77970\n",
      "Epoch 98/200\n",
      "132/132 [==============================] - 20s 149ms/step - loss: 0.3851 - acc: 0.8227 - val_loss: 0.4773 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.77970\n",
      "Epoch 99/200\n",
      "132/132 [==============================] - 20s 148ms/step - loss: 0.3811 - acc: 0.8172 - val_loss: 0.5012 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.77970\n",
      "Epoch 100/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.3714 - acc: 0.8312 - val_loss: 0.4883 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00100: val_acc improved from 0.77970 to 0.78713, saving model to best_classification_model.h5\n",
      "Epoch 101/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.3788 - acc: 0.8243 - val_loss: 0.4888 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.78713\n",
      "Epoch 102/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.3778 - acc: 0.8272 - val_loss: 0.4897 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.78713\n",
      "Epoch 103/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.3734 - acc: 0.8300 - val_loss: 0.5088 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.78713\n",
      "Epoch 104/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.3633 - acc: 0.8348 - val_loss: 0.5080 - val_acc: 0.7834\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.78713\n",
      "Epoch 105/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.3736 - acc: 0.8272 - val_loss: 0.5177 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.78713\n",
      "Epoch 106/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.3744 - acc: 0.8295 - val_loss: 0.5106 - val_acc: 0.7809\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.78713\n",
      "Epoch 107/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.3656 - acc: 0.8366 - val_loss: 0.5258 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.78713\n",
      "Epoch 108/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.3600 - acc: 0.8364 - val_loss: 0.5118 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.78713\n",
      "Epoch 109/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.3666 - acc: 0.8364 - val_loss: 0.5194 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.78713\n",
      "Epoch 110/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.3603 - acc: 0.8407 - val_loss: 0.5053 - val_acc: 0.7809\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.78713\n",
      "Epoch 111/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.3603 - acc: 0.8326 - val_loss: 0.5136 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.78713\n",
      "Epoch 112/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.3403 - acc: 0.8397 - val_loss: 0.5124 - val_acc: 0.7785\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.78713\n",
      "Epoch 113/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.3637 - acc: 0.8329 - val_loss: 0.5211 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.78713\n",
      "Epoch 114/200\n",
      "132/132 [==============================] - 21s 155ms/step - loss: 0.3605 - acc: 0.8355 - val_loss: 0.5204 - val_acc: 0.7785\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.78713\n",
      "Epoch 115/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.3520 - acc: 0.8402 - val_loss: 0.5409 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.78713\n",
      "Epoch 116/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.3457 - acc: 0.8480 - val_loss: 0.5148 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.78713\n",
      "Epoch 117/200\n",
      "132/132 [==============================] - 21s 157ms/step - loss: 0.3349 - acc: 0.8520 - val_loss: 0.5185 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.78713\n",
      "Epoch 118/200\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.3403 - acc: 0.8456 - val_loss: 0.5306 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.78713\n",
      "Epoch 119/200\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.3520 - acc: 0.8452 - val_loss: 0.5597 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.78713\n",
      "Epoch 120/200\n",
      "132/132 [==============================] - 20s 151ms/step - loss: 0.3362 - acc: 0.8461 - val_loss: 0.5346 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.78713\n",
      "Epoch 121/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.3361 - acc: 0.8504 - val_loss: 0.5239 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.78713\n",
      "Epoch 122/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.3447 - acc: 0.8409 - val_loss: 0.5558 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.78713\n",
      "Epoch 123/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.3378 - acc: 0.8485 - val_loss: 0.5376 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.78713\n",
      "Epoch 124/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 19s 145ms/step - loss: 0.3330 - acc: 0.8501 - val_loss: 0.5792 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.78713\n",
      "Epoch 125/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.3451 - acc: 0.8482 - val_loss: 0.5318 - val_acc: 0.7859\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.78713\n",
      "Epoch 126/200\n",
      "132/132 [==============================] - 19s 147ms/step - loss: 0.3174 - acc: 0.8570 - val_loss: 0.5555 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.78713\n",
      "Epoch 127/200\n",
      "132/132 [==============================] - 19s 147ms/step - loss: 0.3309 - acc: 0.8494 - val_loss: 0.5391 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.78713\n",
      "Epoch 128/200\n",
      "132/132 [==============================] - 20s 151ms/step - loss: 0.3357 - acc: 0.8487 - val_loss: 0.5363 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.78713\n",
      "Epoch 129/200\n",
      "132/132 [==============================] - 20s 149ms/step - loss: 0.3344 - acc: 0.8485 - val_loss: 0.5621 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.78713\n",
      "Epoch 130/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.3345 - acc: 0.8475 - val_loss: 0.5343 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.78713\n",
      "Epoch 131/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.3230 - acc: 0.8556 - val_loss: 0.5342 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.78713\n",
      "Epoch 132/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.3222 - acc: 0.8565 - val_loss: 0.5576 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.78713\n",
      "Epoch 133/200\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.3374 - acc: 0.8504 - val_loss: 0.5518 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.78713\n",
      "Epoch 134/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.3249 - acc: 0.8499 - val_loss: 0.5292 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.78713\n",
      "Epoch 135/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.3286 - acc: 0.8587 - val_loss: 0.5553 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.78713\n",
      "Epoch 136/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.3105 - acc: 0.8651 - val_loss: 0.5710 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.78713\n",
      "Epoch 137/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.3194 - acc: 0.8537 - val_loss: 0.5437 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.78713\n",
      "Epoch 138/200\n",
      "132/132 [==============================] - 19s 143ms/step - loss: 0.3143 - acc: 0.8575 - val_loss: 0.5313 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.78713\n",
      "Epoch 139/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.3051 - acc: 0.8660 - val_loss: 0.5591 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.78713\n",
      "Epoch 140/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.3161 - acc: 0.8632 - val_loss: 0.5865 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.78713\n",
      "Epoch 141/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.3100 - acc: 0.8660 - val_loss: 0.5724 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.78713\n",
      "Epoch 142/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.3057 - acc: 0.8632 - val_loss: 0.5717 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.78713\n",
      "Epoch 143/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.3097 - acc: 0.8665 - val_loss: 0.5808 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.78713\n",
      "Epoch 144/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.2965 - acc: 0.8648 - val_loss: 0.5933 - val_acc: 0.7785\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.78713\n",
      "Epoch 145/200\n",
      "132/132 [==============================] - 19s 143ms/step - loss: 0.2978 - acc: 0.8681 - val_loss: 0.6116 - val_acc: 0.7785\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.78713\n",
      "Epoch 146/200\n",
      "132/132 [==============================] - 19s 143ms/step - loss: 0.2910 - acc: 0.8714 - val_loss: 0.6036 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.78713\n",
      "Epoch 147/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.3103 - acc: 0.8636 - val_loss: 0.5994 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.78713\n",
      "Epoch 148/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2952 - acc: 0.8703 - val_loss: 0.6230 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.78713\n",
      "Epoch 149/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.3050 - acc: 0.8629 - val_loss: 0.6149 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.78713\n",
      "Epoch 150/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2811 - acc: 0.8757 - val_loss: 0.5877 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.78713\n",
      "Epoch 151/200\n",
      "132/132 [==============================] - 19s 143ms/step - loss: 0.2946 - acc: 0.8752 - val_loss: 0.6094 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.78713\n",
      "Epoch 152/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.2925 - acc: 0.8724 - val_loss: 0.6057 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.78713\n",
      "Epoch 153/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.2870 - acc: 0.8724 - val_loss: 0.6082 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.78713\n",
      "Epoch 154/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2820 - acc: 0.8710 - val_loss: 0.6168 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.78713\n",
      "Epoch 155/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2932 - acc: 0.8795 - val_loss: 0.5531 - val_acc: 0.7636\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.78713\n",
      "Epoch 156/200\n",
      "132/132 [==============================] - 19s 147ms/step - loss: 0.2728 - acc: 0.8783 - val_loss: 0.5930 - val_acc: 0.7587\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.78713\n",
      "Epoch 157/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2796 - acc: 0.8807 - val_loss: 0.5812 - val_acc: 0.7636\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.78713\n",
      "Epoch 158/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2802 - acc: 0.8823 - val_loss: 0.5943 - val_acc: 0.7611\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.78713\n",
      "Epoch 159/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.2764 - acc: 0.8797 - val_loss: 0.6087 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.78713\n",
      "Epoch 160/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.2715 - acc: 0.8823 - val_loss: 0.6105 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.78713\n",
      "Epoch 161/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.2719 - acc: 0.8845 - val_loss: 0.5814 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.78713\n",
      "Epoch 162/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.2698 - acc: 0.8859 - val_loss: 0.5783 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.78713\n",
      "Epoch 163/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2610 - acc: 0.8892 - val_loss: 0.5916 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.78713\n",
      "Epoch 164/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2781 - acc: 0.8795 - val_loss: 0.5533 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.78713\n",
      "Epoch 165/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2820 - acc: 0.8800 - val_loss: 0.5766 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.78713\n",
      "Epoch 166/200\n",
      "132/132 [==============================] - 19s 147ms/step - loss: 0.2700 - acc: 0.8793 - val_loss: 0.5404 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.78713\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2602 - acc: 0.8887 - val_loss: 0.5919 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.78713\n",
      "Epoch 168/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2541 - acc: 0.8909 - val_loss: 0.6206 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.78713\n",
      "Epoch 169/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.2506 - acc: 0.8928 - val_loss: 0.6008 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.78713\n",
      "Epoch 170/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.2647 - acc: 0.8835 - val_loss: 0.5996 - val_acc: 0.7636\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.78713\n",
      "Epoch 171/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.2612 - acc: 0.8859 - val_loss: 0.6217 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.78713\n",
      "Epoch 172/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.2562 - acc: 0.8878 - val_loss: 0.5843 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.78713\n",
      "Epoch 173/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2479 - acc: 0.8920 - val_loss: 0.6371 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.78713\n",
      "Epoch 174/200\n",
      "132/132 [==============================] - 20s 148ms/step - loss: 0.2569 - acc: 0.8897 - val_loss: 0.6596 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.78713\n",
      "Epoch 175/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.2536 - acc: 0.8911 - val_loss: 0.6188 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.78713\n",
      "Epoch 176/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.2421 - acc: 0.8925 - val_loss: 0.6422 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.78713\n",
      "Epoch 177/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2562 - acc: 0.8909 - val_loss: 0.6251 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.78713\n",
      "Epoch 178/200\n",
      "132/132 [==============================] - 20s 148ms/step - loss: 0.2458 - acc: 0.8932 - val_loss: 0.6555 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.78713\n",
      "Epoch 179/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.2479 - acc: 0.8911 - val_loss: 0.6489 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.78713\n",
      "Epoch 180/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2395 - acc: 0.9029 - val_loss: 0.6063 - val_acc: 0.7636\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.78713\n",
      "Epoch 181/200\n",
      "132/132 [==============================] - 19s 147ms/step - loss: 0.2486 - acc: 0.8973 - val_loss: 0.6559 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.78713\n",
      "Epoch 182/200\n",
      "132/132 [==============================] - 19s 144ms/step - loss: 0.2423 - acc: 0.8980 - val_loss: 0.6826 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.78713\n",
      "Epoch 183/200\n",
      "132/132 [==============================] - 20s 149ms/step - loss: 0.2394 - acc: 0.8984 - val_loss: 0.7804 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.78713\n",
      "Epoch 184/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2384 - acc: 0.9036 - val_loss: 0.6319 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.78713\n",
      "Epoch 185/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.2445 - acc: 0.8994 - val_loss: 0.6232 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.78713\n",
      "Epoch 186/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2386 - acc: 0.8984 - val_loss: 0.6561 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.78713\n",
      "Epoch 187/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2449 - acc: 0.8973 - val_loss: 0.6965 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.78713\n",
      "Epoch 188/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.2295 - acc: 0.9003 - val_loss: 0.6459 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.78713\n",
      "Epoch 189/200\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.2194 - acc: 0.9077 - val_loss: 0.5807 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.78713\n",
      "Epoch 190/200\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.2327 - acc: 0.9077 - val_loss: 0.7186 - val_acc: 0.7735\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.78713\n",
      "Epoch 191/200\n",
      "132/132 [==============================] - 20s 149ms/step - loss: 0.2175 - acc: 0.9115 - val_loss: 0.6826 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.78713\n",
      "Epoch 192/200\n",
      "132/132 [==============================] - 19s 147ms/step - loss: 0.2286 - acc: 0.9065 - val_loss: 0.6627 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.78713\n",
      "Epoch 193/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.2178 - acc: 0.9067 - val_loss: 0.6942 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.78713\n",
      "Epoch 194/200\n",
      "132/132 [==============================] - 20s 151ms/step - loss: 0.2443 - acc: 0.9010 - val_loss: 0.6379 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.78713\n",
      "Epoch 195/200\n",
      "132/132 [==============================] - 19s 147ms/step - loss: 0.2332 - acc: 0.9015 - val_loss: 0.6827 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.78713\n",
      "Epoch 196/200\n",
      "132/132 [==============================] - 19s 146ms/step - loss: 0.2299 - acc: 0.9079 - val_loss: 0.6377 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.78713\n",
      "Epoch 197/200\n",
      "132/132 [==============================] - 20s 148ms/step - loss: 0.2079 - acc: 0.9145 - val_loss: 0.6949 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.78713\n",
      "Epoch 198/200\n",
      "132/132 [==============================] - 19s 147ms/step - loss: 0.2221 - acc: 0.9029 - val_loss: 0.7095 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.78713\n",
      "Epoch 199/200\n",
      "132/132 [==============================] - 20s 148ms/step - loss: 0.2150 - acc: 0.9134 - val_loss: 0.6706 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.78713\n",
      "Epoch 200/200\n",
      "132/132 [==============================] - 20s 148ms/step - loss: 0.2233 - acc: 0.9124 - val_loss: 0.6529 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.78713\n"
     ]
    }
   ],
   "source": [
    "dev_batch_size = len(dev_sentences)\n",
    "final_model.fit_generator(\n",
    "    sequential_generator('/home/gwiedemann/notebooks/OffLang/sample_train.txt', batch_size), \n",
    "    steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
    "    validation_data = sequential_generator('/home/gwiedemann/notebooks/OffLang/sample_dev.txt', dev_batch_size),\n",
    "    validation_steps = math.ceil(len(dev_sentences) / dev_batch_size),\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "final_model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx2Label={}\n",
    "for key, value in label2Idx.items():\n",
    "    idx2Label[value]=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def return_predictions(filename, model):\n",
    "    gold_file = open('/home/jindal/notebooks/jindal/NER/language_model/gold_pred_simple.txt','w')\n",
    "    pred_file = open('/home/jindal/notebooks/jindal/NER/language_model/pred_simple.txt','w')\n",
    "    true_labels=[]\n",
    "    pred_labels=[]\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "#             print(line.split('\\t')[0], end=' ')\n",
    "            word_embeddings=[]\n",
    "            case_embeddings=[]\n",
    "            char_embeddings=[]\n",
    "            if len(line.split('\\t'))!=3:\n",
    "                    continue\n",
    "            text, simple_label, complex_label = line.split('\\t')\n",
    "            to_write_gold = text+'\\t'+simple_label+'\\t'+simple_label+'\\n'\n",
    "            to_write_pred = text\n",
    "            temp_casing = []\n",
    "            temp_char=[]\n",
    "            temp_word=[]\n",
    "            for word in text.split():\n",
    "                casing =getCasing(word, case2Idx)\n",
    "                temp_casing.append(casing)\n",
    "                temp_char2=[]\n",
    "                for char in word:\n",
    "                    temp_char2.append(char2Idx[char])\n",
    "                temp_char2 = np.array(temp_char2)\n",
    "                temp_char.append(temp_char2)\n",
    "                word_vector = ft.get_word_vector(word.lower())\n",
    "                temp_word.append(word_vector)\n",
    "            temp_char = pad_sequences(temp_char, 52)\n",
    "            word_embeddings.append(temp_word)\n",
    "            case_embeddings.append(temp_casing)\n",
    "            char_embeddings.append(temp_char)\n",
    "    #         output_labels.append(label2Idx[label_simple])\n",
    "            inp= [np.array([temp_word]), np.array([temp_casing]), np.array([temp_char])]\n",
    "    #         yield ([np.array(word_embeddings), np.array(case_embeddings), np.array(char_embeddings)], to_categorical(output_labels, len(label2Idx)))\n",
    "    #         print(final_model.predict_classes(inp))\n",
    "            prediction = model.predict(inp,batch_size=1)[0].argmax(-1)\n",
    "            to_write_pred = to_write_pred + '\\t'+ idx2Label[prediction]+'\\t'+ idx2Label[prediction]+'\\n'\n",
    "            correct = label2Idx[simple_label]\n",
    "#             print(prediction,correct)\n",
    "            gold_file.write(to_write_gold)\n",
    "            pred_file.write(to_write_pred)\n",
    "            pred_labels.append(prediction)\n",
    "            true_labels.append(correct)\n",
    "#             print(idx2Label[final_model.predict(inp, batch_size=1)[0].argmax(-1)])\n",
    "    #         y_classes = y_prob.argmax(axis=-1)\n",
    "        gold_file.close()\n",
    "        pred_file.close()\n",
    "        return pred_labels, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model.load_weights('model_with_pretraining.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7266464364799474"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred , correct = return_predictions('/home/gwiedemann/notebooks/OffLang/sample_dev.txt',final_model)\n",
    "\n",
    "sklearn.metrics.f1_score(correct, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making model without pretraining to compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_input = Input(shape=(None,300), dtype='float32',name='words_input')\n",
    "# words = Embedding(input_dim =50, output_dim=300, trainable=False)(words_input)\n",
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= Dropout(0.5, name='dropout1')(embed_char_out)\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, name='conv'))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(52), name='maxpool')(conv1d_out)\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "output = concatenate([words_input, char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=False, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "# output = TimeDistributed(Dense(vocab_size))(output)\n",
    "# crf = CRF(len(label2Idx))\n",
    "# output = crf(output)\n",
    "# output = Flatten()(output)\n",
    "output = Dense(len(label2Idx), activation = 'softmax')(output)\n",
    "# output = Flatten()(output)\n",
    "# output = Dense(vocab_size, activation='softmax')(output)\n",
    "# model.add(Dense(vocab_size, activation='softmax'))\n",
    "model_without_pre = Model(inputs=[words_input,casing_input, character_input], outputs=[output])\n",
    "model_without_pre.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.6346 - acc: 0.6621\n",
      "Epoch 00001: val_acc improved from -inf to 0.71747, saving model to model_no_pretrain.h5\n",
      "4200/4200 [==============================] - 435s 104ms/step - loss: 0.6348 - acc: 0.6619 - val_loss: 0.5662 - val_acc: 0.7175\n",
      "Epoch 2/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.5695 - acc: 0.7123\n",
      "Epoch 00002: val_acc improved from 0.71747 to 0.73234, saving model to model_no_pretrain.h5\n",
      "4200/4200 [==============================] - 428s 102ms/step - loss: 0.5695 - acc: 0.7124 - val_loss: 0.5208 - val_acc: 0.7323\n",
      "Epoch 3/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.5247 - acc: 0.7440\n",
      "Epoch 00003: val_acc improved from 0.73234 to 0.76828, saving model to model_no_pretrain.h5\n",
      "4200/4200 [==============================] - 432s 103ms/step - loss: 0.5249 - acc: 0.7438 - val_loss: 0.4968 - val_acc: 0.7683\n",
      "Epoch 4/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.4927 - acc: 0.7633\n",
      "Epoch 00004: val_acc did not improve\n",
      "4200/4200 [==============================] - 428s 102ms/step - loss: 0.4927 - acc: 0.7633 - val_loss: 0.5105 - val_acc: 0.7596\n",
      "Epoch 5/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.5004 - acc: 0.7590\n",
      "Epoch 00005: val_acc did not improve\n",
      "4200/4200 [==============================] - 430s 102ms/step - loss: 0.5004 - acc: 0.7590 - val_loss: 0.5023 - val_acc: 0.7633\n",
      "Epoch 6/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.4959 - acc: 0.7647\n",
      "Epoch 00006: val_acc did not improve\n",
      "4200/4200 [==============================] - 427s 102ms/step - loss: 0.4959 - acc: 0.7648 - val_loss: 0.5208 - val_acc: 0.7323\n",
      "Epoch 7/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.4612 - acc: 0.7778\n",
      "Epoch 00007: val_acc improved from 0.76828 to 0.77323, saving model to model_no_pretrain.h5\n",
      "4200/4200 [==============================] - 430s 102ms/step - loss: 0.4612 - acc: 0.7779 - val_loss: 0.4878 - val_acc: 0.7732\n",
      "Epoch 8/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.7833\n",
      "Epoch 00008: val_acc did not improve\n",
      "4200/4200 [==============================] - 428s 102ms/step - loss: 0.4605 - acc: 0.7833 - val_loss: 0.4991 - val_acc: 0.7732\n",
      "Epoch 9/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.7799\n",
      "Epoch 00009: val_acc improved from 0.77323 to 0.77695, saving model to model_no_pretrain.h5\n",
      "4200/4200 [==============================] - 427s 102ms/step - loss: 0.4596 - acc: 0.7798 - val_loss: 0.4990 - val_acc: 0.7770\n",
      "Epoch 10/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.4523 - acc: 0.7890\n",
      "Epoch 00010: val_acc did not improve\n",
      "4200/4200 [==============================] - 429s 102ms/step - loss: 0.4523 - acc: 0.7890 - val_loss: 0.5093 - val_acc: 0.7670\n",
      "Epoch 11/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.4566 - acc: 0.7838\n",
      "Epoch 00011: val_acc did not improve\n",
      "4200/4200 [==============================] - 426s 101ms/step - loss: 0.4567 - acc: 0.7836 - val_loss: 0.5292 - val_acc: 0.7633\n",
      "Epoch 12/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.4329 - acc: 0.8033\n",
      "Epoch 00012: val_acc did not improve\n",
      "4200/4200 [==============================] - 429s 102ms/step - loss: 0.4328 - acc: 0.8033 - val_loss: 0.5721 - val_acc: 0.7683\n",
      "Epoch 13/30\n",
      "4199/4200 [============================>.] - ETA: 0s - loss: 0.4433 - acc: 0.7973\n",
      "Epoch 00013: val_acc did not improve\n",
      "4200/4200 [==============================] - 429s 102ms/step - loss: 0.4432 - acc: 0.7974 - val_loss: 0.5353 - val_acc: 0.7757\n",
      "Epoch 14/30\n",
      "  81/4200 [..............................] - ETA: 6:26 - loss: 0.4068 - acc: 0.8519"
     ]
    }
   ],
   "source": [
    "line_number=0\n",
    "line_number_validation=0\n",
    "# training_file_name = '/home/jindal/notebooks/jindal/NER/language_model/germeval2018.training_simple_labels.txt'\n",
    "training_file_name = '/home/gwiedemann/notebooks/OffLang/sample_train.txt'\n",
    "dev_file_name = '/home/gwiedemann/notebooks/OffLang/sample_dev.txt'\n",
    "checkpoint = ModelCheckpoint('model_no_pretrain.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "model_without_pre.fit_generator(my_generator(training_file_name, 1)\n",
    "                         , epochs =30 , steps_per_epoch = file_len(training_file_name)//1, validation_data=validation_data_generator(dev_file_name, 1),\n",
    "                         validation_steps=807, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6511627906976744"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred , correct = return_predictions('/home/gwiedemann/notebooks/OffLang/sample_dev.txt',model_without_pre)\n",
    "\n",
    "sklearn.metrics.f1_score(correct, pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OTHER': 0, 'OFFENSE': 1}\n"
     ]
    }
   ],
   "source": [
    "print(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
