{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jindal/miniconda3/envs/NER2/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import fastText\n",
    "import math\n",
    "import numpy as np \n",
    "from numpy import random\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Conv1D, Conv2D, Dropout, MaxPooling1D, GlobalMaxPooling1D, Bidirectional, Input, Masking, Flatten, Concatenate\n",
    "from keras import regularizers\n",
    "import os\n",
    "import re\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_number=0\n",
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jindal/notebooks/jindal/NER/language_model'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home/jindal/notebooks/fastText/wiki.de.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()\n",
    "nb_sequence_length = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = [\"mein\", \"dein\", \"unser\", \"mein\"]\n",
    "# for w in words:\n",
    "#     if w in word_vectors:\n",
    "#         print(\"I know \" + w)\n",
    "#     else:\n",
    "#         wv = ft.get_word_vector(w)\n",
    "#         print(wv)\n",
    "#         word_vectors[w] = wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(nb_embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ft.get_word_vector(\"mein\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator(features, labels, batch_size):\n",
    "    \n",
    "    global line_number, lock\n",
    "    batch_features = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims)) # initializing features with zeros\n",
    "    batch_labels = np.zeros((batch_size, 2)) # 2 as one hot\n",
    "\n",
    "    while True:\n",
    "        # print(len(features))\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            lock.acquire()\n",
    "            index = line_number%len(features)\n",
    "            line_number+=1\n",
    "            lock.release()\n",
    "#             index = random.choice(len(features), 1)[0]\n",
    "#             print(index)\n",
    "            batch_features[i] = process_features(features[index], nb_sequence_length, nb_embedding_dims)\n",
    "            # print(batch_features[i])\n",
    "            # print(batch_features[i].shape)\n",
    "            batch_labels[i] = labels[index]\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {}\n",
    "def process_features(textline, nb_sequence_length, nb_embedding_dims): # given a sentence, returns the embedding for it of fixed size\n",
    "    words = re.compile('[\\w-]+|[\\W ]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    # print(words)\n",
    "    features = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors:\n",
    "            wv = word_vectors[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            word_vectors[w] = wv\n",
    "        features[idx] = wv\n",
    "        # print(str(idx) + \" \" + w)\n",
    "        idx = idx + 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [line.strip().split(\"\\t\") for line in open('/home/jindal/notebooks/Resources/OffLang/sample_train.txt', encoding = \"UTF-8\")]\n",
    "dev_lines = [line.strip().split(\"\\t\") for line in open('/home/jindal/notebooks/Resources/OffLang/sample_dev.txt', encoding = \"UTF-8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_sentences = [x[0] for x in train_lines]\n",
    "train_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in train_lines])\n",
    "# train_labels = [0 if x[1] == \"OTHER\" else 1 for x in train_lines]\n",
    "\n",
    "dev_sentences = [x[0] for x in dev_lines]\n",
    "dev_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in dev_lines])\n",
    "# dev_labels = [0 if x[1] == \"OTHER\" else 1 for x in dev_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, recurrent_dropout = 0.5, dropout = 0.5, activation = 'relu', input_shape=(nb_sequence_length, nb_embedding_dims)),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation = 'softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     Conv1D(128, kernel_size = 3, padding = 'valid', input_shape=(nb_sequence_length, nb_embedding_dims), activation = 'relu'),\n",
    "#     MaxPooling1D(5),\n",
    "#     Flatten(),\n",
    "#     Dense(64, activation = 'relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(2, activation = 'softmax')\n",
    "# ])\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = (3, 4, 5)\n",
    "model_input = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "model_layers = Dropout(0.8)(model_input)\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Conv1D(filters = 100,\n",
    "                         kernel_size = sz,\n",
    "                         padding = \"valid\",\n",
    "                         activation = \"relu\",\n",
    "                         strides = 1,\n",
    "                 kernel_regularizer = regularizers.l2(0.0001))(model_layers)\n",
    "    conv = GlobalMaxPooling1D()(conv)\n",
    "    # conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "model_concatenated = Concatenate()(conv_blocks)\n",
    "model_concatenated = Dropout(0.8)(model_concatenated)\n",
    "model_concatenated = Dense(64, activation = \"relu\")(model_concatenated)\n",
    "model_output = Dense(2, activation = \"softmax\")(model_concatenated)\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_epoch = len(train_sentences)\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "steps_per_epoch = math.ceil(samples_per_epoch / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 10/132 [=>............................] - ETA: 1s - loss: 0.6908 - acc: 0.6281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jindal/miniconda3/envs/NER2/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \"\"\"\n",
      "/home/jindal/miniconda3/envs/NER2/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=132, validation_steps=26, validation_data=<generator..., epochs=50)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 1s 11ms/step - loss: 0.7128 - acc: 0.6423 - val_loss: 0.7287 - val_acc: 0.6394\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 2s 11ms/step - loss: 0.6942 - acc: 0.6671 - val_loss: 0.7049 - val_acc: 0.6803\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6914 - acc: 0.6624 - val_loss: 0.7035 - val_acc: 0.6791\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6861 - acc: 0.6681 - val_loss: 0.6996 - val_acc: 0.6779\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6979 - acc: 0.6494 - val_loss: 0.7017 - val_acc: 0.6611\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6881 - acc: 0.6648 - val_loss: 0.6962 - val_acc: 0.6719\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6840 - acc: 0.6626 - val_loss: 0.6855 - val_acc: 0.6767\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6897 - acc: 0.6600 - val_loss: 0.6835 - val_acc: 0.6779\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6850 - acc: 0.6617 - val_loss: 0.6879 - val_acc: 0.6550\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6769 - acc: 0.6702 - val_loss: 0.6757 - val_acc: 0.6791\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6804 - acc: 0.6638 - val_loss: 0.6881 - val_acc: 0.6587\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6703 - acc: 0.6705 - val_loss: 0.6669 - val_acc: 0.6695\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 2s 13ms/step - loss: 0.6783 - acc: 0.6536 - val_loss: 0.6795 - val_acc: 0.6454\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.6837 - acc: 0.6541 - val_loss: 0.6726 - val_acc: 0.6779\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.6636 - acc: 0.6714 - val_loss: 0.6613 - val_acc: 0.6791\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6669 - acc: 0.6669 - val_loss: 0.6621 - val_acc: 0.6550\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.6716 - acc: 0.6626 - val_loss: 0.6548 - val_acc: 0.6599\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6657 - acc: 0.6593 - val_loss: 0.6713 - val_acc: 0.6587\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 2s 11ms/step - loss: 0.6707 - acc: 0.6605 - val_loss: 0.6454 - val_acc: 0.6947\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.6644 - acc: 0.6709 - val_loss: 0.6559 - val_acc: 0.6587\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6613 - acc: 0.6615 - val_loss: 0.6445 - val_acc: 0.6755\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6602 - acc: 0.6615 - val_loss: 0.6452 - val_acc: 0.6671\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6634 - acc: 0.6605 - val_loss: 0.6521 - val_acc: 0.6647\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6607 - acc: 0.6671 - val_loss: 0.6564 - val_acc: 0.6695\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6662 - acc: 0.6631 - val_loss: 0.6486 - val_acc: 0.6791\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 2s 13ms/step - loss: 0.6605 - acc: 0.6645 - val_loss: 0.6425 - val_acc: 0.6875\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6591 - acc: 0.6851 - val_loss: 0.6365 - val_acc: 0.6911\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6634 - acc: 0.6863 - val_loss: 0.6497 - val_acc: 0.6875\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 2s 13ms/step - loss: 0.6601 - acc: 0.6726 - val_loss: 0.6447 - val_acc: 0.6983\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.6672 - acc: 0.6735 - val_loss: 0.6607 - val_acc: 0.7139\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6669 - acc: 0.6723 - val_loss: 0.6334 - val_acc: 0.6983\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.6596 - acc: 0.6795 - val_loss: 0.6441 - val_acc: 0.6959\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6666 - acc: 0.6771 - val_loss: 0.6589 - val_acc: 0.6947\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 2s 11ms/step - loss: 0.6584 - acc: 0.6792 - val_loss: 0.6486 - val_acc: 0.6935\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6613 - acc: 0.6927 - val_loss: 0.6202 - val_acc: 0.7332\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6693 - acc: 0.6901 - val_loss: 0.6605 - val_acc: 0.6851\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.6625 - acc: 0.6830 - val_loss: 0.6318 - val_acc: 0.7163\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.6700 - acc: 0.6821 - val_loss: 0.6472 - val_acc: 0.7175\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6664 - acc: 0.6948 - val_loss: 0.6313 - val_acc: 0.7200\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6577 - acc: 0.6927 - val_loss: 0.6494 - val_acc: 0.7019\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.6627 - acc: 0.6877 - val_loss: 0.6376 - val_acc: 0.7188\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6661 - acc: 0.6925 - val_loss: 0.6476 - val_acc: 0.6983\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 2s 11ms/step - loss: 0.6619 - acc: 0.6979 - val_loss: 0.6301 - val_acc: 0.7416\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6619 - acc: 0.7012 - val_loss: 0.6487 - val_acc: 0.6995\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.6665 - acc: 0.6896 - val_loss: 0.6377 - val_acc: 0.7344\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 1s 10ms/step - loss: 0.6622 - acc: 0.6944 - val_loss: 0.6382 - val_acc: 0.7188\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6591 - acc: 0.7055 - val_loss: 0.6181 - val_acc: 0.7368\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6682 - acc: 0.6911 - val_loss: 0.6368 - val_acc: 0.7067\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6581 - acc: 0.7034 - val_loss: 0.6484 - val_acc: 0.7308\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6580 - acc: 0.7041 - val_loss: 0.6422 - val_acc: 0.7019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f255f7c34e0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    my_generator(train_sentences, train_labels, batch_size), \n",
    "    steps_per_epoch=steps_per_epoch, nb_epoch=epochs,\n",
    "    validation_data = my_generator(dev_sentences, dev_labels, batch_size),\n",
    "    validation_steps = math.ceil(len(dev_sentences) / batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
