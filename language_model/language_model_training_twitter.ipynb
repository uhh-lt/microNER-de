{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '/home/jindal/notebooks/jindal/NER')\n",
    "import fastText\n",
    "import numpy as np \n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import sklearn\n",
    "import pickle, threading\n",
    "from keras.utils import to_categorical\n",
    "import linecache\n",
    "from keras.callbacks import Callback\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsSaver(Callback):\n",
    "    def __init__(self, model, N):\n",
    "        self.model = model\n",
    "        self.N = N\n",
    "        self.batch = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if self.batch % self.N == 0:\n",
    "            name = 'german_lm.h5'\n",
    "#             print(\"model saved %s\" %self.batch)\n",
    "            self.model.save_weights(name)\n",
    "        self.batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home/jindal/notebooks/fastText/wiki.de.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(sentences, word2Idx, case2Idx, char2Idx):\n",
    "    #{'numeric': 0, 'allLower': 1, 'contains_digit': 6, 'PADDING_TOKEN': 7, 'other': 4, 'allUpper': 2, 'mainly_numeric': 5, 'initialUpper': 3}\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "#         labelIndices = []\n",
    "        \n",
    "        for word,char in sentence:  \n",
    "            wordCount += 1\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]                 \n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                charIdx.append(char2Idx[x])\n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "#             labelIndices.append(label2Idx[label])\n",
    "           \n",
    "        dataset.append([wordIndices,caseIndices, charIndices]) \n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]\n",
    "\n",
    "def padding(Sentences):\n",
    "    maxlen = 52\n",
    "    for sentence in Sentences:\n",
    "        char = sentence[2]\n",
    "        for x in char:\n",
    "            maxlen = max(maxlen,len(x))\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(textline:'takes line as an input'):\n",
    "    \n",
    "    textline = re.sub('@[\\\\w_]+', 'USER_MENTION', textline) # replaces @username with 'usermention'\n",
    "    textline = re.sub('\\\\|LBR\\\\|', '', textline) # to replace linebreak\n",
    "    textline = re.sub('\\\\.\\\\.\\\\.+', '...', textline) #????????\n",
    "    textline = re.sub('!!+', '!!', textline)#?????????\n",
    "    textline = re.sub('\\\\?\\\\?+', '??', textline)\n",
    "    words = re.compile('[\\\\U00010000-\\\\U0010ffff]|[\\\\w-]+|[^ \\\\w\\\\U00010000-\\\\U0010ffff]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the first 1m sentences for use\n",
    "def create_train_data(filename:'name of original file', n_lines:'no of lines to use for training from the twitter data',\n",
    "                     new_file_name:'name of file to create'):\n",
    "    new_file = open(new_file_name,'wb')\n",
    "    n_lines_read=0\n",
    "    with open(filename,'rb') as f:\n",
    "        for line in f:\n",
    "            if n_lines_read == n_lines:\n",
    "                break\n",
    "            line = line.decode()\n",
    "            ids, text = line.split('\\t')\n",
    "            n_lines_read+=1\n",
    "            text = twitter_tokenizer(text)\n",
    "            to_write = ' '.join(text)\n",
    "            to_write+='\\n'\n",
    "#             print(to_write)\n",
    "            new_file.write(to_write.encode())\n",
    "            \n",
    "#             assert n_lines_read<=10\n",
    "    f.close()\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_data('/home/jindal/notebooks/twitter_data/twitter-2011_de', 1000000, \n",
    "                  '/home/jindal/notebooks/jindal/NER/language_model/training_lines_twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size =51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the sentences to a fixed size of 51\n",
    "def generate_sequences(out_file, in_file ) -> None:\n",
    "    output = open(out_file,'wb')\n",
    "    with open(in_file,'rb') as f:\n",
    "        for line in f:\n",
    "            x+=1\n",
    "            text = line.decode().split()\n",
    "            for i in range(len(text)):\n",
    "                if i+1 >= window_size:\n",
    "                    temp = text[i-window_size+1:i+1]\n",
    "                else:\n",
    "                    temp = ['0' for i in range(window_size - (i +1))] + text[:i+1]\n",
    "                string = ' '.join(temp)+' \\n'\n",
    "                output.write(string.encode())\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequences('/home/jindal/notebooks/jindal/NER/language_model/twitter_dataset_sequences_one_million',\n",
    "                   '/home/jindal/notebooks/jindal/NER/language_model/training_lines_twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert\n",
    "with open('twitter_dataset_sequences_one_million') as f:\n",
    "    for line in f:\n",
    "        assert len(line.split()) == 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Hard coded case lookup ::\n",
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n"
     ]
    }
   ],
   "source": [
    "print(caseEmbeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters={}\n",
    "\n",
    "with open ('/home/jindal/notebooks/jindal/NER/language_model/twitter_dataset_sequences_one_million') as f:\n",
    "    for line in f:\n",
    "        for word in line:\n",
    "            word =str(word)\n",
    "            for char in word:\n",
    "#                 print(char)\n",
    "                characters[char]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'т': True, '\\ue410': True, '\\ue018': True, '¾': True, 'Đ': True, '♔': True, 'რ': True, '\\ue401': True, 'у': True, '\\ue224': True, 'һ': True, '\\ue115': True, '安': True, '↓': True, '\\ue05a': True, '🐟': True, 'Ι': True, '★': True, 'Î': True, '\\ue147': True, 'Û': True, '♋': True, '\\ue005': True, '└': True, 'K': True, '¸': True, 'ツ': True, '👌': True, '\\ue40f': True, '↗': True, 'ײ': True, '›': True, '\\u200f': True, 'р': True, '\\ue50b': True, '○': True, 'ę': True, '\\ue21f': True, '∫': True, '\\ue529': True, '\\ue502': True, '/': True, '\\ue023': True, '❤': True, '☞': True, '➨': True, '╦': True, '➋': True, '\\x9e': True, '❋': True, '@': True, 'f': True, '♦': True, 'ő': True, '❍': True, '\\ue03c': True, 'ِ': True, 'x': True, 'ò': True, 'Γ': True, '^': True, '-': True, 'ˆ': True, '\\ue24d': True, '❥': True, '✋': True, '.': True, '\\ue122': True, '聖': True, '▪': True, '&': True, '👼': True, \"'\": True, '\\ue13e': True, '̐': True, '\\ue052': True, '\\ue695': True, '!': True, '✖': True, 'ɔ': True, '¶': True, '』': True, 'ī': True, '👏': True, '؟': True, '\\ue426': True, '\\u200e': True, 'Å': True, '€': True, '🏁': True, '\\ue102': True, '흠': True, 'ㅠ': True, 'ⓔ': True, 'N': True, '٤': True, '㋖': True, 'е': True, 'Ľ': True, 'Π': True, '̲': True, '\\ue327': True, 'A': True, '\\ue016': True, '◎': True, '\\ue31c': True, 'ｏ': True, 'Q': True, '̡': True, 'ć': True, '\\ue50c': True, 'Б': True, '\\ue405': True, '📝': True, '>': True, '☯': True, '\\ue118': True, '♍': True, '˘': True, '»': True, '✭': True, 'ｌ': True, '❖': True, '魔': True, '́': True, '🍀': True, 'ð': True, '\\ue107': True, '▂': True, '💃': True, 'š': True, '\\ue51f': True, 'ō': True, '文': True, '\\ue404': True, '%': True, '۶': True, '\\ue14c': True, '\\ue21c': True, 'j': True, 'ơ': True, '❦': True, 'ś': True, 'œ': True, '\\ue105': True, 'ı': True, 'Ú': True, '┘': True, 'ā': True, '\\ue524': True, 'ù': True, '\\ue34b': True, 'Ê': True, '\\ue011': True, 'η': True, '\\ue402': True, '\\ue201': True, 'à': True, '鸡': True, '\\ue429': True, '\\ue40d': True, '◕': True, '†': True, 'ノ': True, 'G': True, '“': True, 'ƞ': True, '：': True, '☊': True, '雅': True, '\\ue70b': True, 'Ç': True, 'ˇ': True, '料': True, '✿': True, '➲': True, '¥': True, 'É': True, 'Ó': True, '😣': True, '\\ue329': True, 'ㅁ': True, '\\ue10c': True, '\\ue12a': True, '▽': True, '\\ue017': True, '💦': True, '✌': True, 'ஜ': True, '😰': True, '¦': True, '▾': True, 'Ǥ': True, '😓': True, '🙅': True, '+': True, 'Ʒ': True, '➸': True, '→': True, '˛': True, 'Ï': True, '］': True, 'ƒ': True, 't': True, '\\x84': True, '냠': True, 'Г': True, '\\ue337': True, '\\ue106': True, '\\ue43a': True, '\\ue008': True, '囧': True, ' ': True, 'Ž': True, '₪': True, '\\ue423': True, 'Í': True, '繁': True, '⁰': True, '٥': True, 'τ': True, '큐': True, '\\ue137': True, 'ш': True, '🙇': True, 'ű': True, 'ǻ': True, 'm': True, '\\ue03e': True, '\\ue13c': True, '3': True, '5': True, '😊': True, '‚': True, '̝': True, '\\ue32c': True, '😔': True, '{': True, 'v': True, '❝': True, '²': True, '\\x83': True, '（': True, '\\ue110': True, '\\ue24c': True, 'o': True, 'Ӝ': True, '那': True, 'و': True, '\\x96': True, 'к': True, '▓': True, '➮': True, 'Ώ': True, '2': True, '☇': True, 'ʃ': True, '\\ue114': True, '＞': True, 'Á': True, 'W': True, 'щ': True, '雀': True, '✔': True, 'ч': True, '🐳': True, '｡': True, 'ü': True, 'ѕ': True, '¼': True, 'Щ': True, ':': True, '✯': True, 'Ψ': True, 'T': True, '╤': True, '☟': True, '🏈': True, '̥': True, 'ｷ': True, 'ⓠ': True, '❾': True, '➯': True, '✦': True, '\\ue04a': True, 'û': True, 'ㅅ': True, '\\ue225': True, 'ц': True, '9': True, '㋡': True, '\\ue01b': True, '\\x89': True, 'з': True, 'н': True, '҈': True, '̊': True, '😂': True, '✈': True, '\\x01': True, 'ל': True, '橋': True, '¹': True, '∙': True, 'ԃ': True, 'Ğ': True, '😝': True, 'ى': True, '➜': True, 'ј': True, 'p': True, 'ţ': True, '🔥': True, 'Ⓑ': True, 'К': True, '『': True, 'l': True, '😭': True, '\\ue019': True, '☀': True, 'Ξ': True, '🏃': True, '⛄': True, '\\ue406': True, '∂': True, 'ヅ': True, '―': True, 'ğ': True, '〇': True, 'っ': True, '\\ue003': True, '♞': True, '💋': True, 'い': True, 'с': True, 'Δ': True, '\\ue108': True, '🎵': True, '〈': True, '\\ue64f': True, 'g': True, '‿': True, 'ɪ': True, '😥': True, 'ế': True, '冇': True, '↘': True, 'y': True, 'ř': True, 'ỏ': True, '┃': True, '\\ue32e': True, '\\ue336': True, '⌂': True, '≈': True, 'Տ': True, '<': True, '🐶': True, 'ڰ': True, '\\ue304': True, 'Ƹ': True, '¬': True, 'ė': True, 'X': True, '4': True, '̵': True, 'ڿ': True, '♼': True, 'L': True, 'Ȋ': True, '☤': True, '✄': True, '\\ue11a': True, 'ē': True, '\\uf8ff': True, 'ã': True, '✝': True, '✘': True, '＄': True, '⓴': True, '品': True, '(': True, '\\ue326': True, '\\x94': True, '✟': True, '\\ue219': True, '▻': True, '…': True, 'È': True, 'م': True, '¿': True, 'ε': True, '응': True, '끝': True, 'ψ': True, '씟': True, '\\ue41f': True, 'а': True, '\\ue04e': True, '♨': True, '\\ue40e': True, '➟': True, '笑': True, 'μ': True, 'â': True, 'ɜ': True, 'Ｔ': True, '藝': True, '中': True, 'þ': True, 'ミ': True, '⚑': True, '#': True, '─': True, '📖': True, '\\ue411': True, '⇨': True, '\\ue10e': True, '✰': True, '\\x93': True, '\\ue32a': True, '÷': True, 'ㅋ': True, 'Ø': True, 'Ł': True, 'ڡ': True, 'ټ': True, 'Ÿ': True, '♕': True, 'D': True, '\\ue038': True, 'ૐ': True, 'Ò': True, 'õ': True, '\\ue11b': True, '⇦': True, '\\x92': True, '\\ue656': True, 'æ': True, '＜': True, '\\ue13f': True, ']': True, 'Ƙ': True, '➡': True, '😞': True, '♛': True, '無': True, '☐': True, '❒': True, '≠': True, 'À': True, '′': True, '¨': True, '\\ue32b': True, '☺': True, 'α': True, '\\ue13b': True, '\\ue65f': True, 'ņ': True, '┌': True, '▲': True, '・': True, '\\ue412': True, '—': True, 'ﾐ': True, '?': True, '\\ue050': True, 'Ʀ': True, '\\ue152': True, '☛': True, '◄': True, 'ů': True, 'ď': True, '\\ue030': True, '0': True, '👻': True, '\\ue517': True, '₩': True, '✶': True, '✓': True, '😠': True, '☪': True, 'מ': True, '←': True, '\\ue054': True, '\\ue328': True, '\\ue253': True, '\\ue036': True, '\\ue317': True, '♩': True, 'ô': True, '😒': True, '☁': True, '∞': True, '♬': True, '³': True, 'ɟ': True, '❞': True, '💢': True, 'ご': True, '\\ue055': True, 'z': True, '㏘': True, 'ー': True, 'ǝ': True, '\\ue11d': True, '̀': True, '\\ue331': True, '\\ue407': True, '£': True, 'Ŋ': True, '山': True, '\\ue01d': True, '\\ue427': True, '\\ue420': True, 'İ': True, '➤': True, '０': True, '✺': True, '\\x80': True, 'ą': True, 'ʏ': True, 'Ủ': True, '\\ue159': True, '\\ue10b': True, 'Ν': True, '\\ue510': True, '\\ue00e': True, '\\ue012': True, 'Ý': True, '❣': True, '™': True, 'Ƒ': True, '简': True, '*': True, '☆': True, '➠': True, '\\ue119': True, '©': True, 'ا': True, '🎉': True, '✬': True, '弾': True, '\\ue409': True, 'Â': True, 'é': True, '̃': True, 'ý': True, 'Я': True, 'ˌ': True, '😲': True, '光': True, '🚙': True, 'h': True, '̬': True, 'Þ': True, '✽': True, '\\ue408': True, '】': True, 'ç': True, '【': True, '♠': True, '➀': True, '，': True, '🎤': True, '\\\\': True, '\\ue21a': True, '\\ue30e': True, 'ن': True, '\\ue043': True, '�': True, '⋈': True, '℘': True, '8': True, 'د': True, 'ן': True, '😄': True, '§': True, '■': True, '\\ue40c': True, 'ﾔ': True, '\\ue30f': True, '🔫': True, '☾': True, '\\ue20c': True, '\\x82': True, '\\ue40b': True, 'U': True, '↯': True, '⇩': True, '下': True, '∪': True, 'Z': True, '∀': True, '∟': True, '´': True, '°': True, '💝': True, 'ｗ': True, '\\ue101': True, '\\ue418': True, '▶': True, '\\ue717': True, '💔': True, '😚': True, '┐': True, '\\ue01a': True, '\\ue312': True, '体': True, '🐤': True, '😖': True, '\\ue536': True, 'ê': True, '’': True, 'ƍ': True, '\\u200b': True, 'Æ': True, '◦': True, 'ㅎ': True, '¢': True, '宫': True, '☎': True, ';': True, '\\ue034': True, 'ó': True, '[': True, 'ⅲ': True, 'n': True, '✇': True, '♜': True, 'ⓞ': True, '💗': True, '➹': True, 'ź': True, 'ї': True, '‘': True, '\\ue6ec': True, '♈': True, '∇': True, '\\ue10d': True, 'F': True, '〉': True, 'Ñ': True, '×': True, 'Ë': True, '̅': True, '😉': True, 'Ʋ': True, '゜': True, '\\ue116': True, '；': True, 'א': True, 'ה': True, 'ړ': True, '❘': True, ',': True, '\\ue340': True, 'ͼ': True, 'Œ': True, '\\ue414': True, '❔': True, '\\ue12b': True, 'Ʊ': True, '웃': True, 'M': True, 'ờ': True, '👀': True, 'ă': True, '［': True, '値': True, 'i': True, '«': True, '\\ue707': True, '🌟': True, 'ⓟ': True, '❆': True, '\\ue413': True, 'י': True, 'ͽ': True, '\\ue526': True, '┏': True, '☹': True, '︻': True, 'ǟ': True, '\\ue301': True, '﹏': True, '\\ue520': True, 'B': True, 'ə': True, '∩': True, '♤': True, '\\x9f': True, '✍': True, '\\ue23c': True, '\\ue443': True, 'J': True, '|': True, 'Ô': True, '₡': True, '‼': True, 'ы': True, 'Ⓖ': True, '☃': True, '☢': True, '\\ue6ff': True, '\\x19': True, '😍': True, '\\ue231': True, 'ԅ': True, '☑': True, '※': True, '⌘': True, '☮': True, 'b': True, 'ñ': True, '\\ue136': True, '☂': True, 'Ⓢ': True, 'Ö': True, '☔': True, '😱': True, 'π': True, 'є': True, '卐': True, 'Ã': True, 'E': True, 'Ù': True, '\\x95': True, 'ä': True, '\\x99': True, '◅': True, '\\ue32d': True, '🙏': True, 'Ð': True, '\\ue204': True, '☝': True, 'Ｖ': True, '\\ue708': True, 's': True, 'Ȝ': True, '\\x97': True, '🎓': True, '✩': True, 'ΰ': True, '̷': True, '🏀': True, '\\ue419': True, 'Ő': True, 'げ': True, 'ゞ': True, '↪': True, 'Т': True, '➼': True, '″': True, '®': True, '$': True, 'ಠ': True, '̮': True, '➛': True, '╬': True, '\\x1b': True, 'ʊ': True, '✨': True, 'ʘ': True, '\\ue01c': True, '\\ue724': True, '／': True, 'd': True, '栖': True, '░': True, '\\ue415': True, '！': True, '\\ue403': True, '👐': True, '♏': True, '□': True, '楽': True, 'ㅜ': True, '►': True, '😘': True, '\\ue43e': True, 'Ì': True, '\\ue416': True, '♥': True, 'ú': True, '❁': True, '\\ue112': True, '\\ue022': True, 'ě': True, '‾': True, 'ß': True, '\\ue230': True, '▷': True, 'e': True, 'c': True, 'S': True, '↻': True, '⩾': True, '✾': True, '\\ue447': True, '\\ue424': True, '\\ue12f': True, '∆': True, '🇯': True, '\\n': True, '№': True, '\\ue42b': True, '❏': True, 'Λ': True, '\\ue057': True, 'ì': True, '╥': True, '`': True, 'Ɩ': True, 'ÿ': True, '„': True, 'C': True, '翔': True, 'H': True, '端': True, '體': True, 'Y': True, '\\ue330': True, 'ɑ': True, '”': True, 'м': True, '│': True, '💤': True, '✉': True, '\\ue041': True, '\\ue40a': True, 'u': True, 'д': True, 'ｰ': True, '▒': True, '🚗': True, 'ง': True, 'ゅ': True, 'υ': True, '」': True, '🙌': True, '😪': True, '￡': True, '̶': True, '₠': True, '\\x7f': True, '\\ue048': True, '\\ue154': True, '➔': True, '\\ue516': True, '\\ue41e': True, 'ו': True, 'ϟ': True, 'ẞ': True, '😌': True, '💣': True, 'č': True, '「': True, '￼': True, '\\ue047': True, 'ĝ': True, '\\ue004': True, '\\ue032': True, '◀': True, '💀': True, '）': True, '7': True, '±': True, 'ㅂ': True, '⋗': True, '\\ue417': True, '}': True, 'ο': True, '⇒': True, '💟': True, '😏': True, '\\x8a': True, 'β': True, '＠': True, 'ﾟ': True, '\\ue50f': True, 'k': True, 'ℓ': True, '☄': True, '🍴': True, '●': True, 'ƪ': True, '❧': True, '\\ue140': True, '▿': True, 'ª': True, 'Ⓞ': True, '①': True, 'µ': True, '😷': True, '≧': True, '👍': True, 'O': True, '？': True, '👶': True, 'ℊ': True, '브': True, '❙': True, 'ݓ': True, 'ت': True, '┓': True, '┎': True, '\\ue111': True, '💓': True, '🐙': True, '♫': True, '\\ue058': True, '爆': True, '😁': True, '💩': True, 'Д': True, 'ɘ': True, 'P': True, '·': True, '👎': True, '💻': True, '👋': True, '😢': True, 'Ơ': True, 'Ь': True, 'ö': True, 'カ': True, '\\ue010': True, 'í': True, 'ˈ': True, '▬': True, '6': True, '\\ue51c': True, '█': True, 'ƺ': True, 'ッ': True, 'ⓐ': True, 'ў': True, '☼': True, 'Β': True, '∅': True, '➧': True, 'П': True, '˝': True, 'ё': True, 'V': True, '\\ue141': True, '\\ue41d': True, '½': True, '̴': True, 'ø': True, 'ⓣ': True, 'А': True, 'Ä': True, '◇': True, '\\ue131': True, '♚': True, '҂': True, '譜': True, '↑': True, 'ậ': True, '\\ue71f': True, 'q': True, '=': True, 'ќ': True, 'Ă': True, '\\ue41a': True, 'I': True, '̇': True, '̯': True, '\\ue202': True, 'ۣ': True, 'む': True, '🐛': True, '⌣': True, '✐': True, 'ş': True, '❚': True, 'ⓘ': True, '\\uf04a': True, '\\ue31d': True, 'Ｊ': True, 'ë': True, '💐': True, 'ⓗ': True, '術': True, 'ь': True, '\\ue113': True, '❈': True, '☠': True, 'Σ': True, 'r': True, 'î': True, 'w': True, 'ⓚ': True, '商': True, '♺': True, '1': True, 'Ü': True, 'a': True, '\\ue120': True, 'б': True, '\\ue303': True, '냐': True, '\\ue01e': True, '\\ue42a': True, '̨': True, '‹': True, 'R': True, '◯': True, 'º': True, '−': True, '√': True, '\\uf8eb': True, '▼': True, '\\ue52f': True, '✂': True, '═': True, '🏆': True, '¡': True, '➻': True, 'Ձ': True, 'Õ': True, '💙': True, '¯': True, 'ï': True, '来': True, '\\ue00a': True, '🎃': True, '･': True, '━': True, '\\ue6fc': True, '♪': True, '\\ue322': True, '▸': True, '💪': True, '＃': True, '\\ue33b': True, '\\ue421': True, '動': True, 'ღ': True, '格': True, 'ł': True, '\\ue059': True, '自': True, '\\ue528': True, 'ⓤ': True, 'え': True, '_': True, '➳': True, '♉': True, '\\ue126': True, 'ಥ': True, '😜': True, '架': True, 'פ': True, '⋙': True, '\\x9c': True, '☻': True, '﹠': True, 'Ɣ': True, '✪': True, '☕': True, '\\ue148': True, '♖': True, 'ⓛ': True, '\\ue00d': True, '➣': True, 'す': True, '\\ue056': True, '\"': True, '͡': True, '🇵': True, '➝': True, 'è': True, 'Ŧ': True, '̩': True, 'á': True, 'أ': True, '☜': True, '\\xad': True, '–': True, '♻': True, '💜': True, '😡': True, '➪': True, 'σ': True, 'Ĝ': True, 'ừ': True, '̈': True, '\\ue04c': True, 'ω': True, '•': True, 'ż': True, '\\ue43f': True, 'バ': True, '\\ue31f': True, '\\ue42d': True, '¤': True, '٩': True, '＼': True, '\\ue045': True, '\\ue725': True, '｀': True, '💁': True, '➽': True, '➾': True, 'å': True, '⎷': True, 'Ѽ': True, '￣': True, '🎶': True, '△': True, '⋆': True, '\\ue04f': True, '✞': True, '✗': True, '유': True, '~': True, '✎': True, '˜': True, '̄': True, 'ﻬ': True, '\\ue41c': True, 'Ş': True, 'ف': True, '🐔': True, '┒': True, '☉': True, ')': True, '\\ue719': True, '♧': True, 'ƙ': True, '、': True, '⌒': True, '\\ue324': True, '😳': True, '😃': True, '♡': True, 'ɣ': True}\n"
     ]
    }
   ],
   "source": [
    "print(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2Idx={}\n",
    "for char in characters:\n",
    "    char2Idx[char] = len(char2Idx)\n",
    "# print(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the vocab of a file along with a counter of the vocab\n",
    "def get_vocab(infile:\"inputfile\"):\n",
    "    toy_vocab ={}\n",
    "    toy_vocab_counter = defaultdict(int)\n",
    "\n",
    "\n",
    "    with open(infile) as f:\n",
    "        for line in f:\n",
    "    #         print(line)\n",
    "            line = line.split()\n",
    "    #         print(len(line))\n",
    "            for word in line:\n",
    "    #             word = word\n",
    "    #             print(word)\n",
    "                toy_vocab[word]=True\n",
    "                toy_vocab_counter[word]+=1\n",
    "    return toy_vocab, toy_vocab_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, freq = get_vocab('/home/jindal/notebooks/jindal/NER/language_model/training_lines_twitter')\n",
    "most_common_words = sorted(freq, key=freq.get, reverse=True)[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER_MENTION\n"
     ]
    }
   ],
   "source": [
    "print(most_common_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_less_frequent(inpfile, outfile):\n",
    "    outfile = open(outfile,'w')\n",
    "    with open(inpfile) as f:\n",
    "        for line in f:\n",
    "            temp = []\n",
    "            for word in line.split():\n",
    "                if word in most_common_words:\n",
    "                    temp.append(word)\n",
    "                else:\n",
    "                    temp.append('NAN')\n",
    "            string = ' '.join(temp)\n",
    "            string+='\\n'\n",
    "            outfile.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-263af0b5f8d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreplace_less_frequent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_lines_twitter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'twitter_1m_pruned_lines'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-c1f234a6abe4>\u001b[0m in \u001b[0;36mreplace_less_frequent\u001b[0;34m(inpfile, outfile)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmost_common_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                     \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replace_less_frequent('training_lines_twitter', 'twitter_1m_pruned_lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_vocab, freq = get_vocab('twitter_dataset_sequences_one_million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Terharuan sih aku hehe \n",
      "\n"
     ]
    }
   ],
   "source": [
    "line = linecache.getline('/home/jindal/notebooks/jindal/NER/language_model/twitter_dataset_sequences_one_million', 19)\n",
    "\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Idx={}\n",
    "for word in toy_vocab.keys():\n",
    "    word2Idx[word] = len(word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "874053"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2Idx['und']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(most_co))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_number=0\n",
    "# lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator(file :\"input training file\", batch_size, total_lines):\n",
    "    \n",
    "    \n",
    "    global line_number, lock\n",
    "        \n",
    "    while True:\n",
    "        word_embeddings = []\n",
    "        case_embeddings = []\n",
    "        char_embeddings = []\n",
    "        output_labels = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            index = line_number%total_lines +1\n",
    "#             print(index)\n",
    "            line_number+=1\n",
    "#             lock.release()\n",
    "#             index = random.choice(len(features), 1)[0]\n",
    "#             print(index)\n",
    "            line = linecache.getline(file, index)\n",
    "            line = line.split()\n",
    "#             print(line)\n",
    "            temp_casing = []\n",
    "            temp_char=[]\n",
    "            temp_word=[]\n",
    "#             print(line)\n",
    "            if len(line)!=51:\n",
    "                print(index)\n",
    "                continue\n",
    "            for word in line[:-1]:\n",
    "#                 print(word)\n",
    "                casing =getCasing(word, case2Idx)\n",
    "#                 print(casing)\n",
    "                temp_casing.append(casing)\n",
    "                temp_char2=[]\n",
    "                for char in word:\n",
    "                    temp_char2.append(char2Idx[char])\n",
    "                temp_char2 = np.array(temp_char2)\n",
    "#                 print(temp_char2)\n",
    "#                 temp_char2 = pad_sequences(temp_char2, 52, padding='post')\n",
    "#                 temp_char.append(pad_sequences(temp_char2, 52, padding='post'))\n",
    "                temp_char.append(temp_char2)\n",
    "                word_vector = ft.get_word_vector(word.lower())\n",
    "                temp_word.append(word_vector)\n",
    "            temp_char = pad_sequences(temp_char, 52)\n",
    "#             print(temp_word)\n",
    "#             print(len(temp_word))\n",
    "#             print(temp_casing)\n",
    "#             print(temp_char)\n",
    "#             print(len(temp_char))\n",
    "#             print(\" **************** \")\n",
    "            word_embeddings.append(temp_word)\n",
    "            case_embeddings.append(temp_casing)\n",
    "            char_embeddings.append(temp_char)\n",
    "            output_labels.append(ft.get_word_vector(line[-1]))\n",
    "#             batch_features[i] = process_features(line, window_size-1, nb_embedding_dims)\n",
    "            # print(batch_features[i])\n",
    "            # print(batch_features[i].shape)\n",
    "#             batch_labels[i] = labels[index]\n",
    "        yield ([np.array(word_embeddings), np.array(case_embeddings), np.array(char_embeddings)], np.array(output_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp, output in my_generator('toy_dataset_sequences_shuffled',10):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2Idx.keys())+1\n",
    "print(len(most_common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 30) 34200       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, None, 52, 30) 0           char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_19 (TimeDistri (None, None, 52, 30) 2730        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maxpool (TimeDistributed)       (None, None, 1, 30)  0           time_distributed_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_20 (TimeDistri (None, None, 30)     0           maxpool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, None, 30)     0           time_distributed_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, None, 330)    0           words_input[0][0]                \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 400)          849600      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 300)          120300      bidirectional_10[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 1,006,830\n",
      "Trainable params: 1,006,830\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "words_input = Input(shape=(None, 300), dtype='float32',name='words_input')\n",
    "# words = Embedding(input_dim =50, output_dim=300, trainable=False)(words_input)\n",
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= Dropout(0.5, name='dropout1')(embed_char_out)\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, name='conv'))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(52), name='maxpool')(conv1d_out)\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "output = concatenate([words_input, char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=False, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "# output = TimeDistributed(Dense(vocab_size))(output)\n",
    "# crf = CRF(len(label2Idx))\n",
    "# output = crf(output)\n",
    "# output = Flatten()(output)\n",
    "output = Dense(300, activation='softmax')(output)\n",
    "# output = Flatten()(output)\n",
    "# output = Dense(vocab_size, activation='softmax')(output)\n",
    "# model.add(Dense(vocab_size, activation='softmax'))\n",
    "model = Model(inputs=[words_input,casing_input, character_input], outputs=[output])\n",
    "model.compile(loss='cosine_proximity', optimizer='nadam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  2858/375540 [..............................] - ETA: 24:59:29 - loss: -0.3447 - acc: 0.1259"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-ea224b40d4e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     model.fit_generator(my_generator(file_name, 32, file_len(file_name)), epochs=1, \n\u001b[0;32m----> 5\u001b[0;31m                         steps_per_epoch=file_len(file_name)//32, callbacks=[WeightsSaver(model, 20000)])\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'german_lm_twitter.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "line_number=0\n",
    "file_name = '/home/jindal/notebooks/jindal/NER/language_model/twitter_dataset_sequences_one_million'\n",
    "try:\n",
    "    model.fit_generator(my_generator(file_name, 64, file_len(file_name)), epochs=1, \n",
    "                        steps_per_epoch=file_len(file_name)//64, callbacks=[WeightsSaver(model, 20000)])\n",
    "except Exception as e:\n",
    "    model.save('german_lm_twitter.h5')\n",
    "model.save('german_lm_twitter.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
