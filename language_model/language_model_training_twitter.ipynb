{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '/home/jindal/notebooks/jindal/NER')\n",
    "import fastText\n",
    "import numpy as np \n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import sklearn\n",
    "import pickle, threading\n",
    "from keras.utils import to_categorical\n",
    "import linecache\n",
    "from keras.callbacks import Callback\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsSaver(Callback):\n",
    "    def __init__(self, model, N):\n",
    "        self.model = model\n",
    "        self.N = N\n",
    "        self.batch = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if self.batch % self.N == 0:\n",
    "            name = 'german_lm.h5'\n",
    "#             print(\"model saved %s\" %self.batch)\n",
    "            self.model.save_weights(name)\n",
    "        self.batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home/jindal/notebooks/fastText/wiki.de.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(sentences, word2Idx, case2Idx, char2Idx):\n",
    "    #{'numeric': 0, 'allLower': 1, 'contains_digit': 6, 'PADDING_TOKEN': 7, 'other': 4, 'allUpper': 2, 'mainly_numeric': 5, 'initialUpper': 3}\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "#         labelIndices = []\n",
    "        \n",
    "        for word,char in sentence:  \n",
    "            wordCount += 1\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]                 \n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                charIdx.append(char2Idx[x])\n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "#             labelIndices.append(label2Idx[label])\n",
    "           \n",
    "        dataset.append([wordIndices,caseIndices, charIndices]) \n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]\n",
    "\n",
    "def padding(Sentences):\n",
    "    maxlen = 52\n",
    "    for sentence in Sentences:\n",
    "        char = sentence[2]\n",
    "        for x in char:\n",
    "            maxlen = max(maxlen,len(x))\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(textline:'takes line as an input'):\n",
    "    \n",
    "    textline = re.sub('@[\\\\w_]+', 'USER_MENTION', textline) # replaces @username with 'usermention'\n",
    "    textline = re.sub('\\\\|LBR\\\\|', '', textline) # to replace linebreak\n",
    "    textline = re.sub('\\\\.\\\\.\\\\.+', '...', textline) #????????\n",
    "    textline = re.sub('!!+', '!!', textline)#?????????\n",
    "    textline = re.sub('\\\\?\\\\?+', '??', textline)\n",
    "    words = re.compile('[\\\\U00010000-\\\\U0010ffff]|[\\\\w-]+|[^ \\\\w\\\\U00010000-\\\\U0010ffff]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the first 1m sentences for use\n",
    "def create_train_data(filename:'name of original file', n_lines:'no of lines to use for training from the twitter data',\n",
    "                     new_file_name:'name of file to create'):\n",
    "    new_file = open(new_file_name,'wb')\n",
    "    n_lines_read=0\n",
    "    with open(filename,'rb') as f:\n",
    "        for line in f:\n",
    "            if n_lines_read == n_lines:\n",
    "                break\n",
    "            line = line.decode()\n",
    "            ids, text = line.split('\\t')\n",
    "            n_lines_read+=1\n",
    "            text = twitter_tokenizer(text)\n",
    "            to_write = ' '.join(text)\n",
    "            to_write+='\\n'\n",
    "#             print(to_write)\n",
    "            new_file.write(to_write.encode())\n",
    "            \n",
    "#             assert n_lines_read<=10\n",
    "    f.close()\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_data('/home/jindal/notebooks/twitter_data/twitter-2011_de', 1000000, \n",
    "                  '/home/jindal/notebooks/jindal/NER/language_model/training_lines_twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size =51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the sentences to a fixed size of 51\n",
    "def generate_sequences(out_file, in_file ) -> None:\n",
    "    output = open(out_file,'wb')\n",
    "    with open(in_file,'rb') as f:\n",
    "        for line in f:\n",
    "            x+=1\n",
    "            text = line.decode().split()\n",
    "            for i in range(len(text)):\n",
    "                if i+1 >= window_size:\n",
    "                    temp = text[i-window_size+1:i+1]\n",
    "                else:\n",
    "                    temp = ['0' for i in range(window_size - (i +1))] + text[:i+1]\n",
    "                string = ' '.join(temp)+' \\n'\n",
    "                output.write(string.encode())\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequences('/home/jindal/notebooks/jindal/NER/language_model/twitter_dataset_sequences_one_million',\n",
    "                   '/home/jindal/notebooks/jindal/NER/language_model/training_lines_twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert\n",
    "with open('twitter_dataset_sequences_one_million') as f:\n",
    "    for line in f:\n",
    "        assert len(line.split()) == 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Hard coded case lookup ::\n",
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n"
     ]
    }
   ],
   "source": [
    "print(caseEmbeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters={}\n",
    "\n",
    "with open ('/home/jindal/notebooks/jindal/NER/language_model/twitter_dataset_sequences_one_million') as f:\n",
    "    for line in f:\n",
    "        for word in line:\n",
    "            word =str(word)\n",
    "            for char in word:\n",
    "#                 print(char)\n",
    "                characters[char]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'—Ç': True, '\\ue410': True, '\\ue018': True, '¬æ': True, 'ƒê': True, '‚ôî': True, '·É†': True, '\\ue401': True, '—É': True, '\\ue224': True, '“ª': True, '\\ue115': True, 'ÂÆâ': True, '‚Üì': True, '\\ue05a': True, 'üêü': True, 'Œô': True, '‚òÖ': True, '√é': True, '\\ue147': True, '√õ': True, '‚ôã': True, '\\ue005': True, '‚îî': True, 'K': True, '¬∏': True, '„ÉÑ': True, 'üëå': True, '\\ue40f': True, '‚Üó': True, '◊≤': True, '‚Ä∫': True, '\\u200f': True, '—Ä': True, '\\ue50b': True, '‚óã': True, 'ƒô': True, '\\ue21f': True, '‚à´': True, '\\ue529': True, '\\ue502': True, '/': True, '\\ue023': True, '‚ù§': True, '‚òû': True, '‚û®': True, '‚ï¶': True, '‚ûã': True, '\\x9e': True, '‚ùã': True, '@': True, 'f': True, '‚ô¶': True, '≈ë': True, '‚ùç': True, '\\ue03c': True, 'Ÿê': True, 'x': True, '√≤': True, 'Œì': True, '^': True, '-': True, 'ÀÜ': True, '\\ue24d': True, '‚ù•': True, '‚úã': True, '.': True, '\\ue122': True, 'ËÅñ': True, '‚ñ™': True, '&': True, 'üëº': True, \"'\": True, '\\ue13e': True, 'Ãê': True, '\\ue052': True, '\\ue695': True, '!': True, '‚úñ': True, '…î': True, '¬∂': True, '„Äè': True, 'ƒ´': True, 'üëè': True, 'ÿü': True, '\\ue426': True, '\\u200e': True, '√Ö': True, '‚Ç¨': True, 'üèÅ': True, '\\ue102': True, 'Ìù†': True, '„Ö†': True, '‚ìî': True, 'N': True, 'Ÿ§': True, '„ãñ': True, '–µ': True, 'ƒΩ': True, 'Œ†': True, 'Ã≤': True, '\\ue327': True, 'A': True, '\\ue016': True, '‚óé': True, '\\ue31c': True, 'ÔΩè': True, 'Q': True, 'Ã°': True, 'ƒá': True, '\\ue50c': True, '–ë': True, '\\ue405': True, 'üìù': True, '>': True, '‚òØ': True, '\\ue118': True, '‚ôç': True, 'Àò': True, '¬ª': True, '‚ú≠': True, 'ÔΩå': True, '‚ùñ': True, 'È≠î': True, 'ÃÅ': True, 'üçÄ': True, '√∞': True, '\\ue107': True, '‚ñÇ': True, 'üíÉ': True, '≈°': True, '\\ue51f': True, '≈ç': True, 'Êñá': True, '\\ue404': True, '%': True, '€∂': True, '\\ue14c': True, '\\ue21c': True, 'j': True, '∆°': True, '‚ù¶': True, '≈õ': True, '≈ì': True, '\\ue105': True, 'ƒ±': True, '√ö': True, '‚îò': True, 'ƒÅ': True, '\\ue524': True, '√π': True, '\\ue34b': True, '√ä': True, '\\ue011': True, 'Œ∑': True, '\\ue402': True, '\\ue201': True, '√†': True, 'È∏°': True, '\\ue429': True, '\\ue40d': True, '‚óï': True, '‚Ä†': True, '„Éé': True, 'G': True, '‚Äú': True, '∆û': True, 'Ôºö': True, '‚òä': True, 'ÈõÖ': True, '\\ue70b': True, '√á': True, 'Àá': True, 'Êñô': True, '‚úø': True, '‚û≤': True, '¬•': True, '√â': True, '√ì': True, 'üò£': True, '\\ue329': True, '„ÖÅ': True, '\\ue10c': True, '\\ue12a': True, '‚ñΩ': True, '\\ue017': True, 'üí¶': True, '‚úå': True, '‡Æú': True, 'üò∞': True, '¬¶': True, '‚ñæ': True, '«§': True, 'üòì': True, 'üôÖ': True, '+': True, '∆∑': True, '‚û∏': True, '‚Üí': True, 'Àõ': True, '√è': True, 'ÔºΩ': True, '∆í': True, 't': True, '\\x84': True, 'ÎÉ†': True, '–ì': True, '\\ue337': True, '\\ue106': True, '\\ue43a': True, '\\ue008': True, 'Âõß': True, ' ': True, '≈Ω': True, '‚Ç™': True, '\\ue423': True, '√ç': True, 'ÁπÅ': True, '‚Å∞': True, 'Ÿ•': True, 'œÑ': True, 'ÌÅê': True, '\\ue137': True, '—à': True, 'üôá': True, '≈±': True, '«ª': True, 'm': True, '\\ue03e': True, '\\ue13c': True, '3': True, '5': True, 'üòä': True, '‚Äö': True, 'Ãù': True, '\\ue32c': True, 'üòî': True, '{': True, 'v': True, '‚ùù': True, '¬≤': True, '\\x83': True, 'Ôºà': True, '\\ue110': True, '\\ue24c': True, 'o': True, '”ú': True, 'ÈÇ£': True, 'Ÿà': True, '\\x96': True, '–∫': True, '‚ñì': True, '‚ûÆ': True, 'Œè': True, '2': True, '‚òá': True, ' É': True, '\\ue114': True, 'Ôºû': True, '√Å': True, 'W': True, '—â': True, 'ÈõÄ': True, '‚úî': True, '—á': True, 'üê≥': True, 'ÔΩ°': True, '√º': True, '—ï': True, '¬º': True, '–©': True, ':': True, '‚úØ': True, 'Œ®': True, 'T': True, '‚ï§': True, '‚òü': True, 'üèà': True, 'Ã•': True, 'ÔΩ∑': True, '‚ì†': True, '‚ùæ': True, '‚ûØ': True, '‚ú¶': True, '\\ue04a': True, '√ª': True, '„ÖÖ': True, '\\ue225': True, '—Ü': True, '9': True, '„ã°': True, '\\ue01b': True, '\\x89': True, '–∑': True, '–Ω': True, '“à': True, 'Ãä': True, 'üòÇ': True, '‚úà': True, '\\x01': True, '◊ú': True, 'Ê©ã': True, '¬π': True, '‚àô': True, '‘É': True, 'ƒû': True, 'üòù': True, 'Ÿâ': True, '‚ûú': True, '—ò': True, 'p': True, '≈£': True, 'üî•': True, '‚í∑': True, '–ö': True, '„Äé': True, 'l': True, 'üò≠': True, '\\ue019': True, '‚òÄ': True, 'Œû': True, 'üèÉ': True, '‚õÑ': True, '\\ue406': True, '‚àÇ': True, '„ÉÖ': True, '‚Äï': True, 'ƒü': True, '„Äá': True, '„Å£': True, '\\ue003': True, '‚ôû': True, 'üíã': True, '„ÅÑ': True, '—Å': True, 'Œî': True, '\\ue108': True, 'üéµ': True, '„Äà': True, '\\ue64f': True, 'g': True, '‚Äø': True, '…™': True, 'üò•': True, '·∫ø': True, 'ÂÜá': True, '‚Üò': True, 'y': True, '≈ô': True, '·ªè': True, '‚îÉ': True, '\\ue32e': True, '\\ue336': True, '‚åÇ': True, '‚âà': True, '’è': True, '<': True, 'üê∂': True, '⁄∞': True, '\\ue304': True, '∆∏': True, '¬¨': True, 'ƒó': True, 'X': True, '4': True, 'Ãµ': True, '⁄ø': True, '‚ôº': True, 'L': True, '»ä': True, '‚ò§': True, '‚úÑ': True, '\\ue11a': True, 'ƒì': True, '\\uf8ff': True, '√£': True, '‚úù': True, '‚úò': True, 'ÔºÑ': True, '‚ì¥': True, 'ÂìÅ': True, '(': True, '\\ue326': True, '\\x94': True, '‚úü': True, '\\ue219': True, '‚ñª': True, '‚Ä¶': True, '√à': True, 'ŸÖ': True, '¬ø': True, 'Œµ': True, 'Ïùë': True, 'ÎÅù': True, 'œà': True, 'Ïîü': True, '\\ue41f': True, '–∞': True, '\\ue04e': True, '‚ô®': True, '\\ue40e': True, '‚ûü': True, 'Á¨ë': True, 'Œº': True, '√¢': True, '…ú': True, 'Ôº¥': True, 'Ëóù': True, '‰∏≠': True, '√æ': True, '„Éü': True, '‚öë': True, '#': True, '‚îÄ': True, 'üìñ': True, '\\ue411': True, '‚á®': True, '\\ue10e': True, '‚ú∞': True, '\\x93': True, '\\ue32a': True, '√∑': True, '„Öã': True, '√ò': True, '≈Å': True, '⁄°': True, 'Ÿº': True, '≈∏': True, '‚ôï': True, 'D': True, '\\ue038': True, '‡´ê': True, '√í': True, '√µ': True, '\\ue11b': True, '‚á¶': True, '\\x92': True, '\\ue656': True, '√¶': True, 'Ôºú': True, '\\ue13f': True, ']': True, '∆ò': True, '‚û°': True, 'üòû': True, '‚ôõ': True, 'ÁÑ°': True, '‚òê': True, '‚ùí': True, '‚â†': True, '√Ä': True, '‚Ä≤': True, '¬®': True, '\\ue32b': True, '‚ò∫': True, 'Œ±': True, '\\ue13b': True, '\\ue65f': True, '≈Ü': True, '‚îå': True, '‚ñ≤': True, '„Éª': True, '\\ue412': True, '‚Äî': True, 'Ôæê': True, '?': True, '\\ue050': True, '∆¶': True, '\\ue152': True, '‚òõ': True, '‚óÑ': True, '≈Ø': True, 'ƒè': True, '\\ue030': True, '0': True, 'üëª': True, '\\ue517': True, '‚Ç©': True, '‚ú∂': True, '‚úì': True, 'üò†': True, '‚ò™': True, '◊û': True, '‚Üê': True, '\\ue054': True, '\\ue328': True, '\\ue253': True, '\\ue036': True, '\\ue317': True, '‚ô©': True, '√¥': True, 'üòí': True, '‚òÅ': True, '‚àû': True, '‚ô¨': True, '¬≥': True, '…ü': True, '‚ùû': True, 'üí¢': True, '„Åî': True, '\\ue055': True, 'z': True, '„èò': True, '„Éº': True, '«ù': True, '\\ue11d': True, 'ÃÄ': True, '\\ue331': True, '\\ue407': True, '¬£': True, '≈ä': True, 'Â±±': True, '\\ue01d': True, '\\ue427': True, '\\ue420': True, 'ƒ∞': True, '‚û§': True, 'Ôºê': True, '‚ú∫': True, '\\x80': True, 'ƒÖ': True, ' è': True, '·ª¶': True, '\\ue159': True, '\\ue10b': True, 'Œù': True, '\\ue510': True, '\\ue00e': True, '\\ue012': True, '√ù': True, '‚ù£': True, '‚Ñ¢': True, '∆ë': True, 'ÁÆÄ': True, '*': True, '‚òÜ': True, '‚û†': True, '\\ue119': True, '¬©': True, 'ÿß': True, 'üéâ': True, '‚ú¨': True, 'Âºæ': True, '\\ue409': True, '√Ç': True, '√©': True, 'ÃÉ': True, '√Ω': True, '–Ø': True, 'Àå': True, 'üò≤': True, 'ÂÖâ': True, 'üöô': True, 'h': True, 'Ã¨': True, '√û': True, '‚úΩ': True, '\\ue408': True, '„Äë': True, '√ß': True, '„Äê': True, '‚ô†': True, '‚ûÄ': True, 'Ôºå': True, 'üé§': True, '\\\\': True, '\\ue21a': True, '\\ue30e': True, 'ŸÜ': True, '\\ue043': True, 'ÔøΩ': True, '‚ãà': True, '‚Ñò': True, '8': True, 'ÿØ': True, '◊ü': True, 'üòÑ': True, '¬ß': True, '‚ñ†': True, '\\ue40c': True, 'Ôæî': True, '\\ue30f': True, 'üî´': True, '‚òæ': True, '\\ue20c': True, '\\x82': True, '\\ue40b': True, 'U': True, '‚ÜØ': True, '‚á©': True, '‰∏ã': True, '‚à™': True, 'Z': True, '‚àÄ': True, '‚àü': True, '¬¥': True, '¬∞': True, 'üíù': True, 'ÔΩó': True, '\\ue101': True, '\\ue418': True, '‚ñ∂': True, '\\ue717': True, 'üíî': True, 'üòö': True, '‚îê': True, '\\ue01a': True, '\\ue312': True, '‰Ωì': True, 'üê§': True, 'üòñ': True, '\\ue536': True, '√™': True, '‚Äô': True, '∆ç': True, '\\u200b': True, '√Ü': True, '‚ó¶': True, '„Öé': True, '¬¢': True, 'ÂÆ´': True, '‚òé': True, ';': True, '\\ue034': True, '√≥': True, '[': True, '‚Ö≤': True, 'n': True, '‚úá': True, '‚ôú': True, '‚ìû': True, 'üíó': True, '‚ûπ': True, '≈∫': True, '—ó': True, '‚Äò': True, '\\ue6ec': True, '‚ôà': True, '‚àá': True, '\\ue10d': True, 'F': True, '„Äâ': True, '√ë': True, '√ó': True, '√ã': True, 'ÃÖ': True, 'üòâ': True, '∆≤': True, '„Çú': True, '\\ue116': True, 'Ôºõ': True, '◊ê': True, '◊î': True, '⁄ì': True, '‚ùò': True, ',': True, '\\ue340': True, 'Õº': True, '≈í': True, '\\ue414': True, '‚ùî': True, '\\ue12b': True, '∆±': True, 'ÏõÉ': True, 'M': True, '·ªù': True, 'üëÄ': True, 'ƒÉ': True, 'Ôºª': True, 'ÂÄ§': True, 'i': True, '¬´': True, '\\ue707': True, 'üåü': True, '‚ìü': True, '‚ùÜ': True, '\\ue413': True, '◊ô': True, 'ÕΩ': True, '\\ue526': True, '‚îè': True, '‚òπ': True, 'Ô∏ª': True, '«ü': True, '\\ue301': True, 'Ôπè': True, '\\ue520': True, 'B': True, '…ô': True, '‚à©': True, '‚ô§': True, '\\x9f': True, '‚úç': True, '\\ue23c': True, '\\ue443': True, 'J': True, '|': True, '√î': True, '‚Ç°': True, '‚Äº': True, '—ã': True, '‚íº': True, '‚òÉ': True, '‚ò¢': True, '\\ue6ff': True, '\\x19': True, 'üòç': True, '\\ue231': True, '‘Ö': True, '‚òë': True, '‚Äª': True, '‚åò': True, '‚òÆ': True, 'b': True, '√±': True, '\\ue136': True, '‚òÇ': True, '‚ìà': True, '√ñ': True, '‚òî': True, 'üò±': True, 'œÄ': True, '—î': True, 'Âçê': True, '√É': True, 'E': True, '√ô': True, '\\x95': True, '√§': True, '\\x99': True, '‚óÖ': True, '\\ue32d': True, 'üôè': True, '√ê': True, '\\ue204': True, '‚òù': True, 'Ôº∂': True, '\\ue708': True, 's': True, '»ú': True, '\\x97': True, 'üéì': True, '‚ú©': True, 'Œ∞': True, 'Ã∑': True, 'üèÄ': True, '\\ue419': True, '≈ê': True, '„Åí': True, '„Çû': True, '‚Ü™': True, '–¢': True, '‚ûº': True, '‚Ä≥': True, '¬Æ': True, '$': True, '‡≤†': True, 'ÃÆ': True, '‚ûõ': True, '‚ï¨': True, '\\x1b': True, ' ä': True, '‚ú®': True, ' ò': True, '\\ue01c': True, '\\ue724': True, 'Ôºè': True, 'd': True, 'Ê†ñ': True, '‚ñë': True, '\\ue415': True, 'ÔºÅ': True, '\\ue403': True, 'üëê': True, '‚ôè': True, '‚ñ°': True, 'Ê•Ω': True, '„Öú': True, '‚ñ∫': True, 'üòò': True, '\\ue43e': True, '√å': True, '\\ue416': True, '‚ô•': True, '√∫': True, '‚ùÅ': True, '\\ue112': True, '\\ue022': True, 'ƒõ': True, '‚Äæ': True, '√ü': True, '\\ue230': True, '‚ñ∑': True, 'e': True, 'c': True, 'S': True, '‚Üª': True, '‚©æ': True, '‚úæ': True, '\\ue447': True, '\\ue424': True, '\\ue12f': True, '‚àÜ': True, 'üáØ': True, '\\n': True, '‚Ññ': True, '\\ue42b': True, '‚ùè': True, 'Œõ': True, '\\ue057': True, '√¨': True, '‚ï•': True, '`': True, '∆ñ': True, '√ø': True, '‚Äû': True, 'C': True, 'Áøî': True, 'H': True, 'Á´Ø': True, 'È´î': True, 'Y': True, '\\ue330': True, '…ë': True, '‚Äù': True, '–º': True, '‚îÇ': True, 'üí§': True, '‚úâ': True, '\\ue041': True, '\\ue40a': True, 'u': True, '–¥': True, 'ÔΩ∞': True, '‚ñí': True, 'üöó': True, '‡∏á': True, '„ÇÖ': True, 'œÖ': True, '„Äç': True, 'üôå': True, 'üò™': True, 'Ôø°': True, 'Ã∂': True, '‚Ç†': True, '\\x7f': True, '\\ue048': True, '\\ue154': True, '‚ûî': True, '\\ue516': True, '\\ue41e': True, '◊ï': True, 'œü': True, '·∫û': True, 'üòå': True, 'üí£': True, 'ƒç': True, '„Äå': True, 'Ôøº': True, '\\ue047': True, 'ƒù': True, '\\ue004': True, '\\ue032': True, '‚óÄ': True, 'üíÄ': True, 'Ôºâ': True, '7': True, '¬±': True, '„ÖÇ': True, '‚ãó': True, '\\ue417': True, '}': True, 'Œø': True, '‚áí': True, 'üíü': True, 'üòè': True, '\\x8a': True, 'Œ≤': True, 'Ôº†': True, 'Ôæü': True, '\\ue50f': True, 'k': True, '‚Ñì': True, '‚òÑ': True, 'üç¥': True, '‚óè': True, '∆™': True, '‚ùß': True, '\\ue140': True, '‚ñø': True, '¬™': True, '‚ìÑ': True, '‚ë†': True, '¬µ': True, 'üò∑': True, '‚âß': True, 'üëç': True, 'O': True, 'Ôºü': True, 'üë∂': True, '‚Ñä': True, 'Î∏å': True, '‚ùô': True, '›ì': True, 'ÿ™': True, '‚îì': True, '‚îé': True, '\\ue111': True, 'üíì': True, 'üêô': True, '‚ô´': True, '\\ue058': True, 'ÁàÜ': True, 'üòÅ': True, 'üí©': True, '–î': True, '…ò': True, 'P': True, '¬∑': True, 'üëé': True, 'üíª': True, 'üëã': True, 'üò¢': True, '∆†': True, '–¨': True, '√∂': True, '„Ç´': True, '\\ue010': True, '√≠': True, 'Àà': True, '‚ñ¨': True, '6': True, '\\ue51c': True, '‚ñà': True, '∆∫': True, '„ÉÉ': True, '‚ìê': True, '—û': True, '‚òº': True, 'Œí': True, '‚àÖ': True, '‚ûß': True, '–ü': True, 'Àù': True, '—ë': True, 'V': True, '\\ue141': True, '\\ue41d': True, '¬Ω': True, 'Ã¥': True, '√∏': True, '‚ì£': True, '–ê': True, '√Ñ': True, '‚óá': True, '\\ue131': True, '‚ôö': True, '“Ç': True, 'Ë≠ú': True, '‚Üë': True, '·∫≠': True, '\\ue71f': True, 'q': True, '=': True, '—ú': True, 'ƒÇ': True, '\\ue41a': True, 'I': True, 'Ãá': True, 'ÃØ': True, '\\ue202': True, '€£': True, '„ÇÄ': True, 'üêõ': True, '‚å£': True, '‚úê': True, '≈ü': True, '‚ùö': True, '‚ìò': True, '\\uf04a': True, '\\ue31d': True, 'Ôº™': True, '√´': True, 'üíê': True, '‚ìó': True, 'Ë°ì': True, '—å': True, '\\ue113': True, '‚ùà': True, '‚ò†': True, 'Œ£': True, 'r': True, '√Æ': True, 'w': True, '‚ìö': True, 'ÂïÜ': True, '‚ô∫': True, '1': True, '√ú': True, 'a': True, '\\ue120': True, '–±': True, '\\ue303': True, 'ÎÉê': True, '\\ue01e': True, '\\ue42a': True, 'Ã®': True, '‚Äπ': True, 'R': True, '‚óØ': True, '¬∫': True, '‚àí': True, '‚àö': True, '\\uf8eb': True, '‚ñº': True, '\\ue52f': True, '‚úÇ': True, '‚ïê': True, 'üèÜ': True, '¬°': True, '‚ûª': True, '’Å': True, '√ï': True, 'üíô': True, '¬Ø': True, '√Ø': True, 'Êù•': True, '\\ue00a': True, 'üéÉ': True, 'ÔΩ•': True, '‚îÅ': True, '\\ue6fc': True, '‚ô™': True, '\\ue322': True, '‚ñ∏': True, 'üí™': True, 'ÔºÉ': True, '\\ue33b': True, '\\ue421': True, 'Âãï': True, '·É¶': True, 'Ê†º': True, '≈Ç': True, '\\ue059': True, 'Ëá™': True, '\\ue528': True, '‚ì§': True, '„Åà': True, '_': True, '‚û≥': True, '‚ôâ': True, '\\ue126': True, '‡≤•': True, 'üòú': True, 'Êû∂': True, '◊§': True, '‚ãô': True, '\\x9c': True, '‚òª': True, 'Ôπ†': True, '∆î': True, '‚ú™': True, '‚òï': True, '\\ue148': True, '‚ôñ': True, '‚ìõ': True, '\\ue00d': True, '‚û£': True, '„Åô': True, '\\ue056': True, '\"': True, 'Õ°': True, 'üáµ': True, '‚ûù': True, '√®': True, '≈¶': True, 'Ã©': True, '√°': True, 'ÿ£': True, '‚òú': True, '\\xad': True, '‚Äì': True, '‚ôª': True, 'üíú': True, 'üò°': True, '‚û™': True, 'œÉ': True, 'ƒú': True, '·ª´': True, 'Ãà': True, '\\ue04c': True, 'œâ': True, '‚Ä¢': True, '≈º': True, '\\ue43f': True, '„Éê': True, '\\ue31f': True, '\\ue42d': True, '¬§': True, 'Ÿ©': True, 'Ôºº': True, '\\ue045': True, '\\ue725': True, 'ÔΩÄ': True, 'üíÅ': True, '‚ûΩ': True, '‚ûæ': True, '√•': True, '‚é∑': True, '—º': True, 'Ôø£': True, 'üé∂': True, '‚ñ≥': True, '‚ãÜ': True, '\\ue04f': True, '‚úû': True, '‚úó': True, 'Ïú†': True, '~': True, '‚úé': True, 'Àú': True, 'ÃÑ': True, 'Ôª¨': True, '\\ue41c': True, '≈û': True, 'ŸÅ': True, 'üêî': True, '‚îí': True, '‚òâ': True, ')': True, '\\ue719': True, '‚ôß': True, '∆ô': True, '„ÄÅ': True, '‚åí': True, '\\ue324': True, 'üò≥': True, 'üòÉ': True, '‚ô°': True, '…£': True}\n"
     ]
    }
   ],
   "source": [
    "print(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2Idx={}\n",
    "for char in characters:\n",
    "    char2Idx[char] = len(char2Idx)\n",
    "# print(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the vocab of a file along with a counter of the vocab\n",
    "def get_vocab(infile:\"inputfile\"):\n",
    "    toy_vocab ={}\n",
    "    toy_vocab_counter = defaultdict(int)\n",
    "\n",
    "\n",
    "    with open(infile) as f:\n",
    "        for line in f:\n",
    "    #         print(line)\n",
    "            line = line.split()\n",
    "    #         print(len(line))\n",
    "            for word in line:\n",
    "    #             word = word\n",
    "    #             print(word)\n",
    "                toy_vocab[word]=True\n",
    "                toy_vocab_counter[word]+=1\n",
    "    return toy_vocab, toy_vocab_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, freq = get_vocab('/home/jindal/notebooks/jindal/NER/language_model/training_lines_twitter')\n",
    "most_common_words = sorted(freq, key=freq.get, reverse=True)[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER_MENTION\n"
     ]
    }
   ],
   "source": [
    "print(most_common_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_less_frequent(inpfile, outfile):\n",
    "    outfile = open(outfile,'w')\n",
    "    with open(inpfile) as f:\n",
    "        for line in f:\n",
    "            temp = []\n",
    "            for word in line.split():\n",
    "                if word in most_common_words:\n",
    "                    temp.append(word)\n",
    "                else:\n",
    "                    temp.append('NAN')\n",
    "            string = ' '.join(temp)\n",
    "            string+='\\n'\n",
    "            outfile.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-263af0b5f8d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreplace_less_frequent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_lines_twitter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'twitter_1m_pruned_lines'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-c1f234a6abe4>\u001b[0m in \u001b[0;36mreplace_less_frequent\u001b[0;34m(inpfile, outfile)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmost_common_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                     \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replace_less_frequent('training_lines_twitter', 'twitter_1m_pruned_lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_vocab, freq = get_vocab('twitter_dataset_sequences_one_million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Terharuan sih aku hehe \n",
      "\n"
     ]
    }
   ],
   "source": [
    "line = linecache.getline('/home/jindal/notebooks/jindal/NER/language_model/twitter_dataset_sequences_one_million', 19)\n",
    "\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Idx={}\n",
    "for word in toy_vocab.keys():\n",
    "    word2Idx[word] = len(word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "874053"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2Idx['und']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(most_co))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_number=0\n",
    "# lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator(file :\"input training file\", batch_size, total_lines):\n",
    "    \n",
    "    \n",
    "    global line_number, lock\n",
    "        \n",
    "    while True:\n",
    "        word_embeddings = []\n",
    "        case_embeddings = []\n",
    "        char_embeddings = []\n",
    "        output_labels = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            index = line_number%total_lines +1\n",
    "#             print(index)\n",
    "            line_number+=1\n",
    "#             lock.release()\n",
    "#             index = random.choice(len(features), 1)[0]\n",
    "#             print(index)\n",
    "            line = linecache.getline(file, index)\n",
    "            line = line.split()\n",
    "#             print(line)\n",
    "            temp_casing = []\n",
    "            temp_char=[]\n",
    "            temp_word=[]\n",
    "#             print(line)\n",
    "            if len(line)!=51:\n",
    "                print(index)\n",
    "                continue\n",
    "            for word in line[:-1]:\n",
    "#                 print(word)\n",
    "                casing =getCasing(word, case2Idx)\n",
    "#                 print(casing)\n",
    "                temp_casing.append(casing)\n",
    "                temp_char2=[]\n",
    "                for char in word:\n",
    "                    temp_char2.append(char2Idx[char])\n",
    "                temp_char2 = np.array(temp_char2)\n",
    "#                 print(temp_char2)\n",
    "#                 temp_char2 = pad_sequences(temp_char2, 52, padding='post')\n",
    "#                 temp_char.append(pad_sequences(temp_char2, 52, padding='post'))\n",
    "                temp_char.append(temp_char2)\n",
    "                word_vector = ft.get_word_vector(word.lower())\n",
    "                temp_word.append(word_vector)\n",
    "            temp_char = pad_sequences(temp_char, 52)\n",
    "#             print(temp_word)\n",
    "#             print(len(temp_word))\n",
    "#             print(temp_casing)\n",
    "#             print(temp_char)\n",
    "#             print(len(temp_char))\n",
    "#             print(\" **************** \")\n",
    "            word_embeddings.append(temp_word)\n",
    "            case_embeddings.append(temp_casing)\n",
    "            char_embeddings.append(temp_char)\n",
    "            output_labels.append(ft.get_word_vector(line[-1]))\n",
    "#             batch_features[i] = process_features(line, window_size-1, nb_embedding_dims)\n",
    "            # print(batch_features[i])\n",
    "            # print(batch_features[i].shape)\n",
    "#             batch_labels[i] = labels[index]\n",
    "        yield ([np.array(word_embeddings), np.array(case_embeddings), np.array(char_embeddings)], np.array(output_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp, output in my_generator('toy_dataset_sequences_shuffled',10):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2Idx.keys())+1\n",
    "print(len(most_common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 30) 34200       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout1 (Dropout)              (None, None, 52, 30) 0           char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_19 (TimeDistri (None, None, 52, 30) 2730        dropout1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maxpool (TimeDistributed)       (None, None, 1, 30)  0           time_distributed_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_20 (TimeDistri (None, None, 30)     0           maxpool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, None, 30)     0           time_distributed_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, None, 330)    0           words_input[0][0]                \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 400)          849600      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 300)          120300      bidirectional_10[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 1,006,830\n",
      "Trainable params: 1,006,830\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "words_input = Input(shape=(None, 300), dtype='float32',name='words_input')\n",
    "# words = Embedding(input_dim =50, output_dim=300, trainable=False)(words_input)\n",
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= Dropout(0.5, name='dropout1')(embed_char_out)\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, name='conv'))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(52), name='maxpool')(conv1d_out)\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "output = concatenate([words_input, char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=False, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "# output = TimeDistributed(Dense(vocab_size))(output)\n",
    "# crf = CRF(len(label2Idx))\n",
    "# output = crf(output)\n",
    "# output = Flatten()(output)\n",
    "output = Dense(300, activation='softmax')(output)\n",
    "# output = Flatten()(output)\n",
    "# output = Dense(vocab_size, activation='softmax')(output)\n",
    "# model.add(Dense(vocab_size, activation='softmax'))\n",
    "model = Model(inputs=[words_input,casing_input, character_input], outputs=[output])\n",
    "model.compile(loss='cosine_proximity', optimizer='nadam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  2858/375540 [..............................] - ETA: 24:59:29 - loss: -0.3447 - acc: 0.1259"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-ea224b40d4e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     model.fit_generator(my_generator(file_name, 32, file_len(file_name)), epochs=1, \n\u001b[0;32m----> 5\u001b[0;31m                         steps_per_epoch=file_len(file_name)//32, callbacks=[WeightsSaver(model, 20000)])\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'german_lm_twitter.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "line_number=0\n",
    "file_name = '/home/jindal/notebooks/jindal/NER/language_model/twitter_dataset_sequences_one_million'\n",
    "try:\n",
    "    model.fit_generator(my_generator(file_name, 64, file_len(file_name)), epochs=1, \n",
    "                        steps_per_epoch=file_len(file_name)//64, callbacks=[WeightsSaver(model, 20000)])\n",
    "except Exception as e:\n",
    "    model.save('german_lm_twitter.h5')\n",
    "model.save('german_lm_twitter.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
