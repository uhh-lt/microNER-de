{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "import math\n",
    "import linecache\n",
    "import numpy as np \n",
    "from numpy import random\n",
    "from random import sample\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import re\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "# from attention_utils import get_activations, get_data_recurrent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home/jindal/notebooks/fastText/wiki.de.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()\n",
    "nb_sequence_length = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(textline):\n",
    "    textLine = re.sub(r'http\\S+', 'URL', textline)\n",
    "    textline = re.sub('@[\\w_]+', 'USER_MENTION', textline)\n",
    "    textline = re.sub('\\|LBR\\|', '', textline)\n",
    "    textline = re.sub('\\.\\.\\.+', '...', textline)\n",
    "    textline = re.sub('!!+', '!!', textline)\n",
    "    textline = re.sub('\\?\\?+', '??', textline)\n",
    "    words = re.compile('[\\U00010000-\\U0010ffff]|[\\w-]+|[^ \\w\\U00010000-\\U0010ffff]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    # print(words)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator(filename, batch_size):\n",
    "    \n",
    "    f = open(filename)\n",
    "    \n",
    "#     file_length = sum(1 for line in open(filename, encoding = 'UTF-8'))\n",
    "#     shuffled_indexes = range(1, file_length + 1)\n",
    "    # shuffled_indexes = sample(shuffled_indexes, len(shuffled_indexes))\n",
    "#     index_position = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    while True:\n",
    "        batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "#     batch_features_lg = np.zeros((batch_size, nb_sequence_length, nb_embedding2_dims))\n",
    "    # batch_features_idx = np.zeros((batch_size, nb_sequence_length))\n",
    "        batch_labels = np.zeros((batch_size, 2))\n",
    "        # print(len(features))\n",
    "        for i in range(batch_size):\n",
    "            line = f.readline()\n",
    "            if (\"\" == line):\n",
    "                f.seek(0)\n",
    "                line = f.readline()\n",
    "#             line = linecache.getline(filename, shuffled_indexes[index_position])\n",
    "            data = line.strip().split('\\t')\n",
    "            if len(data)!=2:\n",
    "                i-=1\n",
    "                continue\n",
    "            batch_features_ft[i] = process_features(data[0], nb_sequence_length, nb_embedding_dims)\n",
    "            # print(batch_features_ft[i])\n",
    "            # print(batch_features_ft[i].shape)\n",
    "            batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "#             index_position += 1\n",
    "#             if index_position == file_length:\n",
    "                # shuffle indexes again\n",
    "#                 shuffled_indexes = range(1, file_length + 1)\n",
    "                # shuffled_indexes = sample(shuffled_indexes, len(shuffled_indexes))\n",
    "#                 index_position = 0\n",
    "#                 break\n",
    "        # yield [batch_features_ft, batch_features_lg], batch_labels\n",
    "        yield ([batch_features_ft], batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_ft = {}\n",
    "def process_features(textline, nb_sequence_length, nb_embedding_dims):\n",
    "    words = twitter_tokenizer(textline)\n",
    "    features_ft = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    features_idx = np.zeros(nb_sequence_length)\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            word_vectors_ft[w] = wv\n",
    "        features_ft[idx] = wv\n",
    "        \n",
    "        idx = idx + 1\n",
    "    return features_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/jindal/notebooks/jindal/NER/language_model/FB_counter-speech.csv')\n",
    "\n",
    "line = f.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Islam ist KEINE Religion', '', ' Der islam mit der dazugehörigen scharia ist eine faschistische und unmenschliche Ideologie (ein allumfassendes Regelwerk für das gesamte Leben sozial, kulturell, militärisch, politisch, rechtlich und religiös) um einen barbarischen, pädophilen Propheten, einem Massenmörder und Sklaventreiber der die Welt seit Jahrhunderten unterdrücken und beherrschen will!', 'OFFENSE']\n",
      "4\n",
      "['Islam ist KEINE Religion', '', ' Der islam mit der dazugehörigen scharia ist eine faschistische und unmenschliche Ideologie (ein allumfassendes Regelwerk für das gesamte Leben sozial, kulturell, militärisch, politisch, rechtlich und religiös) um einen barbarischen, pädophilen Propheten, einem Massenmörder und Sklaventreiber der die Welt seit Jahrhunderten unterdrücken und beherrschen will!', 'OFFENSE']\n",
      "4\n",
      "['OTHER']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "f = open('/home/jindal/notebooks/jindal/NER/language_model/FB_counter-speech.csv')\n",
    "\n",
    "for line in f:\n",
    "    line = line.strip().split('\\t')\n",
    "    \n",
    "    try:\n",
    "        assert len(line)==2\n",
    "    except AssertionError as e:\n",
    "        print(line)\n",
    "        print(len(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [line.strip().split(\"\\t\") for line in open('/home/jindal/notebooks/jindal/NER/language_model/FB_train.csv') if len(line.strip().split('\\t'))==2]\n",
    "dev_lines = [line.strip().split(\"\\t\") for line in open('/home/jindal/notebooks/jindal/NER/language_model/FB_dev.csv') if len(line.strip().split())==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [x[0] for x in train_lines]\n",
    "train_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in train_lines])\n",
    "train_labels = [0 if x[1] == \"OTHER\" else 1 for x in train_lines]\n",
    "\n",
    "dev_sentences = [x[0] for x in dev_lines]\n",
    "dev_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in dev_lines])\n",
    "dev_labels = [0 if x[1] == \"OTHER\" else 1 for x in dev_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 75, 200)      320800      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 75, 200)      0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 73, 200)      120200      leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 72, 200)      160200      leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 71, 200)      200200      leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 73, 200)      0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 72, 200)      0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 71, 200)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 200)          0           leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 200)          0           leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 200)          0           leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 200)          0           global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 200)          0           global_max_pooling1d_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 200)          0           global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 600)          0           dropout_13[0][0]                 \n",
      "                                                                 dropout_14[0][0]                 \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 100)          60100       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 100)          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 2)            202         leaky_re_lu_25[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 861,702\n",
      "Trainable params: 861,702\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_input_embedding = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "lstm_block = Bidirectional(LSTM(100, dropout = 0.5, return_sequences=True))(model_input_embedding)\n",
    "lstm_block = LeakyReLU()(lstm_block)\n",
    "\n",
    "filter_sizes = (3, 4, 5)\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Conv1D(\n",
    "        filters = 200,\n",
    "        kernel_size = sz,\n",
    "        padding = 'valid',\n",
    "        strides = 1\n",
    "    )(lstm_block)\n",
    "    conv = LeakyReLU()(conv)\n",
    "    conv = GlobalMaxPooling1D()(conv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "    conv_blocks.append(conv)\n",
    "model_concatenated = concatenate([conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "# model_concatenated = Dropout(0.8)(model_concatenated)\n",
    "model_concatenated = Dense(100)(model_concatenated)\n",
    "model_concatenated = LeakyReLU()(model_concatenated)\n",
    "model_output = Dense(n_labels, activation = \"softmax\")(model_concatenated)\n",
    "model = Model(model_input_embedding, model_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_epoch = len(train_sentences)\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "steps_per_epoch = math.ceil(samples_per_epoch / batch_size)\n",
    "checkpoint = ModelCheckpoint('best_classification_model_FB.h5', \n",
    "                             monitor='val_acc', \n",
    "                             verbose = 1, \n",
    "                             save_best_only = True, \n",
    "                             save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 33s 249ms/step - loss: 0.5878 - acc: 0.7102 - val_loss: 0.5886 - val_acc: 0.6562\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65625, saving model to best_classification_model_FB.h5\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 31s 233ms/step - loss: 0.5493 - acc: 0.7320 - val_loss: 0.5568 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.65625 to 0.71875, saving model to best_classification_model_FB.h5\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 30s 230ms/step - loss: 0.5107 - acc: 0.7547 - val_loss: 0.5029 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.71875 to 0.84375, saving model to best_classification_model_FB.h5\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 30s 229ms/step - loss: 0.4890 - acc: 0.7723 - val_loss: 0.4061 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.84375\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.4603 - acc: 0.7817 - val_loss: 0.6806 - val_acc: 0.6562\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.84375\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 31s 231ms/step - loss: 0.4287 - acc: 0.8026 - val_loss: 0.5394 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.84375\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.3959 - acc: 0.8130 - val_loss: 0.3166 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.84375 to 0.87500, saving model to best_classification_model_FB.h5\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.3714 - acc: 0.8319 - val_loss: 0.3985 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.87500\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 31s 233ms/step - loss: 0.3417 - acc: 0.8390 - val_loss: 0.3949 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.87500\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 30s 231ms/step - loss: 0.3236 - acc: 0.8509 - val_loss: 0.5618 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.87500\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 30s 226ms/step - loss: 0.2950 - acc: 0.8686 - val_loss: 0.4114 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.87500\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 30s 231ms/step - loss: 0.2772 - acc: 0.8783 - val_loss: 0.5739 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.87500\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 30s 229ms/step - loss: 0.2406 - acc: 0.8925 - val_loss: 0.5360 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.87500\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 30s 228ms/step - loss: 0.2296 - acc: 0.8994 - val_loss: 0.9859 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.87500\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 30s 228ms/step - loss: 0.2060 - acc: 0.9167 - val_loss: 0.5666 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.87500\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 31s 233ms/step - loss: 0.1838 - acc: 0.9271 - val_loss: 0.6732 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.87500\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 30s 228ms/step - loss: 0.1638 - acc: 0.9344 - val_loss: 0.9158 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.87500\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 30s 229ms/step - loss: 0.1560 - acc: 0.9380 - val_loss: 0.6398 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.87500\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 30s 229ms/step - loss: 0.1361 - acc: 0.9444 - val_loss: 0.1640 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.87500 to 0.90625, saving model to best_classification_model_FB.h5\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.1213 - acc: 0.9522 - val_loss: 0.9078 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.90625\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.1078 - acc: 0.9600 - val_loss: 0.6347 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.90625\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.1024 - acc: 0.9607 - val_loss: 1.1739 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.90625\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 31s 233ms/step - loss: 0.0987 - acc: 0.9633 - val_loss: 0.9651 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.90625\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 30s 231ms/step - loss: 0.0875 - acc: 0.9635 - val_loss: 0.9772 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.90625\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 31s 234ms/step - loss: 0.0903 - acc: 0.9650 - val_loss: 1.3480 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.90625\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.0713 - acc: 0.9714 - val_loss: 1.2622 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.90625\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 30s 230ms/step - loss: 0.0701 - acc: 0.9721 - val_loss: 1.0961 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.90625\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.0778 - acc: 0.9728 - val_loss: 0.8788 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.90625\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 31s 234ms/step - loss: 0.0663 - acc: 0.9751 - val_loss: 0.9204 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.90625\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.0619 - acc: 0.9754 - val_loss: 0.8877 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.90625\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 30s 229ms/step - loss: 0.0584 - acc: 0.9785 - val_loss: 1.1814 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.90625\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 31s 231ms/step - loss: 0.0530 - acc: 0.9820 - val_loss: 0.7487 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.90625\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 30s 231ms/step - loss: 0.0529 - acc: 0.9804 - val_loss: 1.1972 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.90625\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 31s 231ms/step - loss: 0.0479 - acc: 0.9832 - val_loss: 1.0681 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.90625\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.0481 - acc: 0.9822 - val_loss: 0.9525 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.90625 to 0.93750, saving model to best_classification_model_FB.h5\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 30s 229ms/step - loss: 0.0519 - acc: 0.9806 - val_loss: 1.4356 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.93750\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 30s 231ms/step - loss: 0.0545 - acc: 0.9832 - val_loss: 1.6940 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.93750\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 31s 231ms/step - loss: 0.0429 - acc: 0.9839 - val_loss: 0.6884 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.93750\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 31s 234ms/step - loss: 0.0299 - acc: 0.9903 - val_loss: 1.7476 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.93750\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 30s 229ms/step - loss: 0.0528 - acc: 0.9799 - val_loss: 1.8246 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.93750\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.0439 - acc: 0.9844 - val_loss: 0.3941 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.93750\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 30s 231ms/step - loss: 0.0423 - acc: 0.9837 - val_loss: 1.4669 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.93750\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 31s 231ms/step - loss: 0.0458 - acc: 0.9853 - val_loss: 0.9127 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.93750\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 31s 231ms/step - loss: 0.0402 - acc: 0.9856 - val_loss: 0.6598 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.93750\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 31s 233ms/step - loss: 0.0343 - acc: 0.9879 - val_loss: 0.9256 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.93750\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 31s 231ms/step - loss: 0.0336 - acc: 0.9875 - val_loss: 1.4811 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.93750\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 30s 231ms/step - loss: 0.0442 - acc: 0.9870 - val_loss: 1.4036 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.93750\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.0382 - acc: 0.9863 - val_loss: 1.5057 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.93750\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 31s 232ms/step - loss: 0.0347 - acc: 0.9879 - val_loss: 1.1042 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.93750\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 30s 229ms/step - loss: 0.0348 - acc: 0.9875 - val_loss: 1.0020 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.93750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f445487ecf8>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reset_states()\n",
    "model.fit_generator(\n",
    "    sequential_generator('/home/jindal/notebooks/jindal/NER/language_model/FB_train.csv', batch_size), \n",
    "    steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
    "    validation_data = sequential_generator('/home/jindal/notebooks/jindal/NER/language_model/FB_dev.csv', batch_size),\n",
    "    validation_steps = math.ceil(len(dev_sentences) / batch_size),\n",
    "    callbacks = [checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFER LEARNING HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 75, 200)      320800      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, 75, 200)      0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 73, 200)      120200      leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 72, 200)      160200      leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 71, 200)      200200      leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, 73, 200)      0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, 72, 200)      0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, 71, 200)      0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 200)          0           leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (Global (None, 200)          0           leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (Global (None, 200)          0           leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 200)          0           global_max_pooling1d_25[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 200)          0           global_max_pooling1d_26[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 200)          0           global_max_pooling1d_27[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 600)          0           dropout_25[0][0]                 \n",
      "                                                                 dropout_26[0][0]                 \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 100)          60100       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, 100)          0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 2)            202         leaky_re_lu_45[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 861,702\n",
      "Trainable params: 861,702\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_input_embedding = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "lstm_block = Bidirectional(LSTM(100, dropout = 0.5, return_sequences=True))(model_input_embedding)\n",
    "lstm_block = LeakyReLU()(lstm_block)\n",
    "\n",
    "filter_sizes = (3, 4, 5)\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Conv1D(\n",
    "        filters = 200,\n",
    "        kernel_size = sz,\n",
    "        padding = 'valid',\n",
    "        strides = 1\n",
    "    )(lstm_block)\n",
    "    conv = LeakyReLU()(conv)\n",
    "    conv = GlobalMaxPooling1D()(conv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "    conv_blocks.append(conv)\n",
    "model_concatenated = concatenate([conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "# model_concatenated = Dropout(0.8)(model_concatenated)\n",
    "model_concatenated = Dense(100)(model_concatenated)\n",
    "model_concatenated = LeakyReLU()(model_concatenated)\n",
    "model_output = Dense(n_labels, activation = \"softmax\")(model_concatenated)\n",
    "new_model = Model(model_input_embedding, model_output)\n",
    "new_model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights('best_classification_model_FB.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [line.strip().split(\"\\t\") for line in open('/home/gwiedemann/notebooks/OffLang/sample_train.txt')]\n",
    "dev_lines = [line.strip().split(\"\\t\") for line in open('/home/gwiedemann/notebooks/OffLang/sample_dev.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [x[0] for x in train_lines]\n",
    "train_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in train_lines])\n",
    "train_labels = [0 if x[1] == \"OTHER\" else 1 for x in train_lines]\n",
    "\n",
    "dev_sentences = [x[0] for x in dev_lines]\n",
    "dev_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in dev_lines])\n",
    "dev_labels = [0 if x[1] == \"OTHER\" else 1 for x in dev_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator_germeval(filename, batch_size):\n",
    "    \n",
    "    f = open(filename)\n",
    "    \n",
    "#     file_length = sum(1 for line in open(filename, encoding = 'UTF-8'))\n",
    "#     shuffled_indexes = range(1, file_length + 1)\n",
    "    # shuffled_indexes = sample(shuffled_indexes, len(shuffled_indexes))\n",
    "#     index_position = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    while True:\n",
    "        batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "#     batch_features_lg = np.zeros((batch_size, nb_sequence_length, nb_embedding2_dims))\n",
    "    # batch_features_idx = np.zeros((batch_size, nb_sequence_length))\n",
    "        batch_labels = np.zeros((batch_size, 2))\n",
    "        # print(len(features))\n",
    "        for i in range(batch_size):\n",
    "            line = f.readline()\n",
    "            if (\"\" == line):\n",
    "                f.seek(0)\n",
    "                line = f.readline()\n",
    "#             line = linecache.getline(filename, shuffled_indexes[index_position])\n",
    "            data = line.strip().split('\\t')\n",
    "            if len(data)!=3:\n",
    "                i-=1\n",
    "                continue\n",
    "            batch_features_ft[i] = process_features(data[0], nb_sequence_length, nb_embedding_dims)\n",
    "            # print(batch_features_ft[i])\n",
    "            # print(batch_features_ft[i].shape)\n",
    "            batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "#             index_position += 1\n",
    "#             if index_position == file_length:\n",
    "                # shuffle indexes again\n",
    "#                 shuffled_indexes = range(1, file_length + 1)\n",
    "                # shuffled_indexes = sample(shuffled_indexes, len(shuffled_indexes))\n",
    "#                 index_position = 0\n",
    "#                 break\n",
    "        # yield [batch_features_ft, batch_features_lg], batch_labels\n",
    "        yield ([batch_features_ft], batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_epoch = len(train_sentences)\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "steps_per_epoch = math.ceil(samples_per_epoch / batch_size)\n",
    "checkpoint = ModelCheckpoint('best_classification_model.h5', \n",
    "                             monitor='val_acc', \n",
    "                             verbose = 1, \n",
    "                             save_best_only = True, \n",
    "                             save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 35s 266ms/step - loss: 0.7157 - acc: 0.7150 - val_loss: 0.5108 - val_acc: 0.7548\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75481, saving model to best_classification_model.h5\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 32s 243ms/step - loss: 0.5006 - acc: 0.7464 - val_loss: 0.4754 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75481 to 0.76562, saving model to best_classification_model.h5\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.4623 - acc: 0.7848 - val_loss: 0.4762 - val_acc: 0.7933\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76562 to 0.79327, saving model to best_classification_model.h5\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 32s 242ms/step - loss: 0.4255 - acc: 0.7985 - val_loss: 0.4702 - val_acc: 0.7873\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79327\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 32s 246ms/step - loss: 0.3986 - acc: 0.8227 - val_loss: 0.4552 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79327\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 32s 243ms/step - loss: 0.3657 - acc: 0.8366 - val_loss: 0.4831 - val_acc: 0.7981\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.79327 to 0.79808, saving model to best_classification_model.h5\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.3407 - acc: 0.8487 - val_loss: 0.5044 - val_acc: 0.7861\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79808\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 32s 245ms/step - loss: 0.2917 - acc: 0.8750 - val_loss: 0.5524 - val_acc: 0.7704\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79808\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.2726 - acc: 0.8809 - val_loss: 0.5465 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79808\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 32s 246ms/step - loss: 0.2516 - acc: 0.8906 - val_loss: 0.5398 - val_acc: 0.8041\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.79808 to 0.80409, saving model to best_classification_model.h5\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 32s 243ms/step - loss: 0.2228 - acc: 0.9053 - val_loss: 0.5838 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.80409\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 32s 243ms/step - loss: 0.2050 - acc: 0.9112 - val_loss: 0.6069 - val_acc: 0.7861\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.80409\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 32s 241ms/step - loss: 0.1685 - acc: 0.9325 - val_loss: 0.7905 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80409\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.1502 - acc: 0.9368 - val_loss: 0.7356 - val_acc: 0.7933\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.80409\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.1359 - acc: 0.9474 - val_loss: 0.8213 - val_acc: 0.7873\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.80409\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 32s 243ms/step - loss: 0.1386 - acc: 0.9455 - val_loss: 0.7646 - val_acc: 0.7837\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.80409\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 32s 246ms/step - loss: 0.1197 - acc: 0.9564 - val_loss: 0.8168 - val_acc: 0.7837\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.80409\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.1094 - acc: 0.9569 - val_loss: 0.7831 - val_acc: 0.7885\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.80409\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 32s 246ms/step - loss: 0.0976 - acc: 0.9633 - val_loss: 0.8331 - val_acc: 0.7800\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.80409\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.0905 - acc: 0.9609 - val_loss: 0.8578 - val_acc: 0.8101\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.80409 to 0.81010, saving model to best_classification_model.h5\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 32s 241ms/step - loss: 0.0929 - acc: 0.9654 - val_loss: 0.8655 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.81010\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.0715 - acc: 0.9725 - val_loss: 0.9815 - val_acc: 0.7897\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.81010\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 32s 242ms/step - loss: 0.0726 - acc: 0.9747 - val_loss: 0.9650 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.81010\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 32s 243ms/step - loss: 0.0673 - acc: 0.9723 - val_loss: 1.0818 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.81010\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.0704 - acc: 0.9747 - val_loss: 0.9831 - val_acc: 0.7873\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.81010\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 32s 245ms/step - loss: 0.0521 - acc: 0.9806 - val_loss: 1.0570 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.81010\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 32s 243ms/step - loss: 0.0681 - acc: 0.9742 - val_loss: 0.9809 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.81010\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 32s 241ms/step - loss: 0.0635 - acc: 0.9775 - val_loss: 0.9803 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.81010\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.0697 - acc: 0.9747 - val_loss: 0.8149 - val_acc: 0.8041\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.81010\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.0431 - acc: 0.9834 - val_loss: 1.2092 - val_acc: 0.7897\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.81010\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 32s 245ms/step - loss: 0.0557 - acc: 0.9789 - val_loss: 1.0145 - val_acc: 0.7981\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.81010\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 32s 241ms/step - loss: 0.0584 - acc: 0.9792 - val_loss: 1.0358 - val_acc: 0.8029\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.81010\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.0531 - acc: 0.9813 - val_loss: 1.1067 - val_acc: 0.8041\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.81010\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 32s 243ms/step - loss: 0.0470 - acc: 0.9841 - val_loss: 1.0478 - val_acc: 0.8161\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.81010 to 0.81611, saving model to best_classification_model.h5\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 32s 245ms/step - loss: 0.0500 - acc: 0.9837 - val_loss: 1.0630 - val_acc: 0.8017\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.81611\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.0468 - acc: 0.9822 - val_loss: 1.1002 - val_acc: 0.7933\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.81611\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 32s 246ms/step - loss: 0.0382 - acc: 0.9844 - val_loss: 1.1313 - val_acc: 0.7981\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.81611\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.0467 - acc: 0.9837 - val_loss: 1.0544 - val_acc: 0.8017\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.81611\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.0389 - acc: 0.9830 - val_loss: 1.1320 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.81611\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 32s 246ms/step - loss: 0.0440 - acc: 0.9832 - val_loss: 1.2364 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.81611\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 32s 242ms/step - loss: 0.0399 - acc: 0.9893 - val_loss: 1.1930 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.81611\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.0500 - acc: 0.9813 - val_loss: 0.9196 - val_acc: 0.8017\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.81611\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 32s 246ms/step - loss: 0.0439 - acc: 0.9837 - val_loss: 1.2138 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.81611\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.0570 - acc: 0.9777 - val_loss: 0.9426 - val_acc: 0.8053\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.81611\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 32s 240ms/step - loss: 0.0358 - acc: 0.9865 - val_loss: 1.1137 - val_acc: 0.8161\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.81611\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 32s 243ms/step - loss: 0.0286 - acc: 0.9884 - val_loss: 1.1234 - val_acc: 0.8161\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.81611\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 32s 245ms/step - loss: 0.0420 - acc: 0.9848 - val_loss: 1.0256 - val_acc: 0.8089\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.81611\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 32s 246ms/step - loss: 0.0371 - acc: 0.9870 - val_loss: 1.2096 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.81611 to 0.81971, saving model to best_classification_model.h5\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 32s 241ms/step - loss: 0.0341 - acc: 0.9877 - val_loss: 1.2039 - val_acc: 0.7873\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.81971\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.0264 - acc: 0.9908 - val_loss: 1.2312 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.81971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e102a0be0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_model.fit_generator(\n",
    "    sequential_generator_germeval('/home/gwiedemann/notebooks/OffLang/sample_train.txt', batch_size), \n",
    "    steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
    "    validation_data = sequential_generator_germeval('/home/gwiedemann/notebooks/OffLang/sample_dev.txt', batch_size),\n",
    "    validation_steps = math.ceil(len(dev_sentences) / batch_size),\n",
    "    callbacks = [checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_features = np.zeros((len(dev_sentences), nb_sequence_length, nb_embedding_dims))\n",
    "for i in range(len(dev_sentences)):\n",
    "    testset_features[i] = process_features(dev_sentences[i], nb_sequence_length, nb_embedding_dims)\n",
    "results = new_model.predict(testset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-Data: Prec: 0.690, Rec: 0.715, F1: 0.702, Acc: 0.800\n"
     ]
    }
   ],
   "source": [
    "idx2Label = {0 : \"OTHER\", 1 : \"OFFENSIVE\"}\n",
    "predLabels = results.argmax(axis=-1)\n",
    "devLabels = [0 if x[1] == \"OTHER\" else 1 for x in dev_lines]\n",
    "# print(idx2Label)\n",
    "# print(predLabels)\n",
    "# print(devLabels)\n",
    "f1 = f1_score(devLabels, predLabels, average='binary', pos_label=1)\n",
    "r = recall_score(devLabels, predLabels, average='binary', pos_label=1)\n",
    "p = precision_score(devLabels, predLabels, average='binary', pos_label=1)\n",
    "a = accuracy_score(devLabels, predLabels)\n",
    "print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f, Acc: %.3f\" % (p, r, f1, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
