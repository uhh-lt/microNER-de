{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "import math\n",
    "import linecache\n",
    "import numpy as np \n",
    "from numpy import random\n",
    "from random import sample\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import re\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "# from attention_utils import get_activations, get_data_recurrent\n",
    "from attention_decoder import AttentionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home/jindal/notebooks/fastText/wiki.de.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()\n",
    "nb_sequence_length = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18421504  0.16461958  0.07163134 -0.3582153   0.6343416   0.57077825\n",
      " -0.44131115  0.47140062  0.35997692  1.0691209   0.5383094  -0.00801403\n",
      "  0.58150095 -0.24640413  0.07941529 -0.7907748  -0.64057297  0.87790126\n",
      "  0.1222318   0.9839732  -0.14147948  0.2741151   0.14327082  0.7455819\n",
      " -0.58181334 -0.0139227   0.13299793  0.11719222 -0.03907198 -0.98190314\n",
      "  0.6551781  -0.08076547  0.39160377  0.5933445  -0.29222807 -0.02020206\n",
      " -0.17795676 -0.32914364 -0.9572954  -0.15258092 -0.0530088   0.31237698\n",
      "  0.37407503  0.61072457 -0.0205325   0.00588962  0.3607436   0.3082963\n",
      " -0.3130489  -0.4344106  -0.4184202  -0.16960411  0.5402667  -0.00491837\n",
      "  0.11402972 -0.3362505   0.5770166   0.13003364 -0.2651122   0.28100345\n",
      "  0.07081287 -0.00930306 -0.18135485 -0.2852216  -0.04528273 -0.10418656\n",
      " -0.39689147  0.06003198 -0.20699514  0.13569123 -0.24864858 -0.1452383\n",
      " -0.08365332  0.04030139  0.11422046  0.16102162  0.2925926   0.40926033\n",
      " -0.49666372 -0.6249165  -0.57006294 -0.1205303  -0.3541435   0.34539774\n",
      "  0.08927163  0.39094543  0.12676828 -0.3391294  -0.01737639  0.41220987\n",
      "  0.18247083 -0.27371702 -0.09793004 -0.09703299 -0.23809725  0.00482424\n",
      "  0.12626016  0.06020825 -0.6792434   0.27824506 -0.3823968   0.7319919\n",
      "  0.36686167 -0.36061448  0.01484885  0.9763383   0.623409    0.39604348\n",
      "  0.57920915  0.40461656  0.06412029  0.29473352 -0.54550713 -0.1393995\n",
      " -0.34066236 -0.15662035 -0.21668887  0.24387683  0.15306745 -0.06852037\n",
      " -0.3117108   0.19708864  0.17909242  0.5010868   0.10898975 -0.16986161\n",
      "  0.9560049  -0.4135787  -0.06685685  0.1193997   0.8293503  -0.24338198\n",
      "  0.42319041  0.2755165  -0.4972959  -0.23354629 -0.37044936 -0.31695008\n",
      " -0.21605009  0.4740254  -0.70534116 -0.06025723  0.51637214  0.09974954\n",
      "  0.80836064 -0.14830334  0.54592735  0.3383062  -0.49616945 -0.20657985\n",
      "  0.23393503 -0.2635632   0.1877886   0.14345522  0.25271478  0.12416913\n",
      " -0.7456534   0.61757755  0.01874281  0.5533872   0.28126827  0.00541478\n",
      " -0.09100021  0.0444731  -0.72013533  0.00763062  0.1178807   0.0230649\n",
      "  0.24378376  0.46994466  0.45391506 -0.26565406  0.20607781  0.03113725\n",
      " -0.45015562 -0.41041908 -0.30771878 -0.52397335  0.19247746 -0.19800423\n",
      "  0.4822764   0.2509119  -0.4262801   0.08618617  0.06353179  0.34473392\n",
      " -0.21858718 -0.12085744 -0.23378809  0.5005382   0.29722637 -0.7839036\n",
      " -0.20398131  0.0272021   0.7449148  -0.6136724   0.06676061 -0.7320623\n",
      " -0.06682321 -0.8680542  -0.05150018 -0.16978203  0.07036787  0.24758786\n",
      " -0.4506678  -0.01373566 -0.66633606 -0.6107812  -0.21357138  0.276537\n",
      " -0.52194506 -0.60678613 -0.0226591  -0.06963024 -0.65762794  0.50662637\n",
      " -0.46557644  0.35206243 -0.19921587  0.11906978  0.19821647  0.06738294\n",
      " -0.06134685  0.12963602 -0.3267129   0.32797286 -0.1530252  -0.45084402\n",
      " -0.34599185 -0.3094431   0.39539948 -0.62003976 -0.43392295 -0.7782125\n",
      " -0.6875231  -0.6275458   0.05756379 -0.06242402 -0.28571332 -0.23137537\n",
      "  0.10013065  0.6900759  -0.5614342  -0.3403287  -0.00565185  0.14920928\n",
      "  0.02081837 -0.79067993 -0.20468979 -0.64860773 -0.2864009   0.6845896\n",
      " -0.24665298 -0.6465036  -0.40740034 -0.2957578   0.19472805 -0.3767643\n",
      " -0.78790253  0.26159838 -0.15024838  0.7150919   0.38415137  0.0502214\n",
      " -0.34151486 -0.54310465  0.14574747 -0.38095856  0.14905573 -0.0761458\n",
      "  0.32405275  0.05554063  0.2041448  -0.22336978  0.16194476  0.16817908\n",
      "  0.4853172   0.41617027 -1.2968537  -0.19350487  0.15582001 -0.11769637\n",
      " -0.21940613  0.5784884  -0.09078123  0.55350196  0.5959078  -0.34210765\n",
      "  0.42163587  0.39201993 -0.32519013 -0.22796261  0.00496388 -0.20964834\n",
      "  0.02606254 -0.8017289  -0.04792826 -0.15067603 -0.10220329 -0.4455369 ]\n"
     ]
    }
   ],
   "source": [
    "print(ft.get_word_vector(\"ðŸ¤¢\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(textline):\n",
    "    textline = re.sub('@[\\w_]+', 'USER_MENTION', textline)\n",
    "    textline = re.sub('\\|LBR\\|', '', textline)\n",
    "    textline = re.sub('\\.\\.\\.+', '...', textline)\n",
    "    textline = re.sub('!!+', '!!', textline)\n",
    "    textline = re.sub('\\?\\?+', '??', textline)\n",
    "    words = re.compile('[\\U00010000-\\U0010ffff]|[\\w-]+|[^ \\w\\U00010000-\\U0010ffff]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    # print(words)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator(filename, batch_size):\n",
    "    \n",
    "    file_length = sum(1 for line in open(filename, encoding = 'UTF-8'))\n",
    "    shuffled_indexes = range(1, file_length + 1)\n",
    "    # shuffled_indexes = sample(shuffled_indexes, len(shuffled_indexes))\n",
    "    index_position = 0\n",
    "    \n",
    "    batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "    batch_features_lg = np.zeros((batch_size, nb_sequence_length, nb_embedding2_dims))\n",
    "    # batch_features_idx = np.zeros((batch_size, nb_sequence_length))\n",
    "    batch_labels = np.zeros((batch_size, 2))\n",
    "\n",
    "    while True:\n",
    "        # print(len(features))\n",
    "        for i in range(batch_size):\n",
    "            line = linecache.getline(filename, shuffled_indexes[index_position])\n",
    "            data = line.strip().split('\\t')\n",
    "            batch_features_ft[i], batch_features_lg[i] = process_features(data[0], nb_sequence_length, nb_embedding_dims)\n",
    "            # print(batch_features_ft[i])\n",
    "            # print(batch_features_ft[i].shape)\n",
    "            batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "            index_position += 1\n",
    "            if index_position == file_length:\n",
    "                # shuffle indexes again\n",
    "                shuffled_indexes = range(1, file_length + 1)\n",
    "                # shuffled_indexes = sample(shuffled_indexes, len(shuffled_indexes))\n",
    "                index_position = 0\n",
    "                break\n",
    "        # yield [batch_features_ft, batch_features_lg], batch_labels\n",
    "        yield [batch_features_ft], batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_splitter = re.compile(\"[\\w+]|[\\W+]\", re.UNICODE)\n",
    "word_vectors_ft = {}\n",
    "def process_features(textline, nb_sequence_length, nb_embedding_dims):\n",
    "    words = twitter_tokenizer(textline)\n",
    "    # print(words)\n",
    "    features_ft = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    features_lg = np.zeros((nb_sequence_length, nb_embedding2_dims))\n",
    "    features_idx = np.zeros(nb_sequence_length)\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            word_vectors_ft[w] = wv\n",
    "        features_ft[idx] = wv\n",
    "        \n",
    "        if w in word2Idx:\n",
    "            wv = wordEmbeddings[word2Idx[w]]\n",
    "            widx = word2Idx[w]\n",
    "        else:\n",
    "            wv = wordEmbeddings[word2Idx[\"UNKNOWN_TOKEN\"]]\n",
    "            widx = word2Idx[\"UNKNOWN_TOKEN\"]\n",
    "        features_lg[idx] = wv\n",
    "        features_idx = widx\n",
    "        \n",
    "        idx = idx + 1\n",
    "    return features_ft, features_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [line.strip().split(\"\\t\") for line in open('../OffLang/sample_train.txt', encoding = \"UTF-8\")]\n",
    "dev_lines = [line.strip().split(\"\\t\") for line in open('../OffLang/sample_dev.txt', encoding = \"UTF-8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [x[0] for x in train_lines]\n",
    "train_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in train_lines])\n",
    "# train_labels = [0 if x[1] == \"OTHER\" else 1 for x in train_lines]\n",
    "\n",
    "dev_sentences = [x[0] for x in dev_lines]\n",
    "dev_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in dev_lines])\n",
    "# dev_labels = [0 if x[1] == \"OTHER\" else 1 for x in dev_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters={}\n",
    "for line in train_sentences:\n",
    "    for char in line:\n",
    "        characters[char] = True\n",
    "for line in dev_sentences:\n",
    "    for char in line:\n",
    "        characters[char] = True\n",
    "char2Idx={}\n",
    "for char in characters:\n",
    "    char2Idx[char] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "True\n",
      "[ 0.048675  0.13134  -0.103707 -0.178012 -0.095551 -0.171765 -0.482186\n",
      " -0.595483  0.40201  -0.0764    0.030391  0.053942  0.125109 -0.100393\n",
      " -0.18863  -0.040022  0.124366  0.234751  0.170881 -0.14696  -0.096499\n",
      "  0.076724  0.363462  0.15184   0.028145  0.108519 -0.384469  0.513485\n",
      " -0.080346  0.287282  0.129523 -0.205344 -0.189282  0.00303  -0.314163\n",
      " -0.253001 -0.191423 -0.3048    0.264441  0.04405  -0.715606  0.303819\n",
      "  0.421922  0.370784  0.012607  0.262544 -0.100403  0.354938  0.012659\n",
      " -0.312128  0.202812  0.053679  0.115874  0.039126  0.058098  0.092229\n",
      "  0.309017 -0.191012  0.020758 -0.226308 -0.363673  0.093612 -0.099952\n",
      "  0.361019 -0.094479 -0.422336 -0.18642  -0.236536 -0.519167  0.159582\n",
      "  0.338029 -0.18979   0.180626  0.125307 -0.662534  0.035188  0.484701\n",
      " -0.335092 -0.1776   -0.227792  0.038483 -0.3802   -0.534377 -0.782993\n",
      " -0.089097 -0.205562 -0.044407 -0.051924 -0.575704  0.018059 -0.053812\n",
      "  0.283002  0.514965 -0.486068  0.055361 -0.054333 -0.020527 -0.447207\n",
      " -0.16172  -0.323516 -0.140941  0.359253 -0.026572  0.03811  -0.262805\n",
      " -0.110643  0.071799 -0.294002  0.22942  -0.205949 -0.382046  0.401108\n",
      " -0.276722  0.214104 -0.595008  0.192986  0.455628 -0.350839 -0.045692\n",
      "  0.133086 -0.424625 -0.277588  0.024055  0.147867 -0.08691   0.571668\n",
      "  0.143583 -0.81787   0.066866  0.13196  -0.527486 -0.361454 -0.07502\n",
      " -0.335236 -0.333107  0.164847  0.156269  0.441192 -0.607866 -0.255168\n",
      "  0.330981 -0.084682  0.230935  0.307055  0.038844 -0.291582 -0.056673\n",
      "  0.462222  0.803219  0.228475 -0.201439  0.219928  0.383817  0.557068\n",
      " -0.036326 -0.242435  0.233065 -0.228331 -0.508134  0.291427 -0.519309\n",
      "  0.044088 -0.086498  0.302048  0.390556  0.271205  0.217828 -0.050916\n",
      "  0.080193 -0.043897 -0.282846 -0.112332  0.625581  0.106275  0.279431\n",
      " -0.057829 -0.098599 -0.407969 -0.477591 -0.219932  0.086368  0.131618\n",
      "  0.551522  0.111237 -0.223282  0.187202  0.068917 -0.052917  0.161887\n",
      " -0.154098 -0.164078 -0.538337  0.1796   -0.556684  0.071236 -0.328513\n",
      " -0.136216  0.262133  0.002567  0.417626 -0.21965   0.341746  0.227955\n",
      " -0.435188  0.501005 -0.03706  -0.059332  0.129726 -0.312358  0.609214\n",
      "  0.199549  0.015094  0.408183 -0.116013 -0.056285 -0.167054  0.248529\n",
      " -0.056687  0.150728 -0.297201  0.064892 -0.190823  0.408725 -0.109309\n",
      "  0.08771   0.501691  0.220294  0.574126 -0.391221 -0.062028 -0.198907\n",
      " -0.347254 -0.454964 -0.522999 -0.624555 -0.286765 -0.50417  -0.076217\n",
      " -0.326018  0.204136 -0.680892 -0.376736  0.158066 -0.312117  0.264556\n",
      "  0.146177 -0.308771 -0.008688 -0.314001  0.125771 -0.201756 -0.659012\n",
      " -0.622982 -0.040028 -0.485648 -0.177484 -0.464318  0.244852 -0.274885\n",
      " -0.251469 -0.039978 -0.176292  0.329761 -0.472007 -0.102019  0.262121\n",
      "  0.111076 -0.240045 -0.117089 -0.076003  0.616422 -0.408125  0.176903\n",
      "  0.313987 -0.799575 -0.03966   0.252346 -0.7993   -0.436084  0.070221\n",
      "  0.290749 -0.243656 -0.126341 -0.3677    0.14742  -0.125789 -0.181893\n",
      " -0.072211  0.596252 -0.484738  0.453664  0.428786  0.458826  0.23167\n",
      "  0.266693 -0.253493 -0.560137  0.243464  0.339786 -0.142555]\n",
      "True\n",
      "[ 1.337370e-01  3.119880e-01  2.753100e-02  8.878200e-02  6.563800e-01\n",
      "  3.171850e-01 -2.109500e-02  1.406300e-02  5.340040e-01  2.578490e-01\n",
      "  2.588490e-01 -1.048940e-01  3.620060e-01 -2.829930e-01 -1.473980e-01\n",
      "  4.159500e-02  2.373150e-01 -1.166190e-01 -3.209970e-01  1.039400e-01\n",
      " -1.977830e-01  3.936060e-01  4.686630e-01 -1.302900e-01  8.249750e-01\n",
      "  1.997430e-01 -1.715120e-01 -3.668870e-01 -3.890720e-01  5.100300e-02\n",
      "  3.331560e-01  2.992550e-01 -7.163090e-01  4.123360e-01  1.004790e-01\n",
      "  5.032110e-01  2.316500e-01 -3.905700e-01 -6.022000e-02 -1.865600e-02\n",
      " -1.614920e-01 -2.814600e-02 -2.197560e-01  3.904800e-01  2.563850e-01\n",
      "  3.242140e-01  4.594100e-02  2.230130e-01 -2.587380e-01  2.362860e-01\n",
      " -1.809200e-01 -3.878300e-01 -2.002290e-01 -8.909200e-02 -3.415590e-01\n",
      "  2.324870e-01  5.306480e-01 -6.425640e-01  1.317730e-01 -8.793100e-02\n",
      " -1.154690e-01 -2.851320e-01 -6.405800e-02  3.653600e-02 -6.911800e-01\n",
      " -3.942210e-01 -2.342440e-01  5.719840e-01  3.857710e-01 -2.230240e-01\n",
      " -1.377610e-01  4.562030e-01 -8.640000e-04 -2.019700e-01  1.394330e-01\n",
      " -3.189680e-01  2.269320e-01 -8.128900e-01 -1.665450e-01 -2.106860e-01\n",
      " -1.233730e-01 -2.005240e-01 -2.490880e-01 -9.616000e-03  1.376610e-01\n",
      " -3.871670e-01  1.415940e-01 -2.581360e-01 -6.848100e-02 -3.853580e-01\n",
      " -1.739150e-01 -1.205950e-01  3.701800e-02  1.290470e-01  2.718080e-01\n",
      "  1.397000e-03  4.746900e-02 -4.013470e-01  3.374180e-01  1.740330e-01\n",
      " -9.604900e-02  2.309360e-01  1.584430e-01 -1.451420e-01 -7.373700e-01\n",
      " -2.980550e-01  4.691900e-02 -5.936800e-02 -7.401200e-02 -1.220700e-02\n",
      " -6.021500e-01  5.031280e-01  4.563200e-02  2.789290e-01  2.054010e-01\n",
      " -2.391390e-01  4.018770e-01 -2.124840e-01 -4.382370e-01  8.838800e-02\n",
      "  3.819460e-01 -1.218980e-01  1.525900e-02  1.181510e-01 -3.644050e-01\n",
      " -2.683930e-01  6.898590e-01  4.830960e-01  2.731460e-01 -3.676000e-02\n",
      "  3.744300e-02  1.881210e-01  1.214724e+00  3.098700e-01  4.820860e-01\n",
      "  2.049310e-01  4.061040e-01  3.263600e-02 -4.891120e-01 -1.428150e-01\n",
      " -1.036670e-01 -2.730000e-01  8.109000e-02  1.650080e-01  4.265980e-01\n",
      " -1.844760e-01  4.881900e-02  1.883490e-01 -2.180640e-01  8.600010e-01\n",
      " -4.362220e-01  2.241100e-01 -2.195190e-01 -2.854310e-01  2.651080e-01\n",
      "  1.153370e-01 -6.132930e-01 -7.199500e-02  5.914700e-01  6.580810e-01\n",
      " -5.669110e-01  2.940930e-01  2.032530e-01 -9.924000e-03  3.152460e-01\n",
      " -4.149700e-01  4.272700e-02  1.453460e-01  2.757900e-01 -4.792810e-01\n",
      "  3.255270e-01 -3.275100e-02  4.276470e-01  1.296000e-03  9.812100e-02\n",
      "  4.664120e-01  4.249010e-01  6.795100e-01  3.243850e-01 -4.404320e-01\n",
      " -1.189840e-01  2.402580e-01 -5.652590e-01 -3.029400e-02 -2.205800e-01\n",
      "  2.131790e-01  5.143540e-01  1.872420e-01  2.343530e-01 -3.751430e-01\n",
      " -2.849710e-01  3.117460e-01 -2.105000e-03 -1.132240e-01 -4.513940e-01\n",
      "  2.399700e-01 -1.421890e-01  2.725730e-01  1.422000e-01  1.363000e-02\n",
      "  4.398800e-02 -5.415600e-02 -1.313570e-01 -6.133570e-01 -1.785390e-01\n",
      "  6.290530e-01 -3.088300e-02  1.957740e-01  5.562300e-02  5.831850e-01\n",
      " -9.027030e-01 -3.035830e-01 -2.643080e-01 -4.708150e-01 -1.036840e-01\n",
      "  8.604600e-02  1.915070e-01 -2.692180e-01  1.933100e-02 -1.229520e-01\n",
      "  4.319240e-01  5.776540e-01  1.799040e-01  2.684260e-01  2.102300e-02\n",
      "  3.026290e-01  1.055590e-01 -1.077600e-02 -1.911940e-01 -1.600120e-01\n",
      " -4.174390e-01  1.407020e-01 -4.629640e-01 -4.655920e-01 -1.786300e-02\n",
      " -5.892000e-03  1.439410e-01 -3.018000e-02 -4.274200e-01 -1.857600e-01\n",
      " -3.894300e-02  9.635600e-02  6.124000e-02  3.222340e-01 -1.011910e-01\n",
      "  8.335900e-01 -6.326000e-02 -3.348280e-01  2.663600e-01  1.604560e-01\n",
      " -1.681100e-02  7.737500e-01 -4.278070e-01 -6.816000e-03 -2.256400e-02\n",
      " -5.204990e-01  6.070600e-02  8.373060e-01 -7.067040e-01 -5.166860e-01\n",
      "  4.699710e-01  3.903760e-01 -5.749100e-02  2.760740e-01  8.649600e-02\n",
      "  5.949100e-02  2.169430e-01  7.321000e-03 -2.446620e-01 -2.866190e-01\n",
      "  2.862190e-01  1.457470e-01 -2.498320e-01 -1.953710e-01 -7.142650e-01\n",
      "  1.539660e-01  4.816600e-01 -6.295660e-01 -1.410540e-01  2.501710e-01\n",
      "  6.197390e-01  3.949930e-01  4.248100e-01 -3.513890e-01 -3.961120e-01\n",
      " -5.034130e-01 -4.600710e-01  4.257330e-01  3.248450e-01  2.037660e-01\n",
      "  1.645760e-01  4.575000e-02  1.421850e-01 -2.028220e-01  1.563550e-01\n",
      " -3.559900e-02  6.589800e-02  4.639840e-01  1.352500e-02 -5.877830e-01]\n"
     ]
    }
   ],
   "source": [
    "nb_embedding2_dims = wordEmbeddings[1].shape[0]\n",
    "# print(nb_embedding2_dims)\n",
    "# print('fÃ¼r' in word2Idx)\n",
    "# print(wordEmbeddings[word2Idx['fÃ¼r']])\n",
    "# print('Ute' in word2Idx)\n",
    "# print(wordEmbeddings[word2Idx['Ute']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_61 (InputLayer)           (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 75, 200)      320800      input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_168 (LeakyReLU)     (None, 75, 200)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_114 (Conv1D)             (None, 73, 200)      120200      leaky_re_lu_168[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_115 (Conv1D)             (None, 72, 200)      160200      leaky_re_lu_168[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_116 (Conv1D)             (None, 71, 200)      200200      leaky_re_lu_168[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_169 (LeakyReLU)     (None, 73, 200)      0           conv1d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_170 (LeakyReLU)     (None, 72, 200)      0           conv1d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_171 (LeakyReLU)     (None, 71, 200)      0           conv1d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_112 (Globa (None, 200)          0           leaky_re_lu_169[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_113 (Globa (None, 200)          0           leaky_re_lu_170[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_114 (Globa (None, 200)          0           leaky_re_lu_171[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 200)          0           global_max_pooling1d_112[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)           (None, 200)          0           global_max_pooling1d_113[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)           (None, 200)          0           global_max_pooling1d_114[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 600)          0           dropout_105[0][0]                \n",
      "                                                                 dropout_106[0][0]                \n",
      "                                                                 dropout_107[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_55 (Dense)                (None, 100)          60100       concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_172 (LeakyReLU)     (None, 100)          0           dense_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 2)            202         leaky_re_lu_172[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 861,702\n",
      "Trainable params: 861,702\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_input_embedding = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "lstm_block = Bidirectional(LSTM(100, dropout = 0.5, return_sequences=True))(model_input_embedding)\n",
    "lstm_block = LeakyReLU()(lstm_block)\n",
    "\n",
    "filter_sizes = (3, 4, 5)\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Conv1D(\n",
    "        filters = 200,\n",
    "        kernel_size = sz,\n",
    "        padding = 'valid',\n",
    "        strides = 1\n",
    "    )(lstm_block)\n",
    "    conv = LeakyReLU()(conv)\n",
    "    conv = GlobalMaxPooling1D()(conv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "    conv_blocks.append(conv)\n",
    "model_concatenated = concatenate([conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "# model_concatenated = Dropout(0.8)(model_concatenated)\n",
    "model_concatenated = Dense(100)(model_concatenated)\n",
    "model_concatenated = LeakyReLU()(model_concatenated)\n",
    "model_output = Dense(n_labels, activation = \"softmax\")(model_concatenated)\n",
    "model = Model(model_input_embedding, model_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_epoch = len(train_sentences)\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "steps_per_epoch = math.ceil(samples_per_epoch / batch_size)\n",
    "checkpoint = ModelCheckpoint('best_classification_model.h5', \n",
    "                             monitor='val_acc', \n",
    "                             verbose = 1, \n",
    "                             save_best_only = True, \n",
    "                             save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.6189 - acc: 0.6644\n",
      "Epoch 00001: val_acc improved from -inf to 0.68812, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 47s 359ms/step - loss: 0.6199 - acc: 0.6638 - val_loss: 0.6131 - val_acc: 0.6881\n",
      "Epoch 2/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.5199 - acc: 0.7524\n",
      "Epoch 00002: val_acc improved from 0.68812 to 0.74381, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 39s 297ms/step - loss: 0.5216 - acc: 0.7514 - val_loss: 0.5487 - val_acc: 0.7438\n",
      "Epoch 3/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.4794 - acc: 0.7655\n",
      "Epoch 00003: val_acc improved from 0.74381 to 0.75866, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 39s 296ms/step - loss: 0.4804 - acc: 0.7659 - val_loss: 0.5331 - val_acc: 0.7587\n",
      "Epoch 4/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.4593 - acc: 0.7870\n",
      "Epoch 00004: val_acc did not improve\n",
      "132/132 [==============================] - 39s 298ms/step - loss: 0.4611 - acc: 0.7860 - val_loss: 0.5687 - val_acc: 0.7351\n",
      "Epoch 5/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.7982\n",
      "Epoch 00005: val_acc improved from 0.75866 to 0.76856, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 39s 299ms/step - loss: 0.4355 - acc: 0.7973 - val_loss: 0.5298 - val_acc: 0.7686\n",
      "Epoch 6/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8142\n",
      "Epoch 00006: val_acc improved from 0.76856 to 0.78342, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 40s 300ms/step - loss: 0.4038 - acc: 0.8144 - val_loss: 0.4857 - val_acc: 0.7834\n",
      "Epoch 7/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8275\n",
      "Epoch 00007: val_acc did not improve\n",
      "132/132 [==============================] - 39s 298ms/step - loss: 0.3795 - acc: 0.8274 - val_loss: 0.4841 - val_acc: 0.7822\n",
      "Epoch 8/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8466\n",
      "Epoch 00008: val_acc did not improve\n",
      "132/132 [==============================] - 39s 295ms/step - loss: 0.3481 - acc: 0.8464 - val_loss: 0.5343 - val_acc: 0.7748\n",
      "Epoch 9/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8604\n",
      "Epoch 00009: val_acc did not improve\n",
      "132/132 [==============================] - 39s 296ms/step - loss: 0.3255 - acc: 0.8606 - val_loss: 0.5353 - val_acc: 0.7785\n",
      "Epoch 10/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.8643\n",
      "Epoch 00010: val_acc improved from 0.78342 to 0.78713, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 39s 297ms/step - loss: 0.3073 - acc: 0.8641 - val_loss: 0.5371 - val_acc: 0.7871\n",
      "Epoch 11/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2684 - acc: 0.8826\n",
      "Epoch 00011: val_acc improved from 0.78713 to 0.80693, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 39s 298ms/step - loss: 0.2679 - acc: 0.8826 - val_loss: 0.5409 - val_acc: 0.8069\n",
      "Epoch 12/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.8943\n",
      "Epoch 00012: val_acc did not improve\n",
      "132/132 [==============================] - 40s 302ms/step - loss: 0.2484 - acc: 0.8942 - val_loss: 0.6136 - val_acc: 0.7723\n",
      "Epoch 13/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9046\n",
      "Epoch 00013: val_acc did not improve\n",
      "132/132 [==============================] - 40s 299ms/step - loss: 0.2253 - acc: 0.9053 - val_loss: 0.7091 - val_acc: 0.7958\n",
      "Epoch 14/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9187\n",
      "Epoch 00014: val_acc did not improve\n",
      "132/132 [==============================] - 39s 299ms/step - loss: 0.2084 - acc: 0.9181 - val_loss: 0.6483 - val_acc: 0.7847\n",
      "Epoch 15/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9272\n",
      "Epoch 00015: val_acc did not improve\n",
      "132/132 [==============================] - 40s 302ms/step - loss: 0.1779 - acc: 0.9266 - val_loss: 0.7046 - val_acc: 0.7908\n",
      "Epoch 16/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9349\n",
      "Epoch 00016: val_acc did not improve\n",
      "132/132 [==============================] - 39s 295ms/step - loss: 0.1679 - acc: 0.9351 - val_loss: 0.7116 - val_acc: 0.7896\n",
      "Epoch 17/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9396\n",
      "Epoch 00017: val_acc did not improve\n",
      "132/132 [==============================] - 39s 299ms/step - loss: 0.1590 - acc: 0.9399 - val_loss: 0.6550 - val_acc: 0.7908\n",
      "Epoch 18/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9470\n",
      "Epoch 00018: val_acc did not improve\n",
      "132/132 [==============================] - 39s 297ms/step - loss: 0.1477 - acc: 0.9470 - val_loss: 0.7513 - val_acc: 0.7958\n",
      "Epoch 19/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9506\n",
      "Epoch 00019: val_acc did not improve\n",
      "132/132 [==============================] - 39s 299ms/step - loss: 0.1279 - acc: 0.9505 - val_loss: 0.7343 - val_acc: 0.7995\n",
      "Epoch 20/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9530\n",
      "Epoch 00020: val_acc did not improve\n",
      "132/132 [==============================] - 40s 300ms/step - loss: 0.1274 - acc: 0.9529 - val_loss: 0.9891 - val_acc: 0.7859\n",
      "Epoch 21/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9606\n",
      "Epoch 00021: val_acc did not improve\n",
      "132/132 [==============================] - 40s 302ms/step - loss: 0.1158 - acc: 0.9602 - val_loss: 0.8021 - val_acc: 0.8020\n",
      "Epoch 22/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9621\n",
      "Epoch 00022: val_acc did not improve\n",
      "132/132 [==============================] - 40s 300ms/step - loss: 0.1002 - acc: 0.9621 - val_loss: 0.8098 - val_acc: 0.8045\n",
      "Epoch 23/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9640\n",
      "Epoch 00023: val_acc did not improve\n",
      "132/132 [==============================] - 40s 306ms/step - loss: 0.1107 - acc: 0.9635 - val_loss: 0.6604 - val_acc: 0.8007\n",
      "Epoch 24/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9659\n",
      "Epoch 00024: val_acc did not improve\n",
      "132/132 [==============================] - 40s 301ms/step - loss: 0.0981 - acc: 0.9659 - val_loss: 0.7842 - val_acc: 0.7995\n",
      "Epoch 25/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9635\n",
      "Epoch 00025: val_acc improved from 0.80693 to 0.81559, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 40s 300ms/step - loss: 0.1085 - acc: 0.9631 - val_loss: 0.6850 - val_acc: 0.8156\n",
      "Epoch 26/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9747\n",
      "Epoch 00026: val_acc did not improve\n",
      "132/132 [==============================] - 39s 297ms/step - loss: 0.0780 - acc: 0.9749 - val_loss: 0.9588 - val_acc: 0.8057\n",
      "Epoch 27/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9723\n",
      "Epoch 00027: val_acc did not improve\n",
      "132/132 [==============================] - 40s 300ms/step - loss: 0.0900 - acc: 0.9718 - val_loss: 0.9082 - val_acc: 0.7847\n",
      "Epoch 28/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9723\n",
      "Epoch 00028: val_acc did not improve\n",
      "132/132 [==============================] - 40s 302ms/step - loss: 0.0829 - acc: 0.9723 - val_loss: 0.9193 - val_acc: 0.8119\n",
      "Epoch 29/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9785\n",
      "Epoch 00029: val_acc did not improve\n",
      "132/132 [==============================] - 39s 296ms/step - loss: 0.0698 - acc: 0.9773 - val_loss: 0.9005 - val_acc: 0.7636\n",
      "Epoch 30/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9778\n",
      "Epoch 00030: val_acc did not improve\n",
      "132/132 [==============================] - 40s 301ms/step - loss: 0.0734 - acc: 0.9780 - val_loss: 0.8272 - val_acc: 0.7995\n",
      "Epoch 31/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9747\n",
      "Epoch 00031: val_acc did not improve\n",
      "132/132 [==============================] - 40s 301ms/step - loss: 0.0680 - acc: 0.9749 - val_loss: 1.1173 - val_acc: 0.8020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9809\n",
      "Epoch 00032: val_acc did not improve\n",
      "132/132 [==============================] - 39s 293ms/step - loss: 0.0697 - acc: 0.9811 - val_loss: 0.7877 - val_acc: 0.7884\n",
      "Epoch 33/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9802\n",
      "Epoch 00033: val_acc did not improve\n",
      "132/132 [==============================] - 39s 298ms/step - loss: 0.0562 - acc: 0.9804 - val_loss: 1.0038 - val_acc: 0.8007\n",
      "Epoch 34/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9771\n",
      "Epoch 00034: val_acc did not improve\n",
      "132/132 [==============================] - 40s 301ms/step - loss: 0.0710 - acc: 0.9770 - val_loss: 0.8282 - val_acc: 0.7970\n",
      "Epoch 35/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9747\n",
      "Epoch 00035: val_acc did not improve\n",
      "132/132 [==============================] - 39s 298ms/step - loss: 0.0698 - acc: 0.9749 - val_loss: 0.9587 - val_acc: 0.7933\n",
      "Epoch 36/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9773\n",
      "Epoch 00036: val_acc did not improve\n",
      "132/132 [==============================] - 39s 299ms/step - loss: 0.0698 - acc: 0.9775 - val_loss: 0.9352 - val_acc: 0.7785\n",
      "Epoch 37/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9785\n",
      "Epoch 00037: val_acc did not improve\n",
      "132/132 [==============================] - 39s 294ms/step - loss: 0.0689 - acc: 0.9787 - val_loss: 1.0779 - val_acc: 0.7859\n",
      "Epoch 38/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9804\n",
      "Epoch 00038: val_acc did not improve\n",
      "132/132 [==============================] - 40s 300ms/step - loss: 0.0602 - acc: 0.9801 - val_loss: 0.8218 - val_acc: 0.8007\n",
      "Epoch 39/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9823\n",
      "Epoch 00039: val_acc did not improve\n",
      "132/132 [==============================] - 40s 300ms/step - loss: 0.0551 - acc: 0.9822 - val_loss: 0.8916 - val_acc: 0.8094\n",
      "Epoch 40/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9795\n",
      "Epoch 00040: val_acc did not improve\n",
      "132/132 [==============================] - 39s 298ms/step - loss: 0.0650 - acc: 0.9796 - val_loss: 0.8603 - val_acc: 0.8020\n",
      "Epoch 41/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9823\n",
      "Epoch 00041: val_acc did not improve\n",
      "132/132 [==============================] - 40s 300ms/step - loss: 0.0551 - acc: 0.9818 - val_loss: 0.9941 - val_acc: 0.8007\n",
      "Epoch 42/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9838\n",
      "Epoch 00042: val_acc did not improve\n",
      "132/132 [==============================] - 40s 301ms/step - loss: 0.0637 - acc: 0.9839 - val_loss: 0.7653 - val_acc: 0.7933\n",
      "Epoch 43/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9862\n",
      "Epoch 00043: val_acc did not improve\n",
      "132/132 [==============================] - 40s 300ms/step - loss: 0.0481 - acc: 0.9860 - val_loss: 0.9090 - val_acc: 0.8131\n",
      "Epoch 44/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9843\n",
      "Epoch 00044: val_acc did not improve\n",
      "132/132 [==============================] - 40s 301ms/step - loss: 0.0460 - acc: 0.9844 - val_loss: 1.0187 - val_acc: 0.7958\n",
      "Epoch 45/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9852\n",
      "Epoch 00045: val_acc did not improve\n",
      "132/132 [==============================] - 39s 296ms/step - loss: 0.0482 - acc: 0.9853 - val_loss: 0.9974 - val_acc: 0.8032\n",
      "Epoch 46/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9807\n",
      "Epoch 00046: val_acc did not improve\n",
      "132/132 [==============================] - 40s 301ms/step - loss: 0.0616 - acc: 0.9808 - val_loss: 0.8596 - val_acc: 0.7871\n",
      "Epoch 47/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9826\n",
      "Epoch 00047: val_acc did not improve\n",
      "132/132 [==============================] - 40s 303ms/step - loss: 0.0478 - acc: 0.9827 - val_loss: 1.0525 - val_acc: 0.7983\n",
      "Epoch 48/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9812\n",
      "Epoch 00048: val_acc did not improve\n",
      "132/132 [==============================] - 39s 299ms/step - loss: 0.0648 - acc: 0.9811 - val_loss: 0.8825 - val_acc: 0.7908\n",
      "Epoch 49/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9862\n",
      "Epoch 00049: val_acc did not improve\n",
      "132/132 [==============================] - 39s 299ms/step - loss: 0.0468 - acc: 0.9863 - val_loss: 0.9803 - val_acc: 0.7772\n",
      "Epoch 50/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9869\n",
      "Epoch 00050: val_acc did not improve\n",
      "132/132 [==============================] - 39s 296ms/step - loss: 0.0395 - acc: 0.9870 - val_loss: 1.0461 - val_acc: 0.7958\n"
     ]
    }
   ],
   "source": [
    "dev_batch_size = len(dev_sentences)\n",
    "model.fit_generator(\n",
    "    sequential_generator('/home/gwiedemann/notebooks/OffLang/sample_train.txt', batch_size), \n",
    "    steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
    "    validation_data = sequential_generator('/home/gwiedemann/notebooks/OffLang/sample_dev.txt', dev_batch_size),\n",
    "    validation_steps = math.ceil(len(dev_sentences) / dev_batch_size),\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_classification_model.h5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "testset_features_e1 = np.zeros((len(dev_sentences), nb_sequence_length, nb_embedding_dims))\n",
    "testset_features_e2 = np.zeros((len(dev_sentences), nb_sequence_length, nb_embedding2_dims))   \n",
    "for i in range(len(dev_sentences)):\n",
    "    testset_features_e1[i], testset_features_e2[i] = process_features(dev_sentences[i], nb_sequence_length, nb_embedding_dims)\n",
    "results = model.predict(testset_features_e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-Data: Prec: 0.738, Rec: 0.685, F1: 0.711, Acc: 0.816\n"
     ]
    }
   ],
   "source": [
    "idx2Label = {0 : \"OTHER\", 1 : \"OFFENSIVE\"}\n",
    "predLabels = results.argmax(axis=-1)\n",
    "devLabels = [0 if x[1] == \"OTHER\" else 1 for x in dev_lines]\n",
    "# print(idx2Label)\n",
    "# print(predLabels)\n",
    "# print(devLabels)\n",
    "f1 = f1_score(devLabels, predLabels, average='binary', pos_label=1)\n",
    "r = recall_score(devLabels, predLabels, average='binary', pos_label=1)\n",
    "p = precision_score(devLabels, predLabels, average='binary', pos_label=1)\n",
    "a = accuracy_score(devLabels, predLabels)\n",
    "print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f, Acc: %.3f\" % (p, r, f1, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
