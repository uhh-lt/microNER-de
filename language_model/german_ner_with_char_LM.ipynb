{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jindal/miniconda3/envs/NER2/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, 'jindal/NER-Bi-LSTM-CNN')\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import sklearn\n",
    "import pickle\n",
    "from keras.utils import to_categorical\n",
    "import sys\n",
    "\n",
    "epochs = 50\n",
    "trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(sentences, word2Idx, case2Idx, char2Idx):\n",
    "    #{'numeric': 0, 'allLower': 1, 'contains_digit': 6, 'PADDING_TOKEN': 7, 'other': 4, 'allUpper': 2, 'mainly_numeric': 5, 'initialUpper': 3}\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "#         labelIndices = []\n",
    "        \n",
    "        for word,char in sentence:  \n",
    "            wordCount += 1\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]                 \n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                charIdx.append(char2Idx[x])\n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "#             labelIndices.append(label2Idx[label])\n",
    "           \n",
    "        dataset.append([wordIndices,caseIndices, charIndices]) \n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]\n",
    "\n",
    "def padding(Sentences):\n",
    "    maxlen = 52\n",
    "    for sentence in Sentences:\n",
    "        char = sentence[2]\n",
    "        for x in char:\n",
    "            maxlen = max(maxlen,len(x))\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_load(filename):\n",
    "    f = open(filename,'rb')\n",
    "    text = f.read().decode()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    f = open(filename,'wb')\n",
    "    f.write(data.encode())\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = doc_load('/home/jindal/notebooks/wikipedia/de-wiki_tokenized.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan Smithee\n",
      "\n",
      "Alan Smithee steht als Pseudonym f√ºr einen fiktiven Regisseur , der Filme verantwortet , bei denen der eigentliche Regisseur seinen Namen nicht mit dem Werk in Verbindung gebracht haben \n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def clean_doc(doc):\n",
    "#     doc = doc.replace('--',' ')\n",
    "    tokens = doc.split()\n",
    "    print('done split')\n",
    "    table = str.maketrans('','', string.punctuation) # remove the punctuation \n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    print('done1')\n",
    "    tokens = [word for word in  tokens if word.isalpha()] # only take words. No numbers etc\n",
    "    print('done2')\n",
    "#     tokens = [word.lower() for word in tokens]\n",
    "#     print('done3')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done split\n",
      "done1\n",
      "done2\n"
     ]
    }
   ],
   "source": [
    "tokens = clean_doc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713941425\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokens[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokens.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pickle.load(open('tokens.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713941425\n",
      "7237133\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 50+1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "    \n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_doc(sequences, 'wikipedia_clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc_load('/home/jindal/notebooks/jindal/NER/wikipedia_clean.txt')\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Hard coded case lookup ::\n",
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "for i,line in enumerate(lines):\n",
    "    splits = line.split()\n",
    "    sentence = []\n",
    "    for word in splits:\n",
    "#         print(word)\n",
    "        temp = []\n",
    "        char = []\n",
    "        for c in word:\n",
    "            char.append(c)\n",
    "        temp.append(word)\n",
    "        temp.append(char)\n",
    "        sentence.append(temp)\n",
    "    sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickle.txt','wb') as f:\n",
    "    pickle.dump(sentences,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pickle.load(open('pickle.txt','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "characters={}\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        token = word[0]\n",
    "        words[token.lower()]=True\n",
    "        for char in word[1]:\n",
    "            characters[char]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jindal/notebooks/fastText/wiki.de.txt','wb') as f:\n",
    "    all_words = ' '.join(words.keys())\n",
    "    f.write(all_words.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(words.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Idx={}\n",
    "word2Idx={}\n",
    "wordEmbeddings=[]\n",
    "\n",
    "# created a file by the name of german_words.txt in /fastText. Containing all the words in our dataset\n",
    "with open('wikipedia_word_embeddings.txt','rb') as f:\n",
    "    for line in f:\n",
    "        splits = line.split()\n",
    "        word = splits[0].decode()\n",
    "#         print(word.decode())\n",
    "        if len(word2Idx) == 0: #Add padding+unknown\n",
    "            word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "            vector = np.zeros(len(splits)-1) #Zero vector vor 'PADDING' word\n",
    "            wordEmbeddings.append(vector)\n",
    "\n",
    "            word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "            vector = np.random.uniform(-0.25, 0.25, len(splits)-1)\n",
    "            wordEmbeddings.append(vector)\n",
    "            \n",
    "        word2Idx[word]=len(word2Idx)\n",
    "        embedding = np.array([float(num) for num in splits[1:]])\n",
    "        wordEmbeddings.append(embedding)\n",
    "wordEmbeddings=np.array(wordEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wordEmbeddings[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2Idx={}\n",
    "for char in characters:\n",
    "    char2Idx[char] = len(char2Idx)\n",
    "print(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = sentences[:40000]\n",
    "testSentences = sentences[40000:45000]\n",
    "devSentences = sentences[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createMatrices: for every sentence, changes its words, cases,characters, labels to its corresponding id in their embeddings\n",
    "# padding is used to pad the character indices to a fixed size=52\n",
    "train_set = padding(createMatrices(trainSentences,word2Idx, case2Idx, char2Idx))\n",
    "dev_set = padding(createMatrices(devSentences,word2Idx , case2Idx, char2Idx))\n",
    "test_set = padding(createMatrices(testSentences, word2Idx,case2Idx, char2Idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first is the index for word. 2nd for casing. 3rd for the char\n",
    "print(train_set[0])\n",
    "print(len(train_set[0][0]))\n",
    "print(len(train_set[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_inp=[]\n",
    "casing_inp=[]\n",
    "char_inp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_set_wikipedia.txt\",'wb') as f:\n",
    "    pickle.dump(train_set,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "for sentence in train_set:\n",
    "    word = sentence[0][50]\n",
    "    y.append(word)\n",
    "    del sentence[0][-1]\n",
    "    sentence[2] = sentence[2][:-1]\n",
    "    del sentence[1][1]\n",
    "    word_inp.append(sentence[0])\n",
    "    casing_inp.append(sentence[1])\n",
    "    char_inp.append(sentence[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_inp[0]))\n",
    "print(len(casing_inp[0]))\n",
    "print(len(char_inp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2Idx.keys())+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = to_categorical(y, num_classes = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_input = Input(shape=(None,), dtype='int32',name='words_input')\n",
    "words = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1],  weights=[wordEmbeddings], trainable=False)(words_input)\n",
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= Dropout(0.5, name='dropout1')(embed_char_out)\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, name='conv'))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(52), name='maxpool')(conv1d_out)\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "output = concatenate([words,char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=False, dropout=0.50, recurrent_dropout=0.25))(output)\n",
    "# output = TimeDistributed(Dense(vocab_size))(output)\n",
    "# crf = CRF(len(label2Idx))\n",
    "# output = crf(output)\n",
    "# output = Flatten()(output)\n",
    "output = Dense(vocab_size, activation='softmax')(output)\n",
    "# output = Flatten()(output)\n",
    "# output = Dense(vocab_size, activation='softmax')(output)\n",
    "# model.add(Dense(vocab_size, activation='softmax'))\n",
    "model = Model(inputs=[words_input,casing_input, character_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "word_inp = np.array(word_inp)\n",
    "casing_inp = np.array(casing_inp)\n",
    "char_inp = np.array(char_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_inp.shape)\n",
    "print(casing_inp.shape)\n",
    "print(char_inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([word_inp,casing_inp, char_inp],y,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save('german_ner_lm.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
