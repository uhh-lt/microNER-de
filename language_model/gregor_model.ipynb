{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "import math\n",
    "import linecache\n",
    "import numpy as np \n",
    "from numpy import random\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import re\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home/jindal/notebooks/fastText/wiki.de.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()\n",
    "nb_sequence_length = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_ATTENTION_VECTOR = True\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, nb_sequence_length))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(nb_sequence_length, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18421504  0.16461958  0.07163134 -0.3582153   0.6343416   0.57077825\n",
      " -0.44131115  0.47140062  0.35997692  1.0691209   0.5383094  -0.00801403\n",
      "  0.58150095 -0.24640413  0.07941529 -0.7907748  -0.64057297  0.87790126\n",
      "  0.1222318   0.9839732  -0.14147948  0.2741151   0.14327082  0.7455819\n",
      " -0.58181334 -0.0139227   0.13299793  0.11719222 -0.03907198 -0.98190314\n",
      "  0.6551781  -0.08076547  0.39160377  0.5933445  -0.29222807 -0.02020206\n",
      " -0.17795676 -0.32914364 -0.9572954  -0.15258092 -0.0530088   0.31237698\n",
      "  0.37407503  0.61072457 -0.0205325   0.00588962  0.3607436   0.3082963\n",
      " -0.3130489  -0.4344106  -0.4184202  -0.16960411  0.5402667  -0.00491837\n",
      "  0.11402972 -0.3362505   0.5770166   0.13003364 -0.2651122   0.28100345\n",
      "  0.07081287 -0.00930306 -0.18135485 -0.2852216  -0.04528273 -0.10418656\n",
      " -0.39689147  0.06003198 -0.20699514  0.13569123 -0.24864858 -0.1452383\n",
      " -0.08365332  0.04030139  0.11422046  0.16102162  0.2925926   0.40926033\n",
      " -0.49666372 -0.6249165  -0.57006294 -0.1205303  -0.3541435   0.34539774\n",
      "  0.08927163  0.39094543  0.12676828 -0.3391294  -0.01737639  0.41220987\n",
      "  0.18247083 -0.27371702 -0.09793004 -0.09703299 -0.23809725  0.00482424\n",
      "  0.12626016  0.06020825 -0.6792434   0.27824506 -0.3823968   0.7319919\n",
      "  0.36686167 -0.36061448  0.01484885  0.9763383   0.623409    0.39604348\n",
      "  0.57920915  0.40461656  0.06412029  0.29473352 -0.54550713 -0.1393995\n",
      " -0.34066236 -0.15662035 -0.21668887  0.24387683  0.15306745 -0.06852037\n",
      " -0.3117108   0.19708864  0.17909242  0.5010868   0.10898975 -0.16986161\n",
      "  0.9560049  -0.4135787  -0.06685685  0.1193997   0.8293503  -0.24338198\n",
      "  0.42319041  0.2755165  -0.4972959  -0.23354629 -0.37044936 -0.31695008\n",
      " -0.21605009  0.4740254  -0.70534116 -0.06025723  0.51637214  0.09974954\n",
      "  0.80836064 -0.14830334  0.54592735  0.3383062  -0.49616945 -0.20657985\n",
      "  0.23393503 -0.2635632   0.1877886   0.14345522  0.25271478  0.12416913\n",
      " -0.7456534   0.61757755  0.01874281  0.5533872   0.28126827  0.00541478\n",
      " -0.09100021  0.0444731  -0.72013533  0.00763062  0.1178807   0.0230649\n",
      "  0.24378376  0.46994466  0.45391506 -0.26565406  0.20607781  0.03113725\n",
      " -0.45015562 -0.41041908 -0.30771878 -0.52397335  0.19247746 -0.19800423\n",
      "  0.4822764   0.2509119  -0.4262801   0.08618617  0.06353179  0.34473392\n",
      " -0.21858718 -0.12085744 -0.23378809  0.5005382   0.29722637 -0.7839036\n",
      " -0.20398131  0.0272021   0.7449148  -0.6136724   0.06676061 -0.7320623\n",
      " -0.06682321 -0.8680542  -0.05150018 -0.16978203  0.07036787  0.24758786\n",
      " -0.4506678  -0.01373566 -0.66633606 -0.6107812  -0.21357138  0.276537\n",
      " -0.52194506 -0.60678613 -0.0226591  -0.06963024 -0.65762794  0.50662637\n",
      " -0.46557644  0.35206243 -0.19921587  0.11906978  0.19821647  0.06738294\n",
      " -0.06134685  0.12963602 -0.3267129   0.32797286 -0.1530252  -0.45084402\n",
      " -0.34599185 -0.3094431   0.39539948 -0.62003976 -0.43392295 -0.7782125\n",
      " -0.6875231  -0.6275458   0.05756379 -0.06242402 -0.28571332 -0.23137537\n",
      "  0.10013065  0.6900759  -0.5614342  -0.3403287  -0.00565185  0.14920928\n",
      "  0.02081837 -0.79067993 -0.20468979 -0.64860773 -0.2864009   0.6845896\n",
      " -0.24665298 -0.6465036  -0.40740034 -0.2957578   0.19472805 -0.3767643\n",
      " -0.78790253  0.26159838 -0.15024838  0.7150919   0.38415137  0.0502214\n",
      " -0.34151486 -0.54310465  0.14574747 -0.38095856  0.14905573 -0.0761458\n",
      "  0.32405275  0.05554063  0.2041448  -0.22336978  0.16194476  0.16817908\n",
      "  0.4853172   0.41617027 -1.2968537  -0.19350487  0.15582001 -0.11769637\n",
      " -0.21940613  0.5784884  -0.09078123  0.55350196  0.5959078  -0.34210765\n",
      "  0.42163587  0.39201993 -0.32519013 -0.22796261  0.00496388 -0.20964834\n",
      "  0.02606254 -0.8017289  -0.04792826 -0.15067603 -0.10220329 -0.4455369 ]\n"
     ]
    }
   ],
   "source": [
    "print(ft.get_word_vector(\"ðŸ¤¢\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(textline):\n",
    "    textline = re.sub('@[\\w_]+', 'USER_MENTION', textline)\n",
    "    textline = re.sub('\\|LBR\\|', '', textline)\n",
    "    textline = re.sub('\\.\\.\\.+', '...', textline)\n",
    "    textline = re.sub('!!+', '!!', textline)\n",
    "    textline = re.sub('\\?\\?+', '??', textline)\n",
    "    words = re.compile('[\\U00010000-\\U0010ffff]|[\\w-]+|[^ \\w\\U00010000-\\U0010ffff]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_generator(features, labels, batch_size):\n",
    "    \n",
    "    batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "    batch_features_lg = np.zeros((batch_size, nb_sequence_length, nb_embedding2_dims))\n",
    "    batch_labels = np.zeros((batch_size, 2))\n",
    "\n",
    "    while True:\n",
    "        # print(len(features))\n",
    "        for i in range(batch_size):\n",
    "            index = random.choice(len(features), 1)[0]\n",
    "            batch_features_ft[i], batch_features_lg[i] = process_features(features[index], nb_sequence_length, nb_embedding_dims)\n",
    "            # print(batch_features[i])\n",
    "            # print(batch_features[i].shape)\n",
    "            batch_labels[i] = labels[index]\n",
    "        yield [batch_features_ft, batch_features_lg], batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator(filename, batch_size):\n",
    "    \n",
    "    file_length = sum(1 for line in open(filename, encoding = 'UTF-8'))\n",
    "    shuffled_indexes = range(1, file_length + 1)\n",
    "    index_position = 0\n",
    "    \n",
    "    batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "    batch_features_lg = np.zeros((batch_size, nb_sequence_length, nb_embedding2_dims))\n",
    "    batch_labels = np.zeros((batch_size, 2)) # 2 because of simple task\n",
    "\n",
    "    while True:\n",
    "        # print(len(features))\n",
    "        for i in range(batch_size):\n",
    "            line = linecache.getline(filename, shuffled_indexes[index_position])\n",
    "            data = line.strip().split('\\t')\n",
    "            batch_features_ft[i], batch_features_lg[i] = process_features(data[0], nb_sequence_length, nb_embedding_dims)\n",
    "            # print(batch_features_ft[i])\n",
    "            # print(batch_features_ft[i].shape)\n",
    "            batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "            index_position += 1\n",
    "            if index_position == file_length:\n",
    "                # shuffle indexes again\n",
    "                shuffled_indexes = range(1, file_length + 1)\n",
    "                index_position = 0\n",
    "                break\n",
    "        yield [batch_features_ft, batch_features_lg], batch_labels\n",
    "        # yield [batch_features_ft], batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_splitter = re.compile(\"[\\w+]|[\\W+]\", re.UNICODE)\n",
    "word_vectors_ft = {}\n",
    "def process_features(textline, nb_sequence_length, nb_embedding_dims):\n",
    "    words = twitter_tokenizer(textline)\n",
    "    # print(words)\n",
    "    features_ft = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    features_lg = np.zeros((nb_sequence_length, nb_embedding2_dims))\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            word_vectors_ft[w] = wv\n",
    "        features_ft[idx] = wv\n",
    "        \n",
    "        if w in word2Idx:\n",
    "            wv = wordEmbeddings[word2Idx[w]]\n",
    "        else:\n",
    "            wv = wordEmbeddings[word2Idx[\"UNKNOWN_TOKEN\"]]\n",
    "        features_lg[idx] = wv\n",
    "        \n",
    "        idx = idx + 1\n",
    "    return features_ft, features_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charcnn_sequential_generator(filename, batch_size):\n",
    "    \n",
    "    file_length = sum(1 for line in open(filename, encoding = 'UTF-8'))\n",
    "    shuffled_indexes = range(1, file_length + 1)\n",
    "    index_position = 0\n",
    "    \n",
    "    # print(\"INITIIALIZING BATCHHH\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        # print(\"PROOODUCING  BATCHHH\")\n",
    "        \n",
    "        batch_word_embeddings = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "        batch_char_embeddings = []\n",
    "        batch_labels = np.zeros((batch_size, 2))\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            line = linecache.getline(filename, shuffled_indexes[index_position])\n",
    "            data = line.strip().split('\\t')\n",
    "            batch_word_embeddings[i], tmp_char_embeddings = charcnn_process_features(data[0], nb_sequence_length, nb_embedding_dims)\n",
    "            # print(tmp_char_embeddings.shape)\n",
    "            batch_char_embeddings.append(tmp_char_embeddings)\n",
    "            # print(batch_features_ft[i])\n",
    "            # print(batch_features_ft[i].shape)\n",
    "            batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "            index_position += 1\n",
    "            if index_position == file_length:\n",
    "                # shuffle indexes again\n",
    "                shuffled_indexes = range(1, file_length + 1)\n",
    "                index_position = 0\n",
    "                break\n",
    "        if len(batch_char_embeddings) < batch_size:\n",
    "            batch_word_embeddings = batch_word_embeddings[:len(batch_char_embeddings)]\n",
    "            batch_labels = batch_labels[:len(batch_char_embeddings)]\n",
    "        yield [batch_word_embeddings, np.array(batch_char_embeddings)], batch_labels\n",
    "\n",
    "word_vectors_ft = {}\n",
    "def charcnn_process_features(textline, nb_sequence_length, nb_embedding_dims):\n",
    "    words = twitter_tokenizer(textline)\n",
    "    # print(words)\n",
    "    word_embeddings = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    char_embeddings = np.zeros((nb_sequence_length, 52))\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            word_vectors_ft[w] = wv\n",
    "        word_embeddings[idx] = wv\n",
    "        \n",
    "        temp_char = []\n",
    "        for char in w:\n",
    "            temp_char.append(char2Idx[char])\n",
    "        for pos, c in enumerate(temp_char):\n",
    "            char_embeddings[idx][pos] = c\n",
    "\n",
    "        idx = idx + 1\n",
    "    return word_embeddings, char_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [line.strip().split(\"\\t\") for line in open('../OffLang/sample_train.txt', encoding = \"UTF-8\")]\n",
    "dev_lines = [line.strip().split(\"\\t\") for line in open('../OffLang/sample_dev.txt', encoding = \"UTF-8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [x[0] for x in train_lines]\n",
    "train_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in train_lines])\n",
    "# train_labels = [0 if x[1] == \"OTHER\" else 1 for x in train_lines]\n",
    "\n",
    "dev_sentences = [x[0] for x in dev_lines]\n",
    "dev_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in dev_lines])\n",
    "# dev_labels = [0 if x[1] == \"OTHER\" else 1 for x in dev_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters={}\n",
    "for line in train_sentences:\n",
    "    for char in line:\n",
    "        characters[char] = True\n",
    "for line in dev_sentences:\n",
    "    for char in line:\n",
    "        characters[char] = True\n",
    "char2Idx={}\n",
    "for char in characters:\n",
    "    char2Idx[char] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Idx = {}\n",
    "wordEmbeddings = []\n",
    "\n",
    "# fEmbeddings = open(\"../embeddings/model_levy_goldberg_extended.embeddings\", encoding=\"UTF-8\")\n",
    "fEmbeddings = open(\"../embeddings/embed_tweets_de_300M_52D\", encoding=\"UTF-8\")\n",
    "for line in fEmbeddings:\n",
    "    split = line.strip().split(\" \")\n",
    "    word = split[0]\n",
    "    if len(split)-1 != 52:\n",
    "        continue\n",
    "    \n",
    "    if len(word2Idx) == 0: #Add padding+unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector vor 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "    \n",
    "    vector = np.array([float(num) for num in split[1:]])\n",
    "    wordEmbeddings.append(vector)\n",
    "    word2Idx[split[0]] = len(word2Idx)\n",
    "\n",
    "wordEmbeddings = np.array(wordEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "True\n",
      "[-0.479428 -1.072284 -0.234882 -0.181851 -0.38546  -0.047229 -1.079844\n",
      "  0.999567  1.748121  0.26788   1.517133 -0.569696 -0.616138  0.537412\n",
      " -0.120462  0.897762 -1.192519 -0.96012  -0.18896   0.322002 -0.759193\n",
      "  0.037441  0.448546  0.050203  0.498242 -0.367505  0.593616  0.654069\n",
      "  0.237761 -0.970336 -0.04762  -0.596377 -0.428545 -0.933031  0.151061\n",
      " -0.417189  0.069623  0.545726  0.97318  -0.683228  0.729776 -0.954739\n",
      "  0.084312 -0.679187 -0.35613   0.499245 -0.606557 -0.989023  0.940378\n",
      " -0.685228 -0.266337  0.141203]\n",
      "True\n",
      "[ 0.561929 -0.02499   0.393823 -0.18884   0.010402  0.308434 -0.196201\n",
      "  0.172635  0.354278  0.018807  0.225498 -0.085069 -0.040902  0.04759\n",
      "  0.150173 -0.249061 -0.144079  0.016082 -0.429589 -0.296965  0.27181\n",
      " -0.033312  0.156146  0.117304  0.447623  0.019845 -0.03712   0.109375\n",
      " -0.07226  -0.301825  0.132319 -0.856695 -0.17947  -0.232972  0.234949\n",
      "  0.1591    0.026851 -0.361166 -0.297852  0.250002 -0.413429 -0.511242\n",
      " -0.384613  0.021699  0.051884 -0.264685 -0.218094  0.358726 -0.090323\n",
      "  0.212904  0.2092   -0.470325]\n"
     ]
    }
   ],
   "source": [
    "nb_embedding2_dims = wordEmbeddings[1].shape[0]\n",
    "print(nb_embedding2_dims)\n",
    "print('fÃ¼r' in word2Idx)\n",
    "print(wordEmbeddings[word2Idx['fÃ¼r']])\n",
    "print('ute' in word2Idx)\n",
    "print(wordEmbeddings[word2Idx['ute']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, recurrent_dropout = 0.5, dropout = 0.5, activation = 'relu', input_shape=(nb_sequence_length, nb_embedding_dims)),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation = 'softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     Conv1D(128, kernel_size = 3, padding = 'valid', input_shape=(nb_sequence_length, nb_embedding_dims), activation = 'relu'),\n",
    "#     MaxPooling1D(5),\n",
    "#     Flatten(),\n",
    "#     Dense(64, activation = 'relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(2, activation = 'softmax')\n",
    "# ])\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 73, 200)      180200      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 72, 200)      240200      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 71, 200)      300200      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 75, 52)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 73, 200)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 72, 200)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 71, 200)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 100)          61200       input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 200)          0           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 200)          0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 200)          0           leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 100)          0           lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 200)          0           global_max_pooling1d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 200)          0           global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 200)          0           global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 700)          0           leaky_re_lu_19[0][0]             \n",
      "                                                                 dropout_10[0][0]                 \n",
      "                                                                 dropout_11[0][0]                 \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           44864       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 64)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            130         leaky_re_lu_20[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 826,794\n",
      "Trainable params: 826,794\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = (3, 4, 5)\n",
    "model_input_ft = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "# model_layers = Dropout(0.1)(model_input)\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Conv1D(\n",
    "        filters = 200,\n",
    "        kernel_size = sz,\n",
    "        padding = 'valid',\n",
    "        strides = 1\n",
    "    )(model_input_ft)\n",
    "    conv = LeakyReLU()(conv)\n",
    "    conv = GlobalMaxPooling1D()(conv)\n",
    "    conv = Dropout(0.8)(conv)\n",
    "    # conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "model_input_lg = Input(shape = (nb_sequence_length, nb_embedding2_dims))\n",
    "# Variation 1: \n",
    "lstm_block = LSTM(100)(model_input_lg) # , dropout = 0.5, recurrent_dropout = 0.5\n",
    "lstm_block = LeakyReLU()(lstm_block)\n",
    "# Variation 2: \n",
    "# lstm_block = Bidirectional(LSTM(100))(model_input_lg)\n",
    "# lstm_block = LeakyReLU()(lstm_block)\n",
    "# Variation 3: \n",
    "# lstm_block = attention_3d_block(model_input)\n",
    "# lstm_block = LSTM(100)(lstm_block)\n",
    "# lstm_block = LeakyReLU()(lstm_block)\n",
    "# Variation 4:\n",
    "# lstm_block = LSTM(100, return_sequences = True)(model_input_ft)\n",
    "# lstm_block = LeakyReLU()(lstm_block)\n",
    "# lstm_block = AttentionDecoder(100, nb_sequence_length)(lstm_block)\n",
    "# lstm_block = Flatten()(lstm_block)\n",
    "model_concatenated = concatenate([lstm_block, conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "# model_concatenated = Dropout(0.8)(model_concatenated)\n",
    "model_concatenated = Dense(64)(model_concatenated)\n",
    "model_concatenated = LeakyReLU()(model_concatenated)\n",
    "model_output = Dense(n_labels, activation = \"softmax\")(model_concatenated)\n",
    "model = Model([model_input_ft, model_input_lg], model_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings\n",
    "words_input = Input(shape=(None,300), dtype='float32',name='words_input')\n",
    "# character embeddings\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= Dropout(0.5, name='dropout1')(embed_char_out)\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, name='conv'))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(52), name='maxpool')(conv1d_out)\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "# concatenation\n",
    "output = concatenate([words_input, char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=False, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "output = Dense(n_labels, activation='softmax')(output)\n",
    "model_charcnn = Model(inputs=[words_input, character_input], outputs=[output])\n",
    "model_charcnn.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "model_charcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_epoch = len(train_sentences)\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "steps_per_epoch = math.ceil(samples_per_epoch / batch_size)\n",
    "checkpoint = ModelCheckpoint('best_classification_model.h5', \n",
    "                             monitor='val_acc', \n",
    "                             verbose = 1, \n",
    "                             save_best_only = True, \n",
    "                             save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in charcnn_sequential_generator('../OffLang/sample_train.txt', 32):\n",
    "    # featuretype {0,1}, batch_size, words, char index vector\n",
    "    print(a[1].shape)\n",
    "    print(a[1][0].shape)\n",
    "    print(a[1][0][0].shape)\n",
    "    print(a[1][0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.7731 - acc: 0.6372\n",
      "Epoch 00001: val_acc improved from -inf to 0.69802, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 20s 150ms/step - loss: 0.7749 - acc: 0.6366 - val_loss: 0.6290 - val_acc: 0.6980\n",
      "Epoch 2/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.6105 - acc: 0.7006\n",
      "Epoch 00002: val_acc improved from 0.69802 to 0.72649, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.6096 - acc: 0.7010 - val_loss: 0.5423 - val_acc: 0.7265\n",
      "Epoch 3/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.5584 - acc: 0.7378\n",
      "Epoch 00003: val_acc improved from 0.72649 to 0.75000, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 17s 128ms/step - loss: 0.5582 - acc: 0.7379 - val_loss: 0.5294 - val_acc: 0.7500\n",
      "Epoch 4/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.5171 - acc: 0.7655\n",
      "Epoch 00004: val_acc improved from 0.75000 to 0.76238, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.5171 - acc: 0.7649 - val_loss: 0.4878 - val_acc: 0.7624\n",
      "Epoch 5/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.4807 - acc: 0.7841\n",
      "Epoch 00005: val_acc improved from 0.76238 to 0.76609, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 17s 130ms/step - loss: 0.4812 - acc: 0.7829 - val_loss: 0.4878 - val_acc: 0.7661\n",
      "Epoch 6/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.7989\n",
      "Epoch 00006: val_acc improved from 0.76609 to 0.78094, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 0.4597 - acc: 0.7983 - val_loss: 0.4711 - val_acc: 0.7809\n",
      "Epoch 7/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.4400 - acc: 0.8087\n",
      "Epoch 00007: val_acc did not improve\n",
      "132/132 [==============================] - 17s 130ms/step - loss: 0.4395 - acc: 0.8089 - val_loss: 0.4750 - val_acc: 0.7673\n",
      "Epoch 8/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.8251\n",
      "Epoch 00008: val_acc did not improve\n",
      "132/132 [==============================] - 17s 130ms/step - loss: 0.4213 - acc: 0.8253 - val_loss: 0.4673 - val_acc: 0.7710\n",
      "Epoch 9/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8447\n",
      "Epoch 00009: val_acc improved from 0.78094 to 0.78465, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 17s 132ms/step - loss: 0.3917 - acc: 0.8447 - val_loss: 0.4726 - val_acc: 0.7847\n",
      "Epoch 10/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3828 - acc: 0.8488\n",
      "Epoch 00010: val_acc did not improve\n",
      "132/132 [==============================] - 17s 130ms/step - loss: 0.3826 - acc: 0.8482 - val_loss: 0.4928 - val_acc: 0.7710\n",
      "Epoch 11/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3573 - acc: 0.8612\n",
      "Epoch 00011: val_acc did not improve\n",
      "132/132 [==============================] - 17s 128ms/step - loss: 0.3565 - acc: 0.8613 - val_loss: 0.5121 - val_acc: 0.7797\n",
      "Epoch 12/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3419 - acc: 0.8671\n",
      "Epoch 00012: val_acc did not improve\n",
      "132/132 [==============================] - 17s 127ms/step - loss: 0.3407 - acc: 0.8679 - val_loss: 0.5013 - val_acc: 0.7785\n",
      "Epoch 13/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8793\n",
      "Epoch 00013: val_acc did not improve\n",
      "132/132 [==============================] - 17s 130ms/step - loss: 0.3293 - acc: 0.8788 - val_loss: 0.5418 - val_acc: 0.7772\n",
      "Epoch 14/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8822\n",
      "Epoch 00014: val_acc did not improve\n",
      "132/132 [==============================] - 17s 127ms/step - loss: 0.3111 - acc: 0.8823 - val_loss: 0.5405 - val_acc: 0.7636\n",
      "Epoch 15/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.8991\n",
      "Epoch 00015: val_acc did not improve\n",
      "132/132 [==============================] - 17s 130ms/step - loss: 0.2873 - acc: 0.8996 - val_loss: 0.5402 - val_acc: 0.7698\n",
      "Epoch 16/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3027 - acc: 0.8986\n",
      "Epoch 00016: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.3022 - acc: 0.8989 - val_loss: 0.5712 - val_acc: 0.7735\n",
      "Epoch 17/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.9017\n",
      "Epoch 00017: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.2912 - acc: 0.9018 - val_loss: 0.5766 - val_acc: 0.7785\n",
      "Epoch 18/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9136\n",
      "Epoch 00018: val_acc did not improve\n",
      "132/132 [==============================] - 17s 130ms/step - loss: 0.2659 - acc: 0.9141 - val_loss: 0.5843 - val_acc: 0.7785\n",
      "Epoch 19/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2430 - acc: 0.9184\n",
      "Epoch 00019: val_acc did not improve\n",
      "132/132 [==============================] - 17s 130ms/step - loss: 0.2436 - acc: 0.9183 - val_loss: 0.6075 - val_acc: 0.7785\n",
      "Epoch 20/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9251\n",
      "Epoch 00020: val_acc did not improve\n",
      "132/132 [==============================] - 17s 128ms/step - loss: 0.2431 - acc: 0.9254 - val_loss: 0.6468 - val_acc: 0.7847\n",
      "Epoch 21/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2506 - acc: 0.9320\n",
      "Epoch 00021: val_acc did not improve\n",
      "132/132 [==============================] - 17s 128ms/step - loss: 0.2500 - acc: 0.9321 - val_loss: 0.7125 - val_acc: 0.7797\n",
      "Epoch 22/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.9244\n",
      "Epoch 00022: val_acc did not improve\n",
      "132/132 [==============================] - 17s 127ms/step - loss: 0.2678 - acc: 0.9245 - val_loss: 0.6385 - val_acc: 0.7797\n",
      "Epoch 23/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9292\n",
      "Epoch 00023: val_acc improved from 0.78465 to 0.79084, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.2540 - acc: 0.9287 - val_loss: 0.6577 - val_acc: 0.7908\n",
      "Epoch 24/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9320\n",
      "Epoch 00024: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.2432 - acc: 0.9321 - val_loss: 0.6768 - val_acc: 0.7797\n",
      "Epoch 25/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9351\n",
      "Epoch 00025: val_acc did not improve\n",
      "132/132 [==============================] - 17s 128ms/step - loss: 0.2150 - acc: 0.9354 - val_loss: 0.6802 - val_acc: 0.7822\n",
      "Epoch 26/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9401\n",
      "Epoch 00026: val_acc did not improve\n",
      "132/132 [==============================] - 17s 128ms/step - loss: 0.2107 - acc: 0.9401 - val_loss: 0.6742 - val_acc: 0.7797\n",
      "Epoch 27/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9394\n",
      "Epoch 00027: val_acc did not improve\n",
      "132/132 [==============================] - 17s 127ms/step - loss: 0.2255 - acc: 0.9394 - val_loss: 0.6403 - val_acc: 0.7772\n",
      "Epoch 28/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9423\n",
      "Epoch 00028: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.2474 - acc: 0.9422 - val_loss: 0.7671 - val_acc: 0.7649\n",
      "Epoch 29/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9427\n",
      "Epoch 00029: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.2236 - acc: 0.9427 - val_loss: 0.7159 - val_acc: 0.7859\n",
      "Epoch 30/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9466\n",
      "Epoch 00030: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.1859 - acc: 0.9465 - val_loss: 0.7560 - val_acc: 0.7611\n",
      "Epoch 31/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9490\n",
      "Epoch 00031: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.2091 - acc: 0.9493 - val_loss: 0.7171 - val_acc: 0.7748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9511\n",
      "Epoch 00032: val_acc did not improve\n",
      "132/132 [==============================] - 17s 130ms/step - loss: 0.2044 - acc: 0.9515 - val_loss: 0.7919 - val_acc: 0.7710\n",
      "Epoch 33/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9537\n",
      "Epoch 00033: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.1963 - acc: 0.9536 - val_loss: 0.7423 - val_acc: 0.7884\n",
      "Epoch 34/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9463\n",
      "Epoch 00034: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.2151 - acc: 0.9467 - val_loss: 0.7673 - val_acc: 0.7822\n",
      "Epoch 35/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9609\n",
      "Epoch 00035: val_acc did not improve\n",
      "132/132 [==============================] - 17s 127ms/step - loss: 0.1780 - acc: 0.9609 - val_loss: 0.8169 - val_acc: 0.7698\n",
      "Epoch 36/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9556\n",
      "Epoch 00036: val_acc did not improve\n",
      "132/132 [==============================] - 17s 127ms/step - loss: 0.2199 - acc: 0.9555 - val_loss: 0.7854 - val_acc: 0.7661\n",
      "Epoch 37/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9542\n",
      "Epoch 00037: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.2175 - acc: 0.9543 - val_loss: 0.8637 - val_acc: 0.7785\n",
      "Epoch 38/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9630\n",
      "Epoch 00038: val_acc did not improve\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 0.1793 - acc: 0.9628 - val_loss: 0.8547 - val_acc: 0.7797\n",
      "Epoch 39/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9583\n",
      "Epoch 00039: val_acc did not improve\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 0.2026 - acc: 0.9576 - val_loss: 0.8469 - val_acc: 0.7636\n",
      "Epoch 40/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9628\n",
      "Epoch 00040: val_acc did not improve\n",
      "132/132 [==============================] - 17s 128ms/step - loss: 0.1835 - acc: 0.9631 - val_loss: 0.7947 - val_acc: 0.7475\n",
      "Epoch 41/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9563\n",
      "Epoch 00041: val_acc did not improve\n",
      "132/132 [==============================] - 17s 130ms/step - loss: 0.2012 - acc: 0.9564 - val_loss: 0.8169 - val_acc: 0.7735\n",
      "Epoch 42/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9585\n",
      "Epoch 00042: val_acc did not improve\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 0.2009 - acc: 0.9588 - val_loss: 0.8042 - val_acc: 0.7735\n",
      "Epoch 43/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1568 - acc: 0.9692\n",
      "Epoch 00043: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.1556 - acc: 0.9695 - val_loss: 0.8405 - val_acc: 0.7624\n",
      "Epoch 44/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9597\n",
      "Epoch 00044: val_acc did not improve\n",
      "132/132 [==============================] - 17s 128ms/step - loss: 0.1724 - acc: 0.9598 - val_loss: 0.7801 - val_acc: 0.7686\n",
      "Epoch 45/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9635\n",
      "Epoch 00045: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.1841 - acc: 0.9633 - val_loss: 0.9839 - val_acc: 0.7748\n",
      "Epoch 46/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9630\n",
      "Epoch 00046: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.2018 - acc: 0.9631 - val_loss: 1.0310 - val_acc: 0.7710\n",
      "Epoch 47/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9616\n",
      "Epoch 00047: val_acc did not improve\n",
      "132/132 [==============================] - 17s 128ms/step - loss: 0.1923 - acc: 0.9616 - val_loss: 0.9398 - val_acc: 0.7673\n",
      "Epoch 48/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9618\n",
      "Epoch 00048: val_acc did not improve\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 0.2007 - acc: 0.9621 - val_loss: 0.9619 - val_acc: 0.7599\n",
      "Epoch 49/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9571\n",
      "Epoch 00049: val_acc did not improve\n",
      "132/132 [==============================] - 17s 130ms/step - loss: 0.2068 - acc: 0.9569 - val_loss: 0.8965 - val_acc: 0.7599\n",
      "Epoch 50/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9611\n",
      "Epoch 00050: val_acc did not improve\n",
      "132/132 [==============================] - 17s 129ms/step - loss: 0.1915 - acc: 0.9614 - val_loss: 1.0105 - val_acc: 0.7661\n"
     ]
    }
   ],
   "source": [
    "dev_batch_size = len(dev_sentences)\n",
    "model.fit_generator(\n",
    "    sequential_generator('../OffLang/sample_train.txt', batch_size), \n",
    "    steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
    "    validation_data = sequential_generator('../OffLang/sample_dev.txt', dev_batch_size),\n",
    "    validation_steps = math.ceil(len(dev_sentences) / dev_batch_size),\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_batch_size = len(dev_sentences)\n",
    "model_charcnn.fit_generator(\n",
    "    charcnn_sequential_generator('../OffLang/sample_train.txt', batch_size), \n",
    "    steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
    "    validation_data = charcnn_sequential_generator('../OffLang/sample_dev.txt', dev_batch_size),\n",
    "    validation_steps = math.ceil(len(dev_sentences) / dev_batch_size),\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "model_charcnn.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_charcnn.load_weights('best_classification_model.h5')\n",
    "model_charcnn.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "testset_features_e1 = np.zeros((len(dev_sentences), nb_sequence_length, nb_embedding_dims))\n",
    "testset_features_e2 = np.zeros((len(dev_sentences), nb_sequence_length, 52))   \n",
    "for i in range(len(dev_sentences)):\n",
    "    testset_features_e1[i], testset_features_e2[i] = charcnn_process_features(dev_sentences[i], nb_sequence_length, nb_embedding_dims)\n",
    "results = model_charcnn.predict([testset_features_e1, testset_features_e2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2Label = {0 : \"OTHER\", 1 : \"OFFENSIVE\"}\n",
    "predLabels = results.argmax(axis=-1)\n",
    "devLabels = [0 if x[1] == \"OTHER\" else 1 for x in dev_lines]\n",
    "# print(idx2Label)\n",
    "# print(predLabels)\n",
    "# print(devLabels)\n",
    "f1 = f1_score(devLabels, predLabels, average='binary', pos_label=1)\n",
    "r = recall_score(devLabels, predLabels, average='binary', pos_label=1)\n",
    "p = precision_score(devLabels, predLabels, average='binary', pos_label=1)\n",
    "a = accuracy_score(devLabels, predLabels)\n",
    "print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f, Acc: %.3f\" % (p, r, f1, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP / NP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
