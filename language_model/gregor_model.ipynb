{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jindal/miniconda3/envs/NER2/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import fastText\n",
    "import math\n",
    "import linecache\n",
    "import numpy as np \n",
    "from random import shuffle\n",
    "from numpy import random\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import re\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home/jindal/notebooks/fastText/wiki.de.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()\n",
    "nb_sequence_length = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_ATTENTION_VECTOR = True\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, nb_sequence_length))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(nb_sequence_length, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(textline):\n",
    "    textline = re.sub('@[\\w_]+', 'USER_MENTION', textline)\n",
    "    textline = re.sub('\\|LBR\\|', '', textline)\n",
    "    textline = re.sub('\\.\\.\\.+', '...', textline)\n",
    "    textline = re.sub('!!+', '!!', textline)\n",
    "    textline = re.sub('\\?\\?+', '??', textline)\n",
    "    words = re.compile('[\\U00010000-\\U0010ffff]|[\\w-]+|[^ \\w\\U00010000-\\U0010ffff]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_generator(features, labels, batch_size):\n",
    "    \n",
    "    batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "    batch_features_lg = np.zeros((batch_size, nb_sequence_length, nb_embedding2_dims))\n",
    "    batch_labels = np.zeros((batch_size, 2))\n",
    "\n",
    "    while True:\n",
    "        # print(len(features))\n",
    "        for i in range(batch_size):\n",
    "            index = random.choice(len(features), 1)[0]\n",
    "            batch_features_ft[i], batch_features_lg[i] = process_features(features[index], nb_sequence_length, nb_embedding_dims)\n",
    "            # print(batch_features[i])\n",
    "            # print(batch_features[i].shape)\n",
    "            batch_labels[i] = labels[index]\n",
    "        yield [batch_features_ft, batch_features_lg], batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator(filename, batch_size):\n",
    "    \n",
    "    file_length = sum(1 for line in open(filename, encoding = 'UTF-8'))\n",
    "    shuffled_indexes = range(1, file_length + 1) # start from 1 because linecache is 1 indexed\n",
    "#     shuffle(shuffled_indexes)\n",
    "    \n",
    "    index_position = 0\n",
    "    \n",
    "    batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "#     batch_features_lg = np.zeros((batch_size, nb_sequence_length, nb_embedding2_dims)) # levy goldberg: not needed\n",
    "    batch_labels = np.zeros((batch_size, 2)) # 2 because of simple task\n",
    "\n",
    "    while True:\n",
    "        # print(len(features))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            line = linecache.getline(filename, shuffled_indexes[index_position])\n",
    "            data = line.strip().split('\\t')\n",
    "            batch_features_ft[i] = process_features(data[0], nb_sequence_length, nb_embedding_dims)\n",
    "            # print(batch_features_ft[i])\n",
    "            # print(batch_features_ft[i].shape)\n",
    "            batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "            index_position += 1\n",
    "            if index_position == file_length:\n",
    "                # shuffle indexes again\n",
    "                shuffled_indexes = range(1, file_length + 1)\n",
    "#                 shuffle(shuffled_indexes)\n",
    "                index_position = 0\n",
    "                break\n",
    "        yield [batch_features_ft], batch_labels\n",
    "        # yield [batch_features_ft], batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_splitter = re.compile(\"[\\w+]|[\\W+]\", re.UNICODE)\n",
    "word_vectors_ft = {}\n",
    "def process_features(textline, nb_sequence_length, nb_embedding_dims):\n",
    "    words = twitter_tokenizer(textline)\n",
    "    # print(words)\n",
    "    features_ft = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "#     features_lg = np.zeros((nb_sequence_length, nb_embedding2_dims))\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            word_vectors_ft[w] = wv\n",
    "        features_ft[idx] = wv\n",
    "        \n",
    "#         if w in word2Idx:\n",
    "#             wv = wordEmbeddings[word2Idx[w]]\n",
    "#         else:\n",
    "#             wv = wordEmbeddings[word2Idx[\"UNKNOWN_TOKEN\"]]\n",
    "#         features_lg[idx] = wv\n",
    "        \n",
    "        idx = idx + 1\n",
    "    return features_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charcnn_sequential_generator(filename, batch_size):\n",
    "    \n",
    "    file_length = sum(1 for line in open(filename, encoding = 'UTF-8'))\n",
    "    shuffled_indexes = list(range(1, file_length + 1))\n",
    "    shuffle(shuffled_indexes)\n",
    "    index_position = 0\n",
    "    \n",
    "    # print(\"INITIIALIZING BATCHHH\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        # print(\"PROOODUCING  BATCHHH\")\n",
    "        \n",
    "        batch_word_embeddings = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "        batch_char_embeddings = []\n",
    "        batch_labels = np.zeros((batch_size, 2)) # 2 for the simple task\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            line = linecache.getline(filename, shuffled_indexes[index_position])\n",
    "            data = line.strip().split('\\t')\n",
    "            batch_word_embeddings[i], tmp_char_embeddings = charcnn_process_features(data[0], nb_sequence_length, nb_embedding_dims)\n",
    "            # print(tmp_char_embeddings.shape)\n",
    "            batch_char_embeddings.append(tmp_char_embeddings)\n",
    "            # print(batch_features_ft[i])\n",
    "            # print(batch_features_ft[i].shape)\n",
    "            batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "            index_position += 1\n",
    "            if index_position == file_length:\n",
    "                # shuffle indexes again\n",
    "                shuffled_indexes = range(1, file_length + 1)\n",
    "                shuffle(shuffled_indexes)\n",
    "                index_position = 0\n",
    "                break\n",
    "        if len(batch_char_embeddings) < batch_size:\n",
    "            batch_word_embeddings = batch_word_embeddings[:len(batch_char_embeddings)]\n",
    "            batch_labels = batch_labels[:len(batch_char_embeddings)]\n",
    "        yield [batch_word_embeddings, np.array(batch_char_embeddings)], batch_labels\n",
    "\n",
    "word_vectors_ft = {}\n",
    "def charcnn_process_features(textline, nb_sequence_length, nb_embedding_dims):\n",
    "    words = twitter_tokenizer(textline)\n",
    "    # print(words)\n",
    "    word_embeddings = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    char_embeddings = np.zeros((nb_sequence_length, 52))\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            word_vectors_ft[w] = wv\n",
    "        word_embeddings[idx] = wv\n",
    "        \n",
    "        temp_char = []\n",
    "        for char in w:\n",
    "            temp_char.append(char2Idx[char])\n",
    "        for pos, c in enumerate(temp_char):\n",
    "            char_embeddings[idx][pos] = c\n",
    "\n",
    "        idx = idx + 1\n",
    "    return word_embeddings, char_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [line.strip().split(\"\\t\") for line in open('/home/gwiedemann/notebooks/OffLang/sample_train.txt', encoding = \"UTF-8\")]\n",
    "dev_lines = [line.strip().split(\"\\t\") for line in open('/home/gwiedemann/notebooks/OffLang/sample_dev.txt', encoding = \"UTF-8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [x[0] for x in train_lines]\n",
    "train_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in train_lines])\n",
    "# train_labels = [0 if x[1] == \"OTHER\" else 1 for x in train_lines]\n",
    "\n",
    "dev_sentences = [x[0] for x in dev_lines]\n",
    "dev_labels = to_categorical([0 if x[1] == \"OTHER\" else 1 for x in dev_lines])\n",
    "# dev_labels = [0 if x[1] == \"OTHER\" else 1 for x in dev_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters={}\n",
    "for line in train_sentences:\n",
    "    for char in line:\n",
    "        characters[char] = True\n",
    "for line in dev_sentences:\n",
    "    for char in line:\n",
    "        characters[char] = True\n",
    "char2Idx={}\n",
    "for char in characters:\n",
    "    char2Idx[char] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Idx = {}\n",
    "wordEmbeddings = []\n",
    "\n",
    "# fEmbeddings = open(\"../embeddings/model_levy_goldberg_extended.embeddings\", encoding=\"UTF-8\")\n",
    "fEmbeddings = open(\"../embeddings/embed_tweets_de_300M_52D\", encoding=\"UTF-8\")\n",
    "for line in fEmbeddings:\n",
    "    split = line.strip().split(\" \")\n",
    "    word = split[0]\n",
    "    if len(split)-1 != 52:\n",
    "        continue\n",
    "    \n",
    "    if len(word2Idx) == 0: #Add padding+unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector vor 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "    \n",
    "    vector = np.array([float(num) for num in split[1:]])\n",
    "    wordEmbeddings.append(vector)\n",
    "    word2Idx[split[0]] = len(word2Idx)\n",
    "\n",
    "wordEmbeddings = np.array(wordEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_embedding2_dims = wordEmbeddings[1].shape[0]\n",
    "print(nb_embedding2_dims)\n",
    "print('für' in word2Idx)\n",
    "print(wordEmbeddings[word2Idx['für']])\n",
    "print('ute' in word2Idx)\n",
    "print(wordEmbeddings[word2Idx['ute']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, recurrent_dropout = 0.5, dropout = 0.5, activation = 'relu', input_shape=(nb_sequence_length, nb_embedding_dims)),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation = 'softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     Conv1D(128, kernel_size = 3, padding = 'valid', input_shape=(nb_sequence_length, nb_embedding_dims), activation = 'relu'),\n",
    "#     MaxPooling1D(5),\n",
    "#     Flatten(),\n",
    "#     Dense(64, activation = 'relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(2, activation = 'softmax')\n",
    "# ])\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 73, 200)      180200      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 72, 200)      240200      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 71, 200)      300200      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 73, 200)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 72, 200)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 71, 200)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 100)          160400      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 200)          0           leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 200)          0           leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 200)          0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 100)          0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 200)          0           global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 200)          0           global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 200)          0           global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 700)          0           leaky_re_lu_14[0][0]             \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           44864       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 64)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            130         leaky_re_lu_15[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 925,994\n",
      "Trainable params: 925,994\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = (3, 4, 5)\n",
    "model_input_ft = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "# model_layers = Dropout(0.1)(model_input)\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Conv1D(\n",
    "        filters = 200,\n",
    "        kernel_size = sz,\n",
    "        padding = 'valid',\n",
    "        strides = 1\n",
    "    )(model_input_ft)\n",
    "    conv = LeakyReLU()(conv)\n",
    "    conv = GlobalMaxPooling1D()(conv)\n",
    "    conv = Dropout(0.8)(conv)\n",
    "    # conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "# model_input_lg = Input(shape = (nb_sequence_length, nb_embedding2_dims))\n",
    "# model_input_ft = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "# Variation 1: \n",
    "lstm_block = LSTM(100)(model_input_ft) # , dropout = 0.5, recurrent_dropout = 0.5\n",
    "lstm_block = LeakyReLU()(lstm_block)\n",
    "# Variation 2: \n",
    "# lstm_block = Bidirectional(LSTM(100))(model_input_lg)\n",
    "# lstm_block = LeakyReLU()(lstm_block)\n",
    "# Variation 3: \n",
    "# lstm_block = attention_3d_block(model_input)\n",
    "# lstm_block = LSTM(100)(lstm_block)\n",
    "# lstm_block = LeakyReLU()(lstm_block)\n",
    "# Variation 4:\n",
    "# lstm_block = LSTM(100, return_sequences = True)(model_input_ft)\n",
    "# lstm_block = LeakyReLU()(lstm_block)\n",
    "# lstm_block = AttentionDecoder(100, nb_sequence_length)(lstm_block)\n",
    "# lstm_block = Flatten()(lstm_block)\n",
    "model_concatenated = concatenate([lstm_block, conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "# model_concatenated = Dropout(0.8)(model_concatenated)\n",
    "model_concatenated = Dense(64)(model_concatenated)\n",
    "model_concatenated = LeakyReLU()(model_concatenated)\n",
    "model_output = Dense(n_labels, activation = \"softmax\")(model_concatenated)\n",
    "model = Model([model_input_ft], model_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings\n",
    "words_input = Input(shape=(None,300), dtype='float32',name='words_input')\n",
    "# character embeddings\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= Dropout(0.5, name='dropout1')(embed_char_out)\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, name='conv'))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(52), name='maxpool')(conv1d_out)\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "# concatenation\n",
    "output = concatenate([words_input, char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=False, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "output = Dense(n_labels, activation='softmax')(output)\n",
    "model_charcnn = Model(inputs=[words_input, character_input], outputs=[output])\n",
    "model_charcnn.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "model_charcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_epoch = len(train_sentences)\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "steps_per_epoch = math.ceil(samples_per_epoch / batch_size)\n",
    "checkpoint = ModelCheckpoint('best_classification_model.h5', \n",
    "                             monitor='val_acc', \n",
    "#                              verbose = 1, \n",
    "                             save_best_only = True, \n",
    "                             save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in charcnn_sequential_generator('../OffLang/sample_train.txt', 32):\n",
    "    # featuretype {0,1}, batch_size, words, char index vector\n",
    "    print(a[1].shape)\n",
    "    print(a[1][0].shape)\n",
    "    print(a[1][0][0].shape)\n",
    "    print(a[1][0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.8352 - acc: 0.6458\n",
      "Epoch 00001: val_acc did not improve\n",
      "132/132 [==============================] - 18s 139ms/step - loss: 0.8397 - acc: 0.6456 - val_loss: 0.7480 - val_acc: 0.7153\n",
      "Epoch 2/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.6414 - acc: 0.7366\n",
      "Epoch 00002: val_acc did not improve\n",
      "132/132 [==============================] - 17s 127ms/step - loss: 0.6410 - acc: 0.7360 - val_loss: 0.6195 - val_acc: 0.6646\n",
      "Epoch 3/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.7736\n",
      "Epoch 00003: val_acc improved from 0.74257 to 0.75248, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 17s 127ms/step - loss: 0.5104 - acc: 0.7730 - val_loss: 0.4937 - val_acc: 0.7525\n",
      "Epoch 4/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.4623 - acc: 0.8115\n",
      "Epoch 00004: val_acc improved from 0.75248 to 0.76114, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 16s 123ms/step - loss: 0.4622 - acc: 0.8116 - val_loss: 0.4930 - val_acc: 0.7611\n",
      "Epoch 5/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8402\n",
      "Epoch 00005: val_acc improved from 0.76114 to 0.78342, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 17s 125ms/step - loss: 0.4069 - acc: 0.8404 - val_loss: 0.4988 - val_acc: 0.7834\n",
      "Epoch 6/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8533\n",
      "Epoch 00006: val_acc improved from 0.78342 to 0.78465, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 16s 123ms/step - loss: 0.3575 - acc: 0.8535 - val_loss: 0.5048 - val_acc: 0.7847\n",
      "Epoch 7/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.8817\n",
      "Epoch 00007: val_acc improved from 0.78465 to 0.78589, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 17s 125ms/step - loss: 0.3030 - acc: 0.8816 - val_loss: 0.5629 - val_acc: 0.7859\n",
      "Epoch 8/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9127\n",
      "Epoch 00008: val_acc improved from 0.78589 to 0.78960, saving model to best_classification_model.h5\n",
      "132/132 [==============================] - 17s 126ms/step - loss: 0.2418 - acc: 0.9131 - val_loss: 0.6329 - val_acc: 0.7896\n",
      "Epoch 9/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.9208\n",
      "Epoch 00009: val_acc did not improve\n",
      "132/132 [==============================] - 18s 138ms/step - loss: 0.2378 - acc: 0.9207 - val_loss: 0.6620 - val_acc: 0.7748\n",
      "Epoch 10/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9494\n",
      "Epoch 00010: val_acc did not improve\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 0.1802 - acc: 0.9496 - val_loss: 0.7662 - val_acc: 0.7661\n",
      "Epoch 11/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1681 - acc: 0.9563\n",
      "Epoch 00011: val_acc did not improve\n",
      "132/132 [==============================] - 17s 131ms/step - loss: 0.1672 - acc: 0.9564 - val_loss: 0.8800 - val_acc: 0.7624\n",
      "Epoch 12/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9628\n",
      "Epoch 00012: val_acc did not improve\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 0.1386 - acc: 0.9626 - val_loss: 0.8213 - val_acc: 0.7686\n",
      "Epoch 13/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9707\n",
      "Epoch 00013: val_acc did not improve\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 0.1371 - acc: 0.9706 - val_loss: 1.0090 - val_acc: 0.7710\n",
      "Epoch 14/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9747\n",
      "Epoch 00014: val_acc did not improve\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 0.1048 - acc: 0.9749 - val_loss: 1.0251 - val_acc: 0.7822\n",
      "Epoch 15/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.9719\n",
      "Epoch 00015: val_acc did not improve\n",
      "132/132 [==============================] - 18s 133ms/step - loss: 0.1316 - acc: 0.9718 - val_loss: 0.8735 - val_acc: 0.7698\n",
      "Epoch 16/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9778\n",
      "Epoch 00016: val_acc did not improve\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 0.1173 - acc: 0.9777 - val_loss: 0.9533 - val_acc: 0.7760\n",
      "Epoch 17/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9814\n",
      "Epoch 00017: val_acc did not improve\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 0.1182 - acc: 0.9813 - val_loss: 1.1834 - val_acc: 0.7735\n",
      "Epoch 18/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9752\n",
      "Epoch 00018: val_acc did not improve\n",
      "132/132 [==============================] - 17s 132ms/step - loss: 0.1182 - acc: 0.9754 - val_loss: 0.9596 - val_acc: 0.7748\n",
      "Epoch 19/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9831\n",
      "Epoch 00019: val_acc did not improve\n",
      "132/132 [==============================] - 18s 135ms/step - loss: 0.1003 - acc: 0.9832 - val_loss: 1.1618 - val_acc: 0.7785\n",
      "Epoch 20/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9812\n",
      "Epoch 00020: val_acc did not improve\n",
      "132/132 [==============================] - 18s 136ms/step - loss: 0.1353 - acc: 0.9813 - val_loss: 0.9838 - val_acc: 0.7710\n",
      "Epoch 21/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9831\n",
      "Epoch 00021: val_acc did not improve\n",
      "132/132 [==============================] - 18s 133ms/step - loss: 0.1205 - acc: 0.9825 - val_loss: 1.0001 - val_acc: 0.7686\n",
      "Epoch 22/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9871\n",
      "Epoch 00022: val_acc did not improve\n",
      "132/132 [==============================] - 18s 135ms/step - loss: 0.0951 - acc: 0.9870 - val_loss: 1.1154 - val_acc: 0.7698\n",
      "Epoch 23/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9802\n",
      "Epoch 00023: val_acc did not improve\n",
      "132/132 [==============================] - 18s 134ms/step - loss: 0.1092 - acc: 0.9799 - val_loss: 1.0897 - val_acc: 0.7673\n",
      "Epoch 24/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9809\n",
      "Epoch 00024: val_acc did not improve\n",
      "132/132 [==============================] - 18s 135ms/step - loss: 0.1571 - acc: 0.9806 - val_loss: 1.7649 - val_acc: 0.7723\n",
      "Epoch 25/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9804\n",
      "Epoch 00025: val_acc did not improve\n",
      "132/132 [==============================] - 19s 143ms/step - loss: 0.1197 - acc: 0.9804 - val_loss: 1.5219 - val_acc: 0.7649\n",
      "Epoch 26/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9797\n",
      "Epoch 00026: val_acc did not improve\n",
      "132/132 [==============================] - 20s 149ms/step - loss: 0.1484 - acc: 0.9799 - val_loss: 1.3239 - val_acc: 0.7636\n",
      "Epoch 27/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9845\n",
      "Epoch 00027: val_acc did not improve\n",
      "132/132 [==============================] - 20s 149ms/step - loss: 0.1127 - acc: 0.9844 - val_loss: 1.5212 - val_acc: 0.7822\n",
      "Epoch 28/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9821\n",
      "Epoch 00028: val_acc did not improve\n",
      "132/132 [==============================] - 20s 151ms/step - loss: 0.1339 - acc: 0.9822 - val_loss: 1.2632 - val_acc: 0.7710\n",
      "Epoch 29/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9835\n",
      "Epoch 00029: val_acc did not improve\n",
      "132/132 [==============================] - 20s 153ms/step - loss: 0.1256 - acc: 0.9832 - val_loss: 1.1322 - val_acc: 0.7772\n",
      "Epoch 30/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9893\n",
      "Epoch 00030: val_acc did not improve\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.0806 - acc: 0.9893 - val_loss: 1.3079 - val_acc: 0.7710\n",
      "Epoch 31/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9871\n",
      "Epoch 00031: val_acc did not improve\n",
      "132/132 [==============================] - 19s 147ms/step - loss: 0.1190 - acc: 0.9872 - val_loss: 0.9443 - val_acc: 0.7797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9847\n",
      "Epoch 00032: val_acc did not improve\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.1423 - acc: 0.9848 - val_loss: 1.2835 - val_acc: 0.7822\n",
      "Epoch 33/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9726\n",
      "Epoch 00033: val_acc did not improve\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.1752 - acc: 0.9721 - val_loss: 0.6543 - val_acc: 0.7611\n",
      "Epoch 34/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9750\n",
      "Epoch 00034: val_acc did not improve\n",
      "132/132 [==============================] - 20s 148ms/step - loss: 0.1651 - acc: 0.9749 - val_loss: 1.3938 - val_acc: 0.7649\n",
      "Epoch 35/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9893\n",
      "Epoch 00035: val_acc did not improve\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.0781 - acc: 0.9893 - val_loss: 1.3531 - val_acc: 0.7686\n",
      "Epoch 36/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9874\n",
      "Epoch 00036: val_acc did not improve\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.0854 - acc: 0.9875 - val_loss: 1.6373 - val_acc: 0.7772\n",
      "Epoch 37/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9816\n",
      "Epoch 00037: val_acc did not improve\n",
      "132/132 [==============================] - 20s 149ms/step - loss: 0.1427 - acc: 0.9815 - val_loss: 0.8894 - val_acc: 0.7636\n",
      "Epoch 38/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9843\n",
      "Epoch 00038: val_acc did not improve\n",
      "132/132 [==============================] - 20s 155ms/step - loss: 0.1080 - acc: 0.9844 - val_loss: 1.2549 - val_acc: 0.7809\n",
      "Epoch 39/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9912\n",
      "Epoch 00039: val_acc did not improve\n",
      "132/132 [==============================] - 20s 151ms/step - loss: 0.0775 - acc: 0.9912 - val_loss: 1.3950 - val_acc: 0.7847\n",
      "Epoch 40/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9924\n",
      "Epoch 00040: val_acc did not improve\n",
      "132/132 [==============================] - 20s 151ms/step - loss: 0.0775 - acc: 0.9924 - val_loss: 1.2458 - val_acc: 0.7760\n",
      "Epoch 41/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9902\n",
      "Epoch 00041: val_acc did not improve\n",
      "132/132 [==============================] - 20s 154ms/step - loss: 0.0843 - acc: 0.9903 - val_loss: 1.4931 - val_acc: 0.7698\n",
      "Epoch 42/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9914\n",
      "Epoch 00042: val_acc did not improve\n",
      "132/132 [==============================] - 20s 152ms/step - loss: 0.0881 - acc: 0.9915 - val_loss: 1.4723 - val_acc: 0.7785\n",
      "Epoch 43/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9893\n",
      "Epoch 00043: val_acc did not improve\n",
      "132/132 [==============================] - 19s 145ms/step - loss: 0.0945 - acc: 0.9893 - val_loss: 1.3278 - val_acc: 0.7735\n",
      "Epoch 44/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9893\n",
      "Epoch 00044: val_acc did not improve\n",
      "132/132 [==============================] - 19s 141ms/step - loss: 0.1022 - acc: 0.9893 - val_loss: 1.2858 - val_acc: 0.7649\n",
      "Epoch 45/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9909\n",
      "Epoch 00045: val_acc did not improve\n",
      "132/132 [==============================] - 18s 135ms/step - loss: 0.0955 - acc: 0.9910 - val_loss: 1.0391 - val_acc: 0.7797\n",
      "Epoch 46/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9926\n",
      "Epoch 00046: val_acc did not improve\n",
      "132/132 [==============================] - 18s 136ms/step - loss: 0.0834 - acc: 0.9927 - val_loss: 1.3273 - val_acc: 0.7710\n",
      "Epoch 47/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9864\n",
      "Epoch 00047: val_acc did not improve\n",
      "132/132 [==============================] - 19s 143ms/step - loss: 0.1252 - acc: 0.9860 - val_loss: 1.0433 - val_acc: 0.7611\n",
      "Epoch 48/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9740\n",
      "Epoch 00048: val_acc did not improve\n",
      "132/132 [==============================] - 19s 141ms/step - loss: 0.1616 - acc: 0.9742 - val_loss: 1.5891 - val_acc: 0.7834\n",
      "Epoch 49/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9781\n",
      "Epoch 00049: val_acc did not improve\n",
      "132/132 [==============================] - 18s 137ms/step - loss: 0.1337 - acc: 0.9782 - val_loss: 1.3206 - val_acc: 0.7649\n",
      "Epoch 50/50\n",
      "131/132 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9874\n",
      "Epoch 00050: val_acc did not improve\n",
      "132/132 [==============================] - 18s 135ms/step - loss: 0.1013 - acc: 0.9875 - val_loss: 1.2959 - val_acc: 0.7822\n"
     ]
    }
   ],
   "source": [
    "dev_batch_size = len(dev_sentences)\n",
    "model.fit_generator(\n",
    "    sequential_generator('/home/gwiedemann/notebooks//OffLang/sample_train.txt', batch_size), \n",
    "    steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
    "    validation_data = sequential_generator('/home/gwiedemann/notebooks/OffLang/sample_dev.txt', dev_batch_size),\n",
    "    validation_steps = math.ceil(len(dev_sentences) / dev_batch_size),\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_batch_size = len(dev_sentences)\n",
    "model_charcnn.fit_generator(\n",
    "    charcnn_sequential_generator('../OffLang/sample_train.txt', batch_size), \n",
    "    steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
    "    validation_data = charcnn_sequential_generator('../OffLang/sample_dev.txt', dev_batch_size),\n",
    "    validation_steps = math.ceil(len(dev_sentences) / dev_batch_size),\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "model_charcnn.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_charcnn.load_weights('best_classification_model.h5')\n",
    "model_charcnn.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "testset_features_e1 = np.zeros((len(dev_sentences), nb_sequence_length, nb_embedding_dims))\n",
    "testset_features_e2 = np.zeros((len(dev_sentences), nb_sequence_length, 52))   \n",
    "for i in range(len(dev_sentences)):\n",
    "    testset_features_e1[i], testset_features_e2[i] = charcnn_process_features(dev_sentences[i], nb_sequence_length, nb_embedding_dims)\n",
    "results = model_charcnn.predict([testset_features_e1, testset_features_e2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2Label = {0 : \"OTHER\", 1 : \"OFFENSIVE\"}\n",
    "predLabels = results.argmax(axis=-1)\n",
    "devLabels = [0 if x[1] == \"OTHER\" else 1 for x in dev_lines]\n",
    "# print(idx2Label)\n",
    "# print(predLabels)\n",
    "# print(devLabels)\n",
    "f1 = f1_score(devLabels, predLabels, average='binary', pos_label=1)\n",
    "r = recall_score(devLabels, predLabels, average='binary', pos_label=1)\n",
    "p = precision_score(devLabels, predLabels, average='binary', pos_label=1)\n",
    "a = accuracy_score(devLabels, predLabels)\n",
    "print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f, Acc: %.3f\" % (p, r, f1, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP / NP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
