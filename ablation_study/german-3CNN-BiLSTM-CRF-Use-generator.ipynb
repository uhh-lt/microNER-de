{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/jindal/notebooks/jindal/NER/')\n",
    "import os\n",
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import newaxis\n",
    "import sklearn\n",
    "import subprocess\n",
    "import fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]\n",
    "\n",
    "\n",
    "def createMatrices(sentences, label2Idx, case2Idx,char2Idx):\n",
    "    #{'numeric': 0, 'allLower': 1, 'contains_digit': 6, 'PADDING_TOKEN': 7, 'other': 4, 'allUpper': 2, 'mainly_numeric': 5, 'initialUpper': 3}\n",
    "\n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []\n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "        labelIndices = []\n",
    "        \n",
    "        for word,char,label in sentence:  \n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                if x in char2Idx.keys():\n",
    "                    charIdx.append(char2Idx[x])\n",
    "                else:\n",
    "                    charIdx.append(char2Idx['UNKNOWN'])\n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(word)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "            labelIndices.append(label2Idx[label])\n",
    "           \n",
    "        dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def padding(Sentences):\n",
    "    maxlen = 52\n",
    "    for sentence in Sentences:\n",
    "        char = sentence[2]\n",
    "        for x in char:\n",
    "            maxlen = max(maxlen,len(x))\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "    return Sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 80\n",
    "trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i,data in enumerate(dataset):    \n",
    "        tokens, casing,char, labels = data\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    return predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all deriv and part to misc. with BIO\n",
    "def modify_labels(dataset):\n",
    "    bad_labels = ['I-PERderiv','I-OTHpart','B-ORGderiv', 'I-OTH','B-OTHpart','B-LOCderiv','I-LOCderiv','I-OTHderiv','B-PERderiv','B-OTHderiv','B-PERpart','I-PERpart','I-LOCpart','B-LOCpart','I-ORGpart','I-ORGderiv','B-ORGpart','B-OTH']\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            label = word[1]\n",
    "            if label in bad_labels:\n",
    "                first_char = label[0]\n",
    "                if first_char == 'B' :\n",
    "                    word[1] = 'B-MISC'\n",
    "                else:\n",
    "                    word[1] = 'I-MISC'\n",
    "    return dataset\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preproecessing data from GermEval\n",
    "def get_sentences(path):\n",
    "    sentences=[]\n",
    "    with open(path,'rb') as f:\n",
    "    #     lines = f.readlines()\n",
    "        sentence=[]\n",
    "        for line in f:\n",
    "            try:\n",
    "                splits = [x.decode() for x in line.split()]\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            try:\n",
    "                if splits[0]!='#':\n",
    "                    temp = [splits[1],splits[2]]\n",
    "                    sentence.append(temp)\n",
    "                else:\n",
    "                    if len(sentence):\n",
    "                        sentences.append(sentence)\n",
    "                    sentence=[]\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preproecessing data from Conll\n",
    "def get_sentences(filename):\n",
    "    '''\n",
    "        -DOCSTART- -X- -X- O\n",
    "\n",
    "    EU NNP B-NP B-ORG\n",
    "    rejects VBZ B-VP O\n",
    "    German JJ B-NP B-MISC\n",
    "    call NN I-NP O\n",
    "    to TO B-VP O\n",
    "    boycott VB I-VP O\n",
    "    British JJ B-NP B-MISC\n",
    "    lamb NN I-NP O\n",
    "    . . O O\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    read file\n",
    "    return format :\n",
    "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
    "    '''\n",
    "    f = open(filename,'rb')\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        splits = line.split()\n",
    "        try:\n",
    "            word=splits[0].decode()\n",
    "            if word=='-DOCSTART-':\n",
    "                continue\n",
    "            label=splits[-1].decode()\n",
    "            temp=[word,label]\n",
    "            sentence.append(temp)\n",
    "        except Exception as e:\n",
    "            if len(sentence)!=0:\n",
    "                sentences.append(sentence)\n",
    "                sentence=[]\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23999\n",
      "2199\n",
      "5099\n"
     ]
    }
   ],
   "source": [
    "trainSentences = get_sentences('/home/jindal/notebooks/Resources/GermEVAL/NER-de-train.tsv')\n",
    "devSentences = get_sentences('/home/jindal/notebooks/Resources/GermEVAL/NER-de-dev.tsv')\n",
    "testSentences = get_sentences('/home/jindal/notebooks/Resources/GermEVAL/NER-de-test.tsv')\n",
    "\n",
    "print(len(trainSentences))\n",
    "print(len(devSentences))\n",
    "print(len(testSentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conll\n",
    "train_sentences = get_sentences('/home/jindal/notebooks/Resources/CONLL/deu/deu_utf.train')\n",
    "dev_sentences = get_sentences('/home/jindal/notebooks/Resources/CONLL/deu/deu_utf.testa')\n",
    "test_sentences = get_sentences('/home/jindal/notebooks/Resources/CONLL/deu/deu_utf.testb')\n",
    "\n",
    "print(len(train_sentences))\n",
    "print(len(dev_sentences))\n",
    "print(len(test_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Der', 'O'], ['Meteoritensucher', 'O'], ['hatte', 'O'], ['im', 'O'], ['Frühjahr', 'O'], ['auf', 'O'], ['der', 'O'], ['dänischen', 'B-LOCderiv'], ['Insel', 'O'], ['Lolland', 'B-LOC'], ['30', 'O'], ['Gramm', 'O'], ['eines', 'O'], ['Meteoriten', 'O'], ['aufgespürt', 'O'], [',', 'O'], ['der', 'O'], ['Mitte', 'O'], ['Januar', 'O'], ['über', 'O'], ['Nordeuropa', 'B-LOC'], ['beobachtet', 'O'], ['worden', 'O'], ['war', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelSet = set()\n",
    "characters= set()\n",
    "\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for word, label in sentence:\n",
    "            for char in word:\n",
    "                characters.add(char)\n",
    "            labelSet.add(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(labelSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Create a mapping for the labels ::\n",
    "label2Idx = {}\n",
    "for label in labelSet:\n",
    "    label2Idx[label] = len(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOCderiv': 0, 'B-LOC': 1, 'B-PERpart': 14, 'I-LOCpart': 4, 'B-OTH': 15, 'I-ORGderiv': 17, 'B-ORGderiv': 5, 'B-LOCpart': 19, 'O': 6, 'I-LOC': 7, 'I-PERderiv': 16, 'B-OTHderiv': 2, 'I-OTHderiv': 8, 'I-ORGpart': 21, 'I-PER': 3, 'B-PER': 22, 'B-OTHpart': 11, 'B-PERderiv': 9, 'I-PERpart': 24, 'B-ORG': 10, 'B-LOCderiv': 23, 'I-OTH': 18, 'I-ORG': 12, 'B-ORGpart': 13, 'I-OTHpart': 20}\n"
     ]
    }
   ],
   "source": [
    "print(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Hard coded case lookup ::\n",
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "{'other': 4, 'allLower': 1, 'numeric': 0, 'mainly_numeric': 5, 'contains_digit': 6, 'PADDING_TOKEN': 7, 'allUpper': 2, 'initialUpper': 3}\n"
     ]
    }
   ],
   "source": [
    "print(caseEmbeddings)\n",
    "print(case2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'B-PER'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'B-ORG'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'B-PER'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['überzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ö': 138, 'к': 1, 'Ş': 2, 'Á': 176, '⊃': 3, 'č': 4, '€': 232, '\\x9a': 5, 'Ł': 6, ')': 7, '▪': 8, '°': 9, '·': 56, 'UNKNOWN': 328, '太': 13, 'Ž': 12, '傳': 188, '2': 180, 'Z': 242, '妃': 252, '§': 11, 'ā': 14, 'ó': 15, '~': 297, '동': 17, 'ź': 82, 'İ': 18, 'I': 19, 'т': 25, '.': 21, '“': 23, 'ế': 0, 'Π': 31, '別': 61, 'Т': 33, '@': 27, 'ǒ': 281, 'л': 30, 'ę': 32, '*': 275, 'p': 291, 'Â': 42, '_': 38, 'н': 36, 'ь': 37, 'ú': 308, 'r': 39, 'R': 40, ',': 41, 's': 44, '柯': 45, '+': 46, 'Ä': 47, 'α': 48, 'è': 49, 'k': 223, 'È': 68, 'œ': 52, 'c': 53, 'г': 54, 'ъ': 57, '$': 58, ';': 59, 'e': 60, 'B': 62, 'A': 63, '\\x95': 64, 'é': 66, 'o': 65, 'и': 67, '≤': 312, '\\x80': 313, 'ʻ': 69, 'λ': 70, 'ņ': 71, 'y': 224, 'û': 73, 'æ': 75, '⋅': 76, 'ö': 77, '守': 79, '貴': 81, 'κ': 55, '—': 83, '李': 84, '©': 86, 'ã': 131, 'Å': 87, 'ρ': 88, '−': 89, '<': 90, '×': 243, 'б': 91, 'η': 239, 'ς': 93, 'р': 94, 'h': 95, 'Q': 96, 'î': 97, '’': 10, 'τ': 99, 'x': 133, 'ж': 100, '\\x96': 101, 'о': 102, 'с': 270, 'ō': 105, 'š': 106, 'ć': 107, '«': 108, '½': 109, 'м': 111, 'ê': 112, 'У': 113, '‘': 116, '6': 115, 'В': 118, 'в': 119, 'С': 121, '&': 122, 'ô': 123, '´': 125, 'í': 124, 'п': 120, 'N': 127, 'j': 128, 'Š': 130, 'ū': 132, '九': 134, 'ḳ': 136, '造': 135, 'Ü': 137, '佐': 141, 'β': 140, 'д': 142, 'G': 143, 'ě': 191, '>': 260, 'ا': 144, 'P': 145, 'ı': 146, 'X': 285, 'х': 147, '¤': 152, 'ν': 151, 'я': 150, 'ي': 155, '»': 154, '公': 158, 'u': 156, 'オ': 157, 'Ø': 326, '台': 159, '東': 161, '„': 80, 'S': 192, 'Æ': 162, 'ħ': 163, '殿': 167, 'ι': 166, 'à': 129, 'ệ': 259, 'a': 193, 'W': 274, '±': 169, '8': 254, '9': 170, 't': 171, 'υ': 201, '†': 175, 'L': 290, '-': 16, '5': 178, 'À': 317, 'õ': 179, 'D': 139, 'ψ': 315, 'J': 182, 'ラ': 183, 'ċ': 185, 'є': 258, 'ž': 186, ':': 277, 'ë': 187, 'Ц': 126, '士': 189, 'ç': 26, 'ă': 74, 'd': 20, '4': 190, 'H': 148, 'у': 307, '術': 51, 'Î': 149, 'й': 194, '=': 195, 'ř': 196, 'T': 22, '\\x99': 198, 'z': 199, '…': 200, 'ḫ': 204, 'g': 202, 'ą': 203, '\\u200e': 29, '樓': 207, 'µ': 208, 'l': 209, 'M': 210, 'ē': 212, '7': 153, '\\x92': 213, 'ب': 263, \"'\": 215, '[': 234, '²': 216, '→': 217, 'ά': 177, 'i': 264, 'ض': 219, '–': 220, '博': 222, 'Þ': 28, 'v': 205, '\"': 225, 'ż': 227, 'ť': 228, 'w': 295, 'E': 229, '대': 206, 'ń': 231, '#': 233, 'ε': 235, 'ñ': 236, 'ῦ': 160, 'أ': 238, '章': 98, 'Ġ': 240, 'n': 241, ']': 245, 'K': 244, '‚': 304, 'Ш': 246, 'É': 247, 'ğ': 78, '?': 249, '‐': 211, '¸': 250, 'φ': 251, '0': 253, 'â': 256, '鶴': 214, '%': 257, 'ß': 327, '/': 34, 'ĩ': 261, 'σ': 262, 'ś': 35, '학': 92, 'ø': 265, '!': 320, '}': 181, '寝': 267, 'ł': 268, 'ň': 269, '懿': 248, '\\x94': 271, 'O': 272, 'Ż': 104, 'π': 85, '”': 276, '(': 72, '›': 305, 'М': 280, 'з': 165, 'Y': 284, '冲': 103, 'å': 287, 'а': 288, 'm': 273, 'Œ': 289, 'ð': 43, 'ό': 266, '南': 286, 'γ': 300, 'Л': 292, '\\xad': 293, 'ī': 218, 'έ': 168, 'ɨ': 278, 'е': 226, 'ş': 294, '¹': 174, 'ο': 296, 'Е': 197, '″': 314, 'f': 322, 'V': 298, '3': 299, 'q': 301, '≘': 114, '`': 302, 'ـ': 110, '1': 303, 'ы': 164, 'Č': 306, 'U': 255, '‹': 309, '算': 279, 'ü': 310, '鷹': 311, 'И': 221, 'ї': 173, '£': 283, 'Λ': 50, 'á': 316, '³': 172, 'ä': 318, 'ŏ': 319, 'ý': 230, 'ن': 282, '별': 321, 'F': 24, 'b': 323, 'C': 324, 'П': 325, 'ю': 117, '루': 237, 'ő': 184}\n"
     ]
    }
   ],
   "source": [
    "char2Idx={}\n",
    "for char in characters:\n",
    "    char2Idx[char] = len(char2Idx)\n",
    "char2Idx['UNKNOWN'] = len(char2Idx)\n",
    "print(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'B-PER'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'B-ORG'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'B-PER'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['überzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home/jindal/notebooks/fastText/wiki.de.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(trainSentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'B-PER'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'B-ORG'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'B-PER'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['überzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(dataset):\n",
    "    l = []\n",
    "    for i in dataset:\n",
    "        l.append(len(i))\n",
    "    l = set(l)\n",
    "    print(len(l))\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in l:\n",
    "        temp = []\n",
    "        for batch in dataset:\n",
    "            if len(batch) == i:\n",
    "                temp.append(batch)\n",
    "                z += 1\n",
    "        batches.append(temp)\n",
    "#         batch_len.append(z)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "train_batches = createBatches(trainSentences)\n",
    "dev_batches = createBatches(devSentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Alles', 'O'], ['richtig', 'O'], ['.', 'O']]\n",
      "[['Farben', 'O'], ['eingeführt', 'O'], ['.', 'O']]\n",
      "51\n",
      "3\n",
      "[['Brennt', 'O'], ['nichts', 'O'], ['nieder', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(batches[0][0])\n",
    "print(batches[0][1])\n",
    "print(len(batches))\n",
    "print(len(batches[0]))\n",
    "print(batches[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batches: 'list of training/dev sentences- batches already created'):\n",
    "    global line_number\n",
    "    \n",
    "    while True:\n",
    "        total_lines = len(dataset)\n",
    "        for batch in batches:\n",
    "            word_embeddings = []\n",
    "            case_embeddings = []\n",
    "            char_embeddings = []\n",
    "\n",
    "            output_labels = []\n",
    "            for index in range(len(batch)): # batches made according to the size of the sentences. len(batch) gives the size of current batch            \n",
    "#                 index = line_number%total_lines\n",
    "#                 line_number+=1\n",
    "                sentence = batch[index]\n",
    "    #             print(sentence)\n",
    "                temp_casing = []\n",
    "                temp_char=[]\n",
    "                temp_word=[]\n",
    "                temp_output=[]\n",
    "                for word in sentence:\n",
    "                    word, label = word\n",
    "                    casing =getCasing(word, case2Idx)\n",
    "                    temp_casing.append(casing)\n",
    "                    temp_char2=[]\n",
    "                    for char in word:\n",
    "                        if char in char2Idx.keys():\n",
    "                            temp_char2.append(char2Idx[char])\n",
    "                        else:\n",
    "                            temp_char2.append(char2Idx['UNKNOWN']) # To incorporate the words which are not in the vocab\n",
    "                    temp_char2 = np.array(temp_char2)\n",
    "                    temp_char.append(temp_char2)\n",
    "                    word_vector = ft.get_word_vector(word.lower())\n",
    "                    temp_word.append(word_vector)\n",
    "                    temp_output.append(label2Idx[label])\n",
    "                temp_char = pad_sequences(temp_char, 52)\n",
    "                word_embeddings.append(temp_word)\n",
    "                case_embeddings.append(temp_casing)\n",
    "                char_embeddings.append(temp_char)\n",
    "                temp_output = to_categorical(temp_output, 25)\n",
    "                output_labels.append(temp_output)\n",
    "    #             output_labels = to_categorical()\n",
    "    #             output_labels = np.array(output_labels)\n",
    "    #             output_labels = output_labels[...,newaxis]\n",
    "\n",
    "    #             print(np.array(word_embeddings).shape)\n",
    "    #             print(np.array(case_embeddings).shape)\n",
    "    #             print(np.array(char_embeddings).shape)\n",
    "    #             print(output_labels.shape)\n",
    "    #             print(\"******************\\n\\n\")\n",
    "            yield ([np.array(word_embeddings), np.array(case_embeddings), np.array(char_embeddings)], np.array(output_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 300)\n",
      "(3, 3)\n",
      "(3, 3, 52)\n",
      "(3, 3, 25)\n",
      "********\n",
      "(32, 4, 300)\n",
      "(32, 4)\n",
      "(32, 4, 52)\n",
      "(32, 4, 25)\n",
      "********\n",
      "(127, 5, 300)\n",
      "(127, 5)\n",
      "(127, 5, 52)\n",
      "(127, 5, 25)\n",
      "********\n",
      "(317, 6, 300)\n",
      "(317, 6)\n",
      "(317, 6, 52)\n",
      "(317, 6, 25)\n",
      "********\n",
      "(497, 7, 300)\n",
      "(497, 7)\n",
      "(497, 7, 52)\n",
      "(497, 7, 25)\n",
      "********\n",
      "(755, 8, 300)\n",
      "(755, 8)\n",
      "(755, 8, 52)\n",
      "(755, 8, 25)\n",
      "********\n",
      "(913, 9, 300)\n",
      "(913, 9)\n",
      "(913, 9, 52)\n",
      "(913, 9, 25)\n",
      "********\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-17e39a422182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mline_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-c6320a6ab1e4>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(batches)\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mtemp_char2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_char2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mtemp_char\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_char2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0mword_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0mtemp_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mtemp_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel2Idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/fastText/FastText.py\u001b[0m in \u001b[0;36mget_word_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetWordVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "line_number=0\n",
    "for inp, out in train_generator(batches):\n",
    "    word, case, char = inp\n",
    "    print(word.shape)\n",
    "    print(case.shape)\n",
    "    print(char.shape)\n",
    "    print(out.shape)\n",
    "    print(\"********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2Label = {v: k for k, v in label2Idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10528       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_67 (TimeDistri (None, None, 52, 32) 3104        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_70 (TimeDistri (None, None, 52, 32) 4128        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_73 (TimeDistri (None, None, 52, 32) 5152        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_68 (TimeDistri (None, None, 1, 32)  0           time_distributed_67[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_71 (TimeDistri (None, None, 1, 32)  0           time_distributed_70[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_74 (TimeDistri (None, None, 1, 32)  0           time_distributed_73[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_69 (TimeDistri (None, None, 32)     0           time_distributed_68[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_72 (TimeDistri (None, None, 32)     0           time_distributed_71[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_75 (TimeDistri (None, None, 32)     0           time_distributed_74[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, None, 404)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_69[0][0]        \n",
      "                                                                 time_distributed_72[0][0]        \n",
      "                                                                 time_distributed_75[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, None, 400)    968000      concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_76 (TimeDistri (None, None, 25)     10025       bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "crf_16 (CRF)                    (None, None, 25)     1325        time_distributed_76[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,002,326\n",
      "Trainable params: 1,002,262\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "words_input = Input(shape=(None,300),dtype='float32',name='words_input')\n",
    "# words = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1],  weights=[wordEmbeddings], trainable=False)(words_input)\n",
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False, name = 'case_embed')(casing_input)\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),32,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "# dropout= Dropout(0.5, name='dropout1')(embed_char_out)\n",
    "\n",
    "# conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, name='conv'))(dropout)\n",
    "# maxpool_out=TimeDistributed(MaxPooling1D(52), name='maxpool')(conv1d_out)\n",
    "# char = TimeDistributed(Flatten())(maxpool_out)\n",
    "# char = Dropout(0.5)(char)\n",
    "# output = concatenate([words, casing, char])\n",
    "\n",
    "kernel_sizes = (3, 4, 5)\n",
    "conv_blocks = []\n",
    "for sz in kernel_sizes:\n",
    "    conv = TimeDistributed(Conv1D(\n",
    "                         kernel_size=sz,\n",
    "                         filters=32,\n",
    "                         padding=\"same\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1))(embed_char_out)\n",
    "    conv = TimeDistributed(MaxPooling1D(52))(conv)\n",
    "    conv = TimeDistributed(Flatten())(conv)\n",
    "#     conv = Dropout(0.5)(conv)\n",
    "    conv_blocks.append(conv)\n",
    "output = concatenate([words_input, casing, conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "\n",
    "output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "output = TimeDistributed(Dense(len(label2Idx)))(output)\n",
    "crf = CRF(len(label2Idx))\n",
    "output = crf(output)\n",
    "model = Model(inputs=[words_input, casing_input,character_input], outputs=[output])\n",
    "model.compile(loss=crf.loss_function, optimizer='nadam', metrics=[crf.accuracy])\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='nadam', )\n",
    "model.summary()\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('generator_NER_best.h5', monitor='f1_metric', verbose=1, save_best_only=True, period=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Epoch 1/1\n",
      "21/51 [===========>..................] - ETA: 12s - loss: -0.5313 - acc: 0.9874"
     ]
    }
   ],
   "source": [
    "# line_number=0\n",
    "model.reset_states()\n",
    "max_f1=0\n",
    "for i in range(80):\n",
    "    print(\"epoch %d\" %i)\n",
    "    model.fit_generator(generator(train_batches), epochs=1, steps_per_epoch= len(batches))\n",
    "    \n",
    "    print(len(devSentences))\n",
    "    \n",
    "    if i>=50:\n",
    "        pred_labels=[]\n",
    "        correct_labels=[]\n",
    "        for sentence in devSentences:\n",
    "            pred_labels_sentence=[]\n",
    "            correct_labels_sentence=[]\n",
    "            temp_sentence=[]\n",
    "            for word in sentence:\n",
    "                w, correct_label = word\n",
    "                correct_labels_sentence.append(label2Idx[correct_label])\n",
    "                temp_sentence.append(w)\n",
    "            pred_label = get_predictions(temp_sentence, model)\n",
    "            pred_label = np.ndarray.tolist(pred_label[0])\n",
    "        #     print(len(pred_label[0]))\n",
    "            for label in pred_label:\n",
    "                label = label.index(max(label))\n",
    "                pred_labels_sentence.append(label)\n",
    "            pred_labels.append(pred_labels_sentence)\n",
    "            correct_labels.append(correct_labels_sentence)\n",
    "        pre_dev, rec_dev, f1_dev = compute_f1(pred_labels, correct_labels, idx2Label)\n",
    "        print(f1_dev)\n",
    "        if max_f1 < f1_dev:\n",
    "            print(\"model saved\")\n",
    "            model.save('generator_NER_best.h5')\n",
    "            max_f1 = f1_dev\n",
    "    #     print((len(correct_labels), len(pred_labels)))\n",
    "#     print(f1_score(correct_labels, pred_labels, average='macro'))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2199\n",
      "(2199, 2199)\n"
     ]
    }
   ],
   "source": [
    "pred_labels=[]\n",
    "correct_labels=[]\n",
    "print(len(devSentences))\n",
    "for sentence in devSentences:\n",
    "    pred_labels_sentence=[]\n",
    "    correct_labels_sentence=[]\n",
    "    temp_sentence=[]\n",
    "    for word in sentence:\n",
    "        w, correct_label = word\n",
    "        correct_labels_sentence.append(label2Idx[correct_label])\n",
    "        temp_sentence.append(w)\n",
    "    pred_label = get_predictions(temp_sentence, model)\n",
    "    pred_label = np.ndarray.tolist(pred_label[0])\n",
    "#     print(len(pred_label[0]))\n",
    "    for label in pred_label:\n",
    "        label = label.index(max(label))\n",
    "        pred_labels_sentence.append(label)\n",
    "    pred_labels.append(pred_labels_sentence)\n",
    "    correct_labels.append(correct_labels_sentence)\n",
    "print((len(pred_labels), len(correct_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dev, rec_dev, f1_dev = compute_f1(pred_labels, correct_labels, idx2Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8189239332096476\n"
     ]
    }
   ],
   "source": [
    "print(f1_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('generator_NER_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jindal/miniconda3/envs/NER2/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for sentence in devSentences:\n",
    "    temp_sentence=[]\n",
    "    for word in sentence:\n",
    "        w, correct_label = word\n",
    "        correct_labels.append(label2Idx[correct_label])\n",
    "        temp_sentence.append(w)\n",
    "    pred_label = get_predictions(temp_sentence, model)\n",
    "    pred_label = np.ndarray.tolist(pred_label[0])\n",
    "#     print(len(pred_label[0]))\n",
    "    for label in pred_label:\n",
    "        label = label.index(max(label))\n",
    "        pred_labels.append(label)\n",
    "temp_f1 = f1_score(correct_labels, pred_labels, average='macro')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5227800135258113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jindal/miniconda3/envs/NER2/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "temp_f1 = f1_score(correct_labels, pred_labels, average='macro')\n",
    "print(temp_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_to_input(sentence):\n",
    "    word_embeddings = []\n",
    "    case_embeddings = []\n",
    "    char_embeddings = []\n",
    "\n",
    "#     output_labels = []\n",
    "    temp_casing = []\n",
    "    temp_char=[]\n",
    "    temp_word=[]\n",
    "#     temp_output=[]\n",
    "    for word in sentence:\n",
    "        casing =getCasing(word, case2Idx)\n",
    "        temp_casing.append(casing)\n",
    "        temp_char2=[]\n",
    "        for char in word:\n",
    "            if char in char2Idx.keys():\n",
    "                temp_char2.append(char2Idx[char])\n",
    "            else:\n",
    "                temp_char2.append(char2Idx['UNKNOWN']) # To incorporate the words which are not in the vocab\n",
    "        temp_char2 = np.array(temp_char2)\n",
    "        temp_char.append(temp_char2)\n",
    "        word_vector = ft.get_word_vector(word.lower())\n",
    "        temp_word.append(word_vector)\n",
    "#         temp_output.append(label2Idx[label])\n",
    "    temp_char = pad_sequences(temp_char, 52)\n",
    "    word_embeddings.append(temp_word)\n",
    "    case_embeddings.append(temp_casing)\n",
    "    char_embeddings.append(temp_char)\n",
    "#     temp_output = to_categorical(temp_output, 25)\n",
    "#     output_labels.append(temp_output)\n",
    "    ans = [np.array(word_embeddings), np.array(case_embeddings), np.array(char_embeddings)]\n",
    "    return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(sentence, model):\n",
    "    inp = convert_sentence_to_input(sentence)\n",
    "    \n",
    "    out = model.predict(inp)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in devSentences:\n",
    "        for word in sentence:\n",
    "            w, label = word\n",
    "            temp_sentence = []\n",
    "            temp_sentence.append(w)\n",
    "        x = convert_sentence_to_input(temp_sentence)\n",
    "#         print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
