{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jindal/miniconda3/envs/NER2/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/home/jindal/notebooks/jindal/NER/')\n",
    "import os\n",
    "import numpy as np \n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import sklearn\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 80\n",
    "trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i,data in enumerate(dataset):    \n",
    "        tokens, casing,char, labels = data\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    return predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all deriv and part to misc. with BIO\n",
    "def modify_labels(dataset):\n",
    "    bad_labels = ['I-PERderiv','I-OTHpart','B-ORGderiv', 'I-OTH','B-OTHpart','B-LOCderiv','I-LOCderiv','I-OTHderiv','B-PERderiv','B-OTHderiv','B-PERpart','I-PERpart','I-LOCpart','B-LOCpart','I-ORGpart','I-ORGderiv','B-ORGpart','B-OTH']\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            label = word[1]\n",
    "            if label in bad_labels:\n",
    "                first_char = label[0]\n",
    "                if first_char == 'B' :\n",
    "                    word[1] = 'B-MISC'\n",
    "                else:\n",
    "                    word[1] = 'I-MISC'\n",
    "    return dataset\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preproecessing data from GermEval\n",
    "def get_sentences(path):\n",
    "    sentences=[]\n",
    "    with open(path,'rb') as f:\n",
    "    #     lines = f.readlines()\n",
    "        sentence=[]\n",
    "        for line in f:\n",
    "            try:\n",
    "                splits = [x.decode() for x in line.split()]\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            try:\n",
    "                if splits[0]!='#':\n",
    "                    temp = [splits[1],splits[2]]\n",
    "                    sentence.append(temp)\n",
    "                else:\n",
    "                    if len(sentence):\n",
    "                        sentences.append(sentence)\n",
    "                    sentence=[]\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preproecessing data from Conll\n",
    "def get_sentences(filename):\n",
    "    '''\n",
    "        -DOCSTART- -X- -X- O\n",
    "\n",
    "    EU NNP B-NP B-ORG\n",
    "    rejects VBZ B-VP O\n",
    "    German JJ B-NP B-MISC\n",
    "    call NN I-NP O\n",
    "    to TO B-VP O\n",
    "    boycott VB I-VP O\n",
    "    British JJ B-NP B-MISC\n",
    "    lamb NN I-NP O\n",
    "    . . O O\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    read file\n",
    "    return format :\n",
    "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
    "    '''\n",
    "    f = open(filename,'rb')\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        splits = line.split()\n",
    "        try:\n",
    "            word=splits[0].decode()\n",
    "            if word=='-DOCSTART-':\n",
    "                continue\n",
    "            label=splits[-1].decode()\n",
    "            temp=[word,label]\n",
    "            sentence.append(temp)\n",
    "        except Exception as e:\n",
    "            if len(sentence)!=0:\n",
    "                sentences.append(sentence)\n",
    "                sentence=[]\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23999\n",
      "2199\n",
      "5099\n"
     ]
    }
   ],
   "source": [
    "train_sentences = get_sentences('/home/jindal/notebooks/Resources/GermEVAL/NER-de-train.tsv')\n",
    "dev_sentences = get_sentences('/home/jindal/notebooks/Resources/GermEVAL/NER-de-dev.tsv')\n",
    "test_sentences = get_sentences('/home/jindal/notebooks/Resources/GermEVAL/NER-de-test.tsv')\n",
    "\n",
    "print(len(train_sentences))\n",
    "print(len(dev_sentences))\n",
    "print(len(test_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12152\n",
      "2867\n",
      "3005\n"
     ]
    }
   ],
   "source": [
    "# conll\n",
    "train_sentences = get_sentences('/home/jindal/notebooks/Resources/CONLL/deu/deu_utf.train')\n",
    "dev_sentences = get_sentences('/home/jindal/notebooks/Resources/CONLL/deu/deu_utf.testa')\n",
    "test_sentences = get_sentences('/home/jindal/notebooks/Resources/CONLL/deu/deu_utf.testb')\n",
    "\n",
    "print(len(train_sentences))\n",
    "print(len(dev_sentences))\n",
    "print(len(test_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Der', 'O'], ['Meteoritensucher', 'O'], ['hatte', 'O'], ['im', 'O'], ['Frühjahr', 'O'], ['auf', 'O'], ['der', 'O'], ['dänischen', 'B-LOCderiv'], ['Insel', 'O'], ['Lolland', 'B-LOC'], ['30', 'O'], ['Gramm', 'O'], ['eines', 'O'], ['Meteoriten', 'O'], ['aufgespürt', 'O'], [',', 'O'], ['der', 'O'], ['Mitte', 'O'], ['Januar', 'O'], ['über', 'O'], ['Nordeuropa', 'B-LOC'], ['beobachtet', 'O'], ['worden', 'O'], ['war', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sentences = modify_labels(train_sentences)\n",
    "# dev_sentences = modify_labels(dev_sentences)\n",
    "# test_sentences = modify_labels(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = addCharInformatioin(train_sentences)\n",
    "devSentences = addCharInformatioin(dev_sentences)\n",
    "testSentences = addCharInformatioin(test_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelSet = set()\n",
    "words = {}\n",
    "characters= set()\n",
    "\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token, char, label in sentence:\n",
    "            for c in char:\n",
    "                characters.add(c)\n",
    "            labelSet.add(label)\n",
    "            words[token.lower()] = True\n",
    "            # words[token] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-OTHpart', 'O', 'B-ORGpart', 'B-PERpart', 'B-LOCderiv', 'I-ORGderiv', 'I-OTH', 'B-OTHderiv', 'B-PERderiv', 'I-OTHderiv', 'I-ORGpart', 'B-OTH', 'B-ORGderiv', 'I-LOC', 'I-PERpart', 'I-LOCpart', 'I-PERderiv', 'B-PER', 'B-LOCpart', 'I-LOCderiv', 'B-ORG', 'B-LOC', 'I-ORG', 'I-PER', 'I-OTHpart'}\n"
     ]
    }
   ],
   "source": [
    "print(labelSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Create a mapping for the labels ::\n",
    "label2Idx = {}\n",
    "for label in labelSet:\n",
    "    label2Idx[label] = len(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-OTHpart': 0, 'B-ORGpart': 2, 'I-OTHderiv': 9, 'B-LOCderiv': 4, 'I-ORGderiv': 5, 'I-ORGpart': 10, 'B-OTH': 11, 'I-OTH': 6, 'B-ORGderiv': 12, 'O': 1, 'I-LOC': 13, 'I-PERpart': 14, 'I-PERderiv': 16, 'B-PER': 17, 'B-LOCpart': 18, 'B-OTHderiv': 7, 'I-LOCderiv': 19, 'B-PERpart': 3, 'B-ORG': 20, 'B-LOC': 21, 'I-ORG': 22, 'B-PERderiv': 8, 'I-PER': 23, 'I-LOCpart': 15, 'I-OTHpart': 24}\n"
     ]
    }
   ],
   "source": [
    "print(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Hard coded case lookup ::\n",
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_words = ' '.join(words.keys())\n",
    "# print(string_words)\n",
    "f = open('/home/jindal/notebooks/fastText/german_words.txt','wb')\n",
    "f.write(string_words.encode())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "{'numeric': 0, 'PADDING_TOKEN': 7, 'contains_digit': 6, 'mainly_numeric': 5, 'initialUpper': 3, 'allUpper': 2, 'other': 4, 'allLower': 1}\n"
     ]
    }
   ],
   "source": [
    "print(caseEmbeddings)\n",
    "print(case2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Idx={}\n",
    "wordEmbeddings=[]\n",
    "\n",
    "# created a file by the name of german_words.txt in /fastText. Containing all the words in our dataset\n",
    "with open('/home/jindal/notebooks/jindal/NER/ablation_study/german_word_embeddings.txt','rb') as f:\n",
    "    for line in f:\n",
    "        splits = line.split()\n",
    "        word = splits[0].decode()\n",
    "#         print(word.decode())\n",
    "        if len(word2Idx) == 0: #Add padding+unknown\n",
    "            word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "            vector = np.zeros(len(splits)-1) #Zero vector vor 'PADDING' word\n",
    "            wordEmbeddings.append(vector)\n",
    "\n",
    "            word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "            vector = np.random.uniform(-0.25, 0.25, len(splits)-1)\n",
    "            wordEmbeddings.append(vector)\n",
    "            \n",
    "        word2Idx[word.lower()]=len(word2Idx)\n",
    "        embedding = np.array([float(num) for num in splits[1:]])\n",
    "        wordEmbeddings.append(embedding)\n",
    "wordEmbeddings=np.array(wordEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Idx={}\n",
    "wordEmbeddings=[]\n",
    "\n",
    "fEmbeddings = open(\"/home/jindal/notebooks/embeddings/embeddings/model_word2vecf.embeddings\", encoding=\"utf-8\")\n",
    "\n",
    "for line in fEmbeddings:\n",
    "    split = line.strip().split(\" \")\n",
    "    word = split[0].lower()\n",
    "#     print(word)\n",
    "    \n",
    "    if len(word2Idx) == 0: #Add padding+unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector vor 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if split[0].lower() in words:\n",
    "        vector = np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word2Idx[word] = len(word2Idx)\n",
    "wordEmbeddings=np.array(wordEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86688\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# print(wordEmbeddings[2])\n",
    "print(len(wordEmbeddings))\n",
    "print(len(wordEmbeddings[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'대': 0, 'Î': 1, 'Ö': 110, '守': 3, '½': 285, 'ö': 4, 'м': 6, '⊃': 7, 'ň': 192, '佐': 8, 'ŏ': 321, '¹': 9, 'æ': 19, 'ą': 12, 'α': 15, '0': 14, '=': 16, 'オ': 17, 'Ġ': 18, '妃': 290, 'Λ': 20, 'A': 21, 'ş': 22, '博': 270, '€': 28, '학': 30, 'q': 26, '-': 27, 'ʻ': 29, 'И': 176, '!': 24, 'В': 31, 'ô': 32, '\\x95': 228, '4': 33, '£': 34, 'ę': 35, '\"': 36, '術': 230, 'ś': 174, 'Ш': 39, '’': 40, 'û': 49, 'b': 43, \"'\": 45, '算': 52, 'j': 46, 'ї': 322, 'Z': 48, 'Č': 50, 'ب': 54, 'ő': 233, 'Â': 57, 'я': 274, 'σ': 55, '−': 56, 'M': 58, 'x': 60, '太': 231, 'Á': 65, '→': 66, 'з': 67, 'Æ': 68, '.': 69, 'è': 276, 'É': 71, 'ь': 73, '貴': 119, 'υ': 296, 'Ł': 239, 'k': 74, 'ā': 238, '\\x80': 76, 'ъ': 226, 'ñ': 77, '(': 78, '‚': 79, 'Y': 80, 'Π': 314, '章': 81, '별': 83, '台': 240, 'O': 84, 'o': 86, 'в': 88, 'õ': 89, '&': 10, 'J': 90, 'Ä': 122, 'ħ': 92, '±': 94, 'κ': 97, '樓': 98, 'ě': 262, 'F': 324, 'š': 99, 'п': 75, 'ë': 101, '李': 103, 'ε': 282, 'u': 104, '\\xad': 105, 'ラ': 106, 'б': 38, 'Š': 109, 'ж': 272, '懿': 277, 'ī': 214, 'C': 111, 'Ø': 112, ',': 114, 'V': 305, 'τ': 313, 'ß': 115, 'ι': 116, ';': 117, 'к': 118, '†': 120, 'н': 121, 'ο': 123, '⋅': 125, 'ά': 124, 'ý': 129, 'č': 128, '»': 130, 'R': 246, 'Œ': 138, 'ç': 135, 'ć': 132, 'с': 131, 'ż': 144, 'ǒ': 143, '#': 137, 'ρ': 136, 'έ': 142, 't': 141, '7': 133, 'ř': 145, 's': 5, 'ê': 147, '”': 149, '9': 11, '}': 148, 'Е': 248, '東': 153, '+': 152, '5': 151, 'л': 202, 'Þ': 155, '士': 157, 'С': 156, 'ы': 292, 'أ': 158, 'Л': 162, '南': 160, 'ю': 190, '©': 163, '\\x99': 166, 'N': 139, 'х': 165, 'n': 167, 'U': 140, '³': 168, 'ḫ': 169, 'ē': 170, '—': 171, 'î': 172, 'Ü': 173, 'й': 177, 'ı': 295, 'f': 178, '1': 267, '~': 179, '·': 297, '冲': 175, 'ς': 85, 'E': 180, 'K': 181, 'r': 182, ']': 183, 'λ': 184, '‐': 185, '@': 186, '<': 187, 'å': 188, 'D': 189, '≘': 197, 'á': 191, '×': 62, 'à': 193, '´': 194, 'Ş': 195, 'd': 299, 'é': 196, 'Q': 87, '%': 23, 'ó': 311, '\\u200e': 198, 'ن': 82, '3': 201, 'ń': 203, 'a': 252, 'œ': 206, 'ا': 207, '‘': 208, 'γ': 199, 'y': 91, 'g': 209, '傳': 108, 'Å': 212, 'W': 304, '$': 213, 'ό': 215, 'â': 200, '¤': 217, '九': 218, 'д': 219, '„': 220, 'г': 37, 'G': 223, '鷹': 224, '≤': 225, 'Ž': 95, '>': 227, 'ł': 127, ':': 229, 'ø': 232, '公': 205, 'ν': 234, '‹': 235, '6': 236, 'T': 237, 'Т': 241, 'е': 96, 'ĩ': 243, 'ņ': 257, 'η': 244, 'т': 134, 'ğ': 245, '_': 242, '造': 247, '\\x92': 61, '別': 72, 'e': 113, 'H': 250, '″': 312, 'v': 150, '`': 64, 'ض': 204, 'i': 59, 'о': 253, 'У': 93, 'ō': 251, 'ψ': 70, 'È': 255, '…': 256, 'µ': 25, 'ế': 258, '?': 317, 'B': 41, 'I': 154, 'L': 325, 'П': 42, 'ă': 261, '›': 100, 'ã': 263, 'и': 264, '寝': 265, '[': 266, 'р': 2, 'β': 102, 'ž': 47, 'p': 268, 'ä': 269, 'ü': 44, '\\x96': 271, 'ệ': 259, '¸': 51, 'z': 273, 'Ż': 275, '“': 260, '²': 309, '°': 278, '«': 279, 'є': 280, 'ɨ': 210, 'w': 281, 'İ': 254, 'ي': 107, 'ú': 283, 'P': 284, 'ź': 286, '▪': 287, 'í': 288, 'ð': 289, '루': 291, '\\x9a': 53, '*': 293, 'а': 327, '鶴': 161, 'М': 294, 'π': 249, 'ḳ': 301, '殿': 298, 'ť': 146, '/': 300, '柯': 216, '2': 303, 'ċ': 63, '8': 306, 'X': 307, 'φ': 302, 'Ц': 13, '\\x94': 315, '–': 310, 'ū': 318, 'c': 308, 'h': 316, 'ῦ': 319, 'у': 320, 'm': 164, 'ـ': 323, 'À': 126, '§': 221, ')': 159, 'S': 211, 'l': 326, '동': 222}\n"
     ]
    }
   ],
   "source": [
    "char2Idx={}\n",
    "for char in characters:\n",
    "    char2Idx[char] = len(char2Idx)\n",
    "print(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', ['S', 'c', 'h', 'a', 'r', 't', 'a', 'u'], 'B-PER'], ['sagte', ['s', 'a', 'g', 't', 'e'], 'O'], ['dem', ['d', 'e', 'm'], 'O'], ['\"', ['\"'], 'O'], ['Tagesspiegel', ['T', 'a', 'g', 'e', 's', 's', 'p', 'i', 'e', 'g', 'e', 'l'], 'B-ORG'], ['\"', ['\"'], 'O'], ['vom', ['v', 'o', 'm'], 'O'], ['Freitag', ['F', 'r', 'e', 'i', 't', 'a', 'g'], 'O'], [',', [','], 'O'], ['Fischer', ['F', 'i', 's', 'c', 'h', 'e', 'r'], 'B-PER'], ['sei', ['s', 'e', 'i'], 'O'], ['\"', ['\"'], 'O'], ['in', ['i', 'n'], 'O'], ['einer', ['e', 'i', 'n', 'e', 'r'], 'O'], ['Weise', ['W', 'e', 'i', 's', 'e'], 'O'], ['aufgetreten', ['a', 'u', 'f', 'g', 'e', 't', 'r', 'e', 't', 'e', 'n'], 'O'], [',', [','], 'O'], ['die', ['d', 'i', 'e'], 'O'], ['alles', ['a', 'l', 'l', 'e', 's'], 'O'], ['andere', ['a', 'n', 'd', 'e', 'r', 'e'], 'O'], ['als', ['a', 'l', 's'], 'O'], ['überzeugend', ['ü', 'b', 'e', 'r', 'z', 'e', 'u', 'g', 'e', 'n', 'd'], 'O'], ['war', ['w', 'a', 'r'], 'O'], ['\"', ['\"'], 'O'], ['.', ['.'], 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createMatrices: for every sentence, changes its words, cases,characters, labels to its corresponding id in their embeddings\n",
    "# padding is used to pad the character indices to a fixed size=52\n",
    "train_set = padding(createMatrices(trainSentences, word2Idx,  label2Idx, case2Idx, char2Idx))\n",
    "dev_set = padding(createMatrices(devSentences, word2Idx, label2Idx, case2Idx, char2Idx))\n",
    "test_set = padding(createMatrices(testSentences, word2Idx, label2Idx, case2Idx, char2Idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44340, 42288, 77401, 2402, 61580, 2402, 15841, 27569, 46353, 22589, 1811, 2402, 4259, 22653, 21388, 37191, 46353, 64772, 72220, 62513, 11802, 75317, 41711, 2402, 70303], [3, 1, 1, 4, 3, 4, 1, 3, 4, 3, 1, 4, 1, 1, 3, 1, 4, 1, 1, 1, 1, 1, 1, 4, 4], array([[211, 308, 316, ...,   0,   0,   0],\n",
      "       [  5, 252, 209, ...,   0,   0,   0],\n",
      "       [299, 113, 164, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [281, 252, 182, ...,   0,   0,   0],\n",
      "       [ 36,   0,   0, ...,   0,   0,   0],\n",
      "       [ 69,   0,   0, ...,   0,   0,   0]], dtype=int32), [17, 1, 1, 1, 20, 1, 1, 1, 1, 17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "25\n",
      "[[211 308 316 ...   0   0   0]\n",
      " [  5 252 209 ...   0   0   0]\n",
      " [299 113 164 ...   0   0   0]\n",
      " ...\n",
      " [281 252 182 ...   0   0   0]\n",
      " [ 36   0   0 ...   0   0   0]\n",
      " [ 69   0   0 ...   0   0   0]]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# train-set[][0]: corresponds to the ids of the words in the sentence\n",
    "# train_set[][1]: corresponds to the ids of the cases of the words\n",
    "# train_set[][2]: contains numpy arrays, one corresponding to every word, each containing the indices of the characters of that word\n",
    "# the numpy arrays have a fixed size (padding or truncation) to 52\n",
    "# train_set[][3]: corresponds to the ids of the labels of every word\n",
    "\n",
    "print(train_set[0])\n",
    "print(len(train_set[0][0])) # gives the number of words in the sentence\n",
    "print((train_set[0][2]))\n",
    "print(len(train_set[2][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "\n",
    "train_batch,train_batch_len = createBatches(train_set)\n",
    "dev_batch,dev_batch_len = createBatches(dev_set)\n",
    "test_batch,test_batch_len = createBatches(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10496       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 52, 32) 3104        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, None, 52, 32) 4128        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, None, 52, 32) 5152        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 1, 32)  0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, None, 1, 32)  0           time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, None, 1, 32)  0           time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    26006400    words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, None, 32)     0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, None, 32)     0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, None, 32)     0           time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 404)    0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "                                                                 time_distributed_6[0][0]         \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 400)    968000      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, None, 25)     10025       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 25)     1325        time_distributed_10[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 27,008,694\n",
      "Trainable params: 1,002,230\n",
      "Non-trainable params: 26,006,464\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "words_input = Input(shape=(None,),dtype='int32',name='words_input')\n",
    "words = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1],  weights=[wordEmbeddings], trainable=False)(words_input)\n",
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),32,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "# dropout= Dropout(0.5, name='dropout1')(embed_char_out)\n",
    "\n",
    "# conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1, name='conv'))(dropout)\n",
    "# maxpool_out=TimeDistributed(MaxPooling1D(52), name='maxpool')(conv1d_out)\n",
    "# char = TimeDistributed(Flatten())(maxpool_out)\n",
    "# char = Dropout(0.5)(char)\n",
    "# output = concatenate([words, casing, char])\n",
    "\n",
    "kernel_sizes = (3, 4, 5)\n",
    "conv_blocks = []\n",
    "for sz in kernel_sizes:\n",
    "    conv = TimeDistributed(Conv1D(\n",
    "                         kernel_size=sz,\n",
    "                         filters=32,\n",
    "                         padding=\"same\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1))(embed_char_out)\n",
    "    conv = TimeDistributed(MaxPooling1D(52))(conv)\n",
    "    conv = TimeDistributed(Flatten())(conv)\n",
    "#     conv = Dropout(0.5)(conv)\n",
    "    conv_blocks.append(conv)\n",
    "output = concatenate([words, casing, conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "\n",
    "output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "output = TimeDistributed(Dense(len(label2Idx)))(output)\n",
    "crf = CRF(len(label2Idx))\n",
    "output = crf(output)\n",
    "model = Model(inputs=[words_input, casing_input,character_input], outputs=[output])\n",
    "# model.compile(loss=crf.loss_function, optimizer=Adam(lr = 0.002), metrics=[crf.accuracy])\n",
    "model.compile(loss=crf.loss_function, optimizer='nadam', metrics=[crf.accuracy])\n",
    "model.summary()\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/80\n",
      "51/51 [==============================] - 12s 240ms/step\n",
      "55/51 [================================] - 12s 226ms/step\n",
      "Epoch 1/80\n",
      "51/51 [==============================] - 13s 249ms/step\n",
      "55/51 [================================] - 13s 236ms/step\n",
      "Epoch 2/80\n",
      "51/51 [==============================] - 12s 234ms/step\n",
      "55/51 [================================] - 12s 221ms/step\n",
      "Epoch 3/80\n",
      "51/51 [==============================] - 13s 248ms/step\n",
      "55/51 [================================] - 13s 234ms/step\n",
      "Epoch 4/80\n",
      "51/51 [==============================] - 12s 243ms/step\n",
      "55/51 [================================] - 13s 229ms/step\n",
      "Epoch 5/80\n",
      "51/51 [==============================] - 12s 242ms/step\n",
      "55/51 [================================] - 13s 229ms/step\n",
      "Epoch 6/80\n",
      "51/51 [==============================] - 12s 237ms/step\n",
      "55/51 [================================] - 12s 224ms/step\n",
      "Epoch 7/80\n",
      "51/51 [==============================] - 12s 245ms/step\n",
      "55/51 [================================] - 13s 231ms/step\n",
      "Epoch 8/80\n",
      "51/51 [==============================] - 13s 252ms/step\n",
      "55/51 [================================] - 13s 239ms/step\n",
      "Epoch 9/80\n",
      "51/51 [==============================] - 12s 239ms/step\n",
      "55/51 [================================] - 12s 226ms/step\n",
      "Epoch 10/80\n",
      "51/51 [==============================] - 13s 245ms/step\n",
      "55/51 [================================] - 13s 232ms/step\n",
      "Epoch 11/80\n",
      "51/51 [==============================] - 13s 252ms/step\n",
      "55/51 [================================] - 13s 239ms/step\n",
      "Epoch 12/80\n",
      "51/51 [==============================] - 13s 251ms/step\n",
      "55/51 [================================] - 13s 238ms/step\n",
      "Epoch 13/80\n",
      "51/51 [==============================] - 13s 248ms/step\n",
      "55/51 [================================] - 13s 236ms/step\n",
      "Epoch 14/80\n",
      "51/51 [==============================] - 13s 252ms/step\n",
      "55/51 [================================] - 13s 240ms/step\n",
      "Epoch 15/80\n",
      "51/51 [==============================] - 13s 259ms/step\n",
      "55/51 [================================] - 14s 245ms/step\n",
      "Epoch 16/80\n",
      "51/51 [==============================] - 13s 248ms/step\n",
      "55/51 [================================] - 13s 235ms/step\n",
      "Epoch 17/80\n",
      "51/51 [==============================] - 13s 259ms/step\n",
      "55/51 [================================] - 13s 245ms/step\n",
      "Epoch 18/80\n",
      "51/51 [==============================] - 13s 250ms/step\n",
      "55/51 [================================] - 13s 236ms/step\n",
      "Epoch 19/80\n",
      "51/51 [==============================] - 13s 247ms/step\n",
      "55/51 [================================] - 13s 233ms/step\n",
      "Epoch 20/80\n",
      "51/51 [==============================] - 13s 249ms/step\n",
      "55/51 [================================] - 13s 237ms/step\n",
      "Epoch 21/80\n",
      "51/51 [==============================] - 13s 253ms/step\n",
      "55/51 [================================] - 13s 239ms/step\n",
      "Epoch 22/80\n",
      "51/51 [==============================] - 13s 252ms/step\n",
      "55/51 [================================] - 13s 239ms/step\n",
      "Epoch 23/80\n",
      "51/51 [==============================] - 12s 245ms/step\n",
      "55/51 [================================] - 13s 231ms/step\n",
      "Epoch 24/80\n",
      "51/51 [==============================] - 13s 259ms/step\n",
      "55/51 [================================] - 14s 246ms/step\n",
      "Epoch 25/80\n",
      "51/51 [==============================] - 13s 255ms/step\n",
      "55/51 [================================] - 13s 241ms/step\n",
      "Epoch 26/80\n",
      "51/51 [==============================] - 13s 254ms/step\n",
      "55/51 [================================] - 13s 240ms/step\n",
      "Epoch 27/80\n",
      "51/51 [==============================] - 13s 254ms/step\n",
      "55/51 [================================] - 13s 241ms/step\n",
      "Epoch 28/80\n",
      "51/51 [==============================] - 13s 259ms/step\n",
      "55/51 [================================] - 14s 246ms/step\n",
      "Epoch 29/80\n",
      "51/51 [==============================] - 13s 248ms/step\n",
      "55/51 [================================] - 13s 235ms/step\n",
      "Epoch 30/80\n",
      "51/51 [==============================] - 13s 261ms/step\n",
      "55/51 [================================] - 14s 247ms/step\n",
      "Epoch 31/80\n",
      "51/51 [==============================] - 13s 259ms/step\n",
      "55/51 [================================] - 13s 245ms/step\n",
      "Epoch 32/80\n",
      "51/51 [==============================] - 13s 250ms/step\n",
      "55/51 [================================] - 13s 236ms/step\n",
      "Epoch 33/80\n",
      "51/51 [==============================] - 12s 242ms/step\n",
      "55/51 [================================] - 13s 229ms/step\n",
      "Epoch 34/80\n",
      "51/51 [==============================] - 13s 252ms/step\n",
      "55/51 [================================] - 13s 239ms/step\n",
      "Epoch 35/80\n",
      "51/51 [==============================] - 13s 253ms/step\n",
      "55/51 [================================] - 13s 239ms/step\n",
      "Epoch 36/80\n",
      "51/51 [==============================] - 12s 241ms/step\n",
      "55/51 [================================] - 13s 228ms/step\n",
      "Epoch 37/80\n",
      "51/51 [==============================] - 12s 245ms/step\n",
      "55/51 [================================] - 13s 231ms/step\n",
      "Epoch 38/80\n",
      "51/51 [==============================] - 13s 251ms/step\n",
      "55/51 [================================] - 13s 238ms/step\n",
      "Epoch 39/80\n",
      "51/51 [==============================] - 13s 259ms/step\n",
      "55/51 [================================] - 13s 245ms/step\n",
      "Epoch 40/80\n",
      "51/51 [==============================] - 13s 245ms/step\n",
      "55/51 [================================] - 13s 232ms/step\n",
      "Epoch 41/80\n",
      "51/51 [==============================] - 13s 254ms/step\n",
      "55/51 [================================] - 13s 240ms/step\n",
      "Epoch 42/80\n",
      "51/51 [==============================] - 12s 244ms/step\n",
      "55/51 [================================] - 13s 231ms/step\n",
      "Epoch 43/80\n",
      "51/51 [==============================] - 12s 238ms/step\n",
      "55/51 [================================] - 12s 226ms/step\n",
      "Epoch 44/80\n",
      "51/51 [==============================] - 13s 246ms/step\n",
      "55/51 [================================] - 13s 232ms/step\n",
      "Epoch 45/80\n",
      "51/51 [==============================] - 12s 242ms/step\n",
      "55/51 [================================] - 13s 228ms/step\n",
      "Epoch 46/80\n",
      "51/51 [==============================] - 13s 250ms/step\n",
      "55/51 [================================] - 13s 237ms/step\n",
      "Epoch 47/80\n",
      "51/51 [==============================] - 12s 237ms/step\n",
      "55/51 [================================] - 12s 224ms/step\n",
      "Epoch 48/80\n",
      "51/51 [==============================] - 13s 247ms/step\n",
      "55/51 [================================] - 13s 233ms/step\n",
      "Epoch 49/80\n",
      "51/51 [==============================] - 12s 244ms/step\n",
      "55/51 [================================] - 13s 232ms/step\n",
      "Epoch 50/80\n",
      "51/51 [==============================] - 12s 241ms/step\n",
      "55/51 [================================] - 13s 227ms/step\n",
      "2198/2199 [============================>.] - ETA: 0s2199 2199\n",
      "Dev-Data: Prec: 0.815, Rec: 0.825, F1: 0.820\n",
      "0.8197026022304832\n",
      "Epoch 51/80\n",
      "51/51 [==============================] - 13s 255ms/step\n",
      "55/51 [================================] - 13s 242ms/step\n",
      "2198/2199 [============================>.] - ETA: 0s2199 2199\n",
      "Dev-Data: Prec: 0.807, Rec: 0.832, F1: 0.819\n",
      "0.8193881312200516\n",
      "Epoch 52/80\n",
      "51/51 [==============================] - 13s 256ms/step\n",
      "55/51 [================================] - 13s 243ms/step\n",
      "1570/2199 [====================>.........] - ETA: 13s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5602cd0fe375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mpredLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrectLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mpre_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrectLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2Label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#             predLabels = np.concatenate(predLabels).ravel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e3fc95c66aa9>\u001b[0m in \u001b[0;36mtag_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcasing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcasing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Predict the classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcorrectLabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1170\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/NER2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in range(15,20):\n",
    "#     print(i)\n",
    "    maxf1 = 0\n",
    "    for epoch in range(epochs):    \n",
    "        print(\"Epoch %d/%d\"%(epoch, epochs))\n",
    "        a = Progbar(len(train_batch_len))\n",
    "        for i, batch in enumerate(iterate_minibatches(train_batch, train_batch_len)):\n",
    "            labels, tokens, casing, char = batch\n",
    "            labels = labels.tolist()\n",
    "    #         print(labels)\n",
    "            for sentence in labels:\n",
    "                for i,label in enumerate(sentence):\n",
    "                    temp = [0]*len(label2Idx)\n",
    "                    value = label[0]\n",
    "                    temp[value]=1\n",
    "                    sentence[i] = temp\n",
    "            labels = np.array(labels)\n",
    "    #         print(labels)\n",
    "            model.train_on_batch([tokens, casing, char], labels)\n",
    "            a.update(i)\n",
    "        if epoch >= 50:\n",
    "            predLabels, correctLabels = tag_dataset(dev_batch)        \n",
    "            pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, idx2Label)\n",
    "#             predLabels = np.concatenate(predLabels).ravel()\n",
    "#             correctLabels = np.concatenate(correctLabels).ravel()\n",
    "            print(len(predLabels), len(correctLabels))\n",
    "#             f1_dev = sklearn.metrics.f1_score(correctLabels,predLabels,average='macro' )\n",
    "            print(\"Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_dev, rec_dev, f1_dev))\n",
    "            print(f1_dev)\n",
    "\n",
    "            if f1_dev > maxf1:\n",
    "                model.save('german_ner_conll.h5')\n",
    "                maxf1 = f1_dev\n",
    "    #         predLabels, correctLabels = tag_dataset(dev_batch)        \n",
    "        #     with open(\"dropout=0.75.txt\",'w') as f:\n",
    "    #         pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, idx2Label)\n",
    "        #     with open('dropout=0.75.txt') as f:\n",
    "        #         x = str(epoch)+ \" \"+f1_dev\n",
    "        #         f.write(x)\n",
    "        #         f.write('\\n')\n",
    "    #         print(\"Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_dev, rec_dev, f1_dev))\n",
    "\n",
    "\n",
    "    id2word = {v: k for k, v in word2Idx.items()}\n",
    "    print(len(correctLabels))        \n",
    "\n",
    "    model.load_weights('german_ner_conll.h5')\n",
    "    predLabels, correctLabels = tag_dataset(test_batch)        \n",
    "    # pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, idx2Label)\n",
    "    # print(\"Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f\" % (pre_dev, rec_dev, f1_dev))\n",
    "\n",
    "    id = 1\n",
    "    file_name='test_conll'+str(x)+'.txt'\n",
    "    input_file_name = '/home/jindal/notebooks/jindal/NER/'+file_name\n",
    "    f = open(input_file_name,'wb')\n",
    "    for sentence_number, batch in enumerate(test_batch):\n",
    "        for word_number, wordid in enumerate(batch[0]):\n",
    "            word = id2word[wordid]\n",
    "            true_label = correctLabels[sentence_number][word_number]\n",
    "            true_label = idx2Label[true_label]\n",
    "            pred_label = predLabels[sentence_number][word_number]\n",
    "            pred_label = idx2Label[pred_label]\n",
    "\n",
    "#             string = str(id) + '\\t' + word + '\\t' + true_label +'\\t' + true_label+'\\t'+pred_label+'\\t'+pred_label+'\\t\\n'\n",
    "            string = word + ' ' + pred_label+' '+pred_label+' '+true_label+'\\n'\n",
    "            string = string.encode()\n",
    "            f.write(string)\n",
    "            id+=1\n",
    "\n",
    "    # command = \"perl nereval.perl\"\n",
    "    # directory_germeval = r'/home/jindal/notebooks/Resources/GermEVAL'\n",
    "    directory_conll = r'/home/jindal/notebooks/Resources/CONLL/2003/ner/bin'\n",
    "    command = \"perl conlleval\"\n",
    "    \n",
    "    output_file_name = '/home/jindal/notebooks/jindal/NER/test_result_conll'+str(x)+'.txt'\n",
    "    output_file=open(output_file_name,'w')\n",
    "    input_file = open(input_file_name)\n",
    "    import subprocess\n",
    "    process = subprocess.Popen(command.split(), stdin=input_file, stdout=output_file, cwd=directory_conll)\n",
    "    out, err = process.communicate()\n",
    "#     print(out)\n",
    "    print('******************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jindal/notebooks/Resources/GermEVAL'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
